{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ilu3/destinationPath/lib/python3.6/site-packages/ipykernel_launcher.py:170: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/ilu3/destinationPath/lib/python3.6/site-packages/pandas/core/frame.py:3997: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def create_splits(path_metadata, path_splits):\n",
    "    \"\"\"\n",
    "    Create curated splits for the MIMIC-CXR-JPG Database. \n",
    "    \n",
    "    The following transformation is applied to the database:\n",
    "        Keeps only affirmative data,\n",
    "        Merges the two set of structured labels\n",
    "        Removes disagreeing samples and multi-class samples\n",
    "        Removes the Pleural Other and Support Devices Class\n",
    "        Keeps only Antero-posterior oriented samples\n",
    "        Undersamples the No Finding class to 5000 samples\n",
    "        Exports the splits into a csv file\n",
    "        \n",
    "    Parameters:\n",
    "        path_metadata (str): The relative path of thefolder that the metadata, chexpert and negbio information is stored\n",
    "        path_splits (str): The relative path that the CSV file will be saved to\n",
    "    \"\"\"\n",
    "    novel_labels = ['Lung Lesion', 'Lung Opacity', 'Enlarged Cardiomediastinum',\n",
    "                    'Pleural Effusion', 'Pneumothorax', 'Fracture']\n",
    "\n",
    "    # Load in data\n",
    "    path_chexpert = Path(os.path.join(path_metadata, 'mimic-cxr-2.0.0-chexpert.csv.gz'))\n",
    "    path_negbio = Path(os.path.join(path_metadata, 'mimic-cxr-2.0.0-negbio.csv.gz'))\n",
    "    path_metadata = Path(os.path.join(path_metadata, 'mimic-cxr-2.0.0-metadata.csv.gz'))\n",
    "\n",
    "    df_chexpert = pd.read_csv(path_chexpert)\n",
    "    df_negbio = pd.read_csv(path_negbio)\n",
    "    df_metadata = pd.read_csv(path_metadata)\n",
    "\n",
    "    # Merge relevant metadata, NegBio labels and Chexpert labels\n",
    "    df = df_negbio.merge(\n",
    "        df_chexpert,\n",
    "        how='left',\n",
    "        left_on=['subject_id','study_id'], right_on=['subject_id','study_id'],\n",
    "        suffixes=('', '_cx')\n",
    "    )\n",
    "\n",
    "    df_metadata.drop([\n",
    "        'PerformedProcedureStepDescription',\n",
    "        'Rows',\n",
    "        'Columns',\n",
    "        'StudyDate',\n",
    "        'StudyTime',\n",
    "        'ProcedureCodeSequence_CodeMeaning',\n",
    "        'ViewCodeSequence_CodeMeaning',\n",
    "        'PatientOrientationCodeSequence_CodeMeaning'\n",
    "    ],axis=1, inplace=True)\n",
    "\n",
    "    df = df_metadata.merge(\n",
    "        df,\n",
    "        how='left',\n",
    "        left_on=['subject_id','study_id'], right_on=['subject_id','study_id'],\n",
    "    )\n",
    "\n",
    "    # Preprocess data:\n",
    "    # Only use data that is a '1.0'\n",
    "    # Remove all disagreeing '1.0' data\n",
    "    # Remove all Pleural Other and Support Devices findings\n",
    "    # Remove all non antero-posterior (AP) data\n",
    "    for key in df.columns:\n",
    "        if key in ('dicom_id', 'subject_id', 'study_id', 'ViewPosition'):\n",
    "            continue\n",
    "\n",
    "        if key[-3:] == '_cx':\n",
    "            continue\n",
    "\n",
    "        # Remove data that is not a '1.0'\n",
    "        df[key] = df[key].map({1: key})\n",
    "        df[key + '_cx'] = df[key + '_cx'].map({1: key})\n",
    "\n",
    "        # Remove all disagreeing '1.0' data\n",
    "        agree_matrix = df[key].fillna(0) == df[key + '_cx'].fillna(0)\n",
    "        df = df[agree_matrix]\n",
    "\n",
    "    # Remove all Pleural Other Data\n",
    "    keep = df['Pleural Other'].map({'Pleural Other': False}).fillna(True)\n",
    "    df = df[keep]\n",
    "    \n",
    "    # Remove all Pleural Other Data\n",
    "    keep = df['Support Devices'].map({'Support Devices': False}).fillna(True)\n",
    "    df = df[keep]\n",
    "\n",
    "    # Remove all non antero-posterior (AP) data\n",
    "    keep = df['ViewPosition'].map({'AP': True}).fillna(False)\n",
    "    df = df[keep]\n",
    "\n",
    "    # Remove Columns\n",
    "    df.drop([key for key in df.columns if key[-3:] == '_cx'], axis=1, inplace=True)\n",
    "    df.drop(['ViewPosition', 'Pleural Other', 'Support Devices'], axis=1, inplace=True)\n",
    "\n",
    "    # Separate columns into path and labels\n",
    "    df_labels = df.copy()\n",
    "    cols_path = [key for key in df.columns if key in ('dicom_id', 'subject_id', 'study_id')]\n",
    "    cols_labels = [key for key in df.columns if key not in ('dicom_id', 'subject_id', 'study_id')]\n",
    "\n",
    "    # Combine columns into a file path and labels\n",
    "    df_labels['file_path'] = df_labels[cols_path].apply(\n",
    "        lambda x: f\"p{str(x.values[1])[:2]}/p{x.values[1]}/s{x.values[2]}/{x.values[0]}.jpg\", axis=1)\n",
    "    df_labels['labels'] = df_labels[cols_labels].apply(lambda x: ','.join(x.dropna().values.tolist()), axis=1)\n",
    "    df_labels.drop(df.columns, axis=1, inplace=True)\n",
    "\n",
    "    # Remove all data that does not have a label\n",
    "    df_labels = df_labels[~(df_labels['labels'] == '')]\n",
    "\n",
    "    # Remove all multi label data\n",
    "    keep = df_labels['labels'].apply(lambda x: ',' not in x)\n",
    "    df_single_labels = df_labels[keep]\n",
    "\n",
    "    df_splits = df_single_labels.copy()\n",
    "\n",
    "    # Create base training splits of: 80% Training and 20% Validation per class\n",
    "    # Label novel classes to be used in episode generation\n",
    "    for label in cols_labels:\n",
    "        # Create a dataframe for the single label in this iteration\n",
    "        df_unsplit = df_splits[df_splits['labels'].apply(lambda x: x == label)]\n",
    "\n",
    "        # Base Classes\n",
    "        if label not in novel_labels:\n",
    "            # Undersample the 'No Finding' Class to 5000 samples\n",
    "            if label == 'No Finding':\n",
    "                df_unsplit = df_unsplit.sample(5000, random_state=1)\n",
    "\n",
    "            df_train = df_unsplit.sample(frac=0.8, random_state=1)\n",
    "            df_validate = df_unsplit.drop(df_train.index)\n",
    "\n",
    "            # Give split designation and merge back into main dataframe\n",
    "            df_train['split'] = 'base_train'\n",
    "            df_validate['split'] = 'base_validate'\n",
    "            df_train.drop(['file_path', 'labels'], axis=1, inplace=True)\n",
    "            df_validate.drop(['file_path', 'labels'], axis=1, inplace=True)\n",
    "            \n",
    "            # Merge the training dataframe into the main dataframe\n",
    "            df_splits = df_splits.merge(\n",
    "                df_train,\n",
    "                how='left',\n",
    "                left_index=True,\n",
    "                right_index=True,\n",
    "                suffixes=('', '_x')\n",
    "            )\n",
    "            \n",
    "            # Combines values of the split columns\n",
    "            if 'split_x' in df_splits.columns:\n",
    "                df_splits['split'] = df_splits[['split', 'split_x']].apply(\n",
    "                    lambda x: ''.join(x.dropna().values.tolist()), axis=1)\n",
    "                df_splits.drop('split_x', axis=1, inplace=True)\n",
    "                \n",
    "            # Merge the validation dataframe into the main dataframe\n",
    "            df_splits = df_splits.merge(\n",
    "                df_validate,\n",
    "                how='left',\n",
    "                left_index=True,\n",
    "                right_index=True,\n",
    "                suffixes=('', '_x')\n",
    "            )\n",
    "\n",
    "            # Combines values of the split columns\n",
    "            if 'split_x' in df_splits.columns:\n",
    "                df_splits['split'] = df_splits[['split', 'split_x']].apply(\n",
    "                    lambda x: ''.join(x.dropna().values.tolist()), axis=1)\n",
    "                df_splits.drop('split_x', axis=1, inplace=True)\n",
    "\n",
    "        # Novel Classes\n",
    "        else:\n",
    "            # Label the entire class as 'novel'\n",
    "            df_unsplit['split'] = 'novel'\n",
    "            df_unsplit.drop(['file_path', 'labels'], axis=1, inplace=True)\n",
    "            \n",
    "            # Merge the novel dataframe into the main dataframe\n",
    "            df_splits = df_splits.merge(\n",
    "                df_unsplit,\n",
    "                how='left',\n",
    "                left_index=True,\n",
    "                right_index=True,\n",
    "                suffixes=('', '_x')\n",
    "            )\n",
    "\n",
    "            # Combines values of the split columns\n",
    "            if 'split_x' in df_splits.columns:\n",
    "                df_splits['split'] = df_splits[['split', 'split_x']].apply(\n",
    "                    lambda x: ''.join(x.dropna().values.tolist()), axis=1)\n",
    "                df_splits.drop('split_x', axis=1, inplace=True)\n",
    "    \n",
    "    # Create splits folder if it does not exist\n",
    "    if not os.path.exists(path_splits):\n",
    "        os.makedirs(path_splits)\n",
    "        \n",
    "    # Export CSV containing splits\n",
    "    df_splits.to_csv(os.path.join(path_splits, 'splits.csv'), index=False)\n",
    "\n",
    "\n",
    "def check_splits(path_csv):\n",
    "    \"\"\"\n",
    "    Sums up the number of training, validation and novel samples per class\n",
    "\n",
    "    Parameters:\n",
    "            path_csv (str): The path to the CSV containing the splits data\n",
    "            \n",
    "    Returns: \n",
    "        dict_train (dict): A dictionary containing the number of training samples per class\n",
    "        dict_validate (dict): A dictionary containing the number of validation samples per class\n",
    "        dict_novel (dict): A dictionary containing the number of novel samples per class\n",
    "    \"\"\"\n",
    "    df_splits = pd.read_csv(path_csv)\n",
    "    dict_train = {}\n",
    "    dict_validate = {}\n",
    "    dict_novel = {}\n",
    "    for index, row in df_splits.iterrows():\n",
    "        # Counts the number of samples per class in the 'base_train' split category\n",
    "        if (row['split'] == 'base_train'):\n",
    "            if row['labels'] in dict_train.keys():\n",
    "                dict_train[row['labels']] += 1\n",
    "            else:\n",
    "                dict_train[row['labels']] = 1\n",
    "                \n",
    "        #Counts the number of samples per class in the 'base_validate' split category\n",
    "        elif (row['split'] == 'base_validate'):\n",
    "            if row['labels'] in dict_validate.keys():\n",
    "                dict_validate[row['labels']] += 1\n",
    "            else:\n",
    "                dict_validate[row['labels']] = 1\n",
    "                \n",
    "        #Counts the number of samples per class in the 'novel' split category\n",
    "        elif (row['split'] == 'novel'):\n",
    "            if row['labels'] in dict_novel.keys():\n",
    "                dict_novel[row['labels']] += 1\n",
    "            else:\n",
    "                dict_novel[row['labels']] = 1\n",
    "            \n",
    "    return dict_train, dict_validate, dict_novel\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    path_metadata = '../../../../scratch/rl80/mimic-cxr-jpg-2.0.0.physionet.org' # Folder of the MIMIC-CXR-JPG metadata\n",
    "    path_splits = '../splits' # Folder to save the splits to\n",
    "    \n",
    "    # Create Splits\n",
    "    create_splits(path_metadata, path_splits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_csv = '../splits/splits.csv'\n",
    "dict_train, dict_validate, dict_novel = check_splits(path_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Atelectasis': 2592,\n",
       " 'Cardiomegaly': 2850,\n",
       " 'Consolidation': 388,\n",
       " 'Edema': 1926,\n",
       " 'No Finding': 4000,\n",
       " 'Pneumonia': 762}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Atelectasis': 648,\n",
       " 'Cardiomegaly': 712,\n",
       " 'Consolidation': 97,\n",
       " 'Edema': 481,\n",
       " 'No Finding': 1000,\n",
       " 'Pneumonia': 190}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Enlarged Cardiomediastinum': 428,\n",
       " 'Fracture': 410,\n",
       " 'Lung Lesion': 462,\n",
       " 'Lung Opacity': 5409,\n",
       " 'Pleural Effusion': 2242,\n",
       " 'Pneumothorax': 1029}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_novel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    from biobertology import get_biobert, get_tokenizer\n",
    "    \n",
    "    path = '../results' # Where to download the pre-trained BioBERT weights to\n",
    "    biobert = get_biobert(model_dir=path, download=True) # Download weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
