{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision import *\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def load_data(class_type, tfms=None, size=224, bs = 16, path_splits='../splits.csv',\n",
    "              path_jpg=Path('../../../../../scratch/rl80/mimic-cxr-jpg-2.0.0.physionet.org/files/')):\n",
    "    df = pd.read_csv(path_splits)\n",
    "\n",
    "    if class_type == 'base':\n",
    "        train_idx = df.index[df['split'] == 'base_train']\n",
    "        valid_idx = df.index[df['split'] == 'base_validate']\n",
    "    elif class_type == 'novel':\n",
    "        train_idx = df.index[df['split'] == 'novel_train']\n",
    "        valid_idx = df.index[df['split'] == 'novel_validate']\n",
    "    else:\n",
    "        raise Exception(\"Invalid class type input\")\n",
    "\n",
    "    ret_data = (ImageList.from_df(df, path_jpg)\n",
    "                .split_by_idxs(train_idx, valid_idx)\n",
    "                .label_from_df(cols='labels')\n",
    "                .transform(tfms=tfms, size=size, resize_method=ResizeMethod.SQUISH)\n",
    "                .databunch(bs=bs)\n",
    "                .normalize(imagenet_stats))\n",
    "\n",
    "    return ret_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "█\r"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../../../../../scratch/rl80/mimic-cxr-jpg-2.0.0.physionet.org/files/models/models/stage3.pth'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-8a5828e9a35d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mParameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_groups\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./models/stage3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/ilu3/destinationPath/lib/python3.6/site-packages/fastai/basic_train.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, file, return_path, with_opt)\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mwith_opt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'model'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mget_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'opt'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 254\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    255\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreturn_path\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ilu3/destinationPath/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    328\u001b[0m         \u001b[0m_legacy_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ilu3/destinationPath/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ilu3/destinationPath/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../../../../../scratch/rl80/mimic-cxr-jpg-2.0.0.physionet.org/files/models/models/stage3.pth'"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "from fastai import *\n",
    "from fastai.vision import *\n",
    "from fastai.callbacks import *\n",
    "import sys\n",
    "sys.path.append('../..')\n",
    "from scripts.data_scripts import *\n",
    "from scripts.layers import *\n",
    "\n",
    "# Parameters for the datablock API\n",
    "tfms=None\n",
    "size=224\n",
    "bs = 16\n",
    "path_splits='../splits.csv'\n",
    "path_jpg=Path('../../../../../scratch/rl80/mimic-cxr-jpg-2.0.0.physionet.org/files/')\n",
    "\n",
    "# Load in novel class data\n",
    "data = load_data('novel')\n",
    "\n",
    "# Create Learner\n",
    "arch = models.densenet161\n",
    "learn = cnn_learner(data, arch, metrics=[error_rate, accuracy], loss_func=torch.nn.CrossEntropyLoss())\n",
    "\n",
    "# Load model\n",
    "model_path = '../../../../../home/ilu3/rl80/Monash_NIH_FYP/models/baseline/stage2_class_10to3'\n",
    "learn = learn.load(model_path)\n",
    "\n",
    "# Get samples per class to create weights\n",
    "# Get index for the relevant labels\n",
    "df = pd.read_csv(path_splits)\n",
    "train_idx = []\n",
    "train_idx.append(df.index[(df['split'] == 'novel_train') & (df['labels'] == 'Enlarged Cardiomediastinum')])\n",
    "train_idx.append(df.index[(df['split'] == 'novel_train') & (df['labels'] == 'Lung Lesion')])\n",
    "train_idx.append(df.index[(df['split'] == 'novel_train') & (df['labels'] == 'Pleural Effusion')])\n",
    "\n",
    "# Hook to get the output of the second last layer\n",
    "def feature_hook(model, input, output):\n",
    "    global activation\n",
    "    activation = torch.cat((activation, output), 0)\n",
    "    \n",
    "hook = learn.model[-1][-2].register_forward_hook(feature_hook)\n",
    "\n",
    "data_novel = []\n",
    "weight = torch.FloatTensor([]).cuda()\n",
    "for i in range(3):\n",
    "    # Create a list containing only samples from each label\n",
    "    data_novel.append((ImageList.from_df(df, path_jpg)\n",
    "                 .split_by_idxs(train_idx[i], train_idx[i])\n",
    "                 .label_from_df(cols='labels')\n",
    "                 .transform(tfms=tfms, size=size, resize_method=ResizeMethod.SQUISH)\n",
    "                 .databunch(bs=bs)\n",
    "                 .normalize(imagenet_stats))\n",
    "               )\n",
    "    \n",
    "    # Get Activations\n",
    "    activation = torch.FloatTensor([]).cuda()\n",
    "    learn.validate(data_novel[i].valid_dl) # Use validation set as the training set is a multiple of the batch size\n",
    "    mean = torch.mean(activation, 0)\n",
    "    weight = torch.cat((weight, mean[None]), 0)\n",
    "\n",
    "# Replace the layer parameters with the new feature average\n",
    "param = nn.Parameter(weight, True)\n",
    "learn.layer_groups[-1][-1].weight = param\n",
    "learn.save('stage3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 512])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0284, -0.0016,  0.0239,  ..., -0.0205,  0.0300,  0.0006],\n",
       "        [ 0.0402, -0.0078,  0.0059,  ...,  0.0379,  0.0095,  0.0173],\n",
       "        [ 0.0153,  0.0122, -0.0195,  ..., -0.0247,  0.0242,  0.0106]],\n",
       "       device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(learn.model.parameters())[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "█\r"
     ]
    }
   ],
   "source": [
    "from fastai import *\n",
    "from fastai.vision import *\n",
    "from fastai.callbacks import *\n",
    "import sys\n",
    "sys.path.append('../..')\n",
    "from scripts.data_scripts import *\n",
    "from scripts.layers import *\n",
    "\n",
    "# Load in base class data\n",
    "data = load_data('novel')\n",
    "\n",
    "# Use custom header to replace classifier layers\n",
    "custom_head = create_head(4416, 3, lin_ftrs=[512])\n",
    "custom_head[-1] = CosineSimilarityLayer(512, 3)\n",
    "\n",
    "# Create Learner\n",
    "arch = models.densenet161\n",
    "learn = cnn_learner(data, arch, metrics=[accuracy],\n",
    "                    loss_func=torch.nn.CrossEntropyLoss(),\n",
    "                    custom_head=custom_head)\n",
    "\n",
    "# Load model\n",
    "model_path = 'stage3_test'\n",
    "learn = learn.load(model_path, strict=False) # Need strict=False to ignore the extra bias parameters\n",
    "\n",
    "# Results\n",
    "results = learn.validate(data.valid_dl)\n",
    "with open('test.txt', 'a') as f:\n",
    "    print('Accuracy:', round(results[1].item(),4), file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test.txt', 'a') as f:\n",
    "    print('Accuracy:', file=f)\n",
    "    print('Accuracy:', file=f)\n",
    "    print('Accuracy:', file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "█\r"
     ]
    }
   ],
   "source": [
    "from fastai import *\n",
    "from fastai.vision import *\n",
    "from fastai.callbacks import *\n",
    "import sys\n",
    "sys.path.append('../..')\n",
    "from scripts.data_scripts import *\n",
    "from scripts.layers import *\n",
    "\n",
    "# Parameters for the datablock API\n",
    "tfms=None\n",
    "size=224\n",
    "bs = 16\n",
    "path_splits='../splits.csv'\n",
    "path_jpg=Path('../../../../../scratch/rl80/mimic-cxr-jpg-2.0.0.physionet.org/files/')\n",
    "\n",
    "# Load in novel class data\n",
    "data = load_data('novel')\n",
    "data_base = load_data('base')\n",
    "\n",
    "# Get samples per class to create weights\n",
    "# Get index for the relevant labels\n",
    "df = pd.read_csv(path_splits)\n",
    "train_idx = []\n",
    "train_idx.append(df.index[(df['split'] == 'novel_train') & (df['labels'] == 'Enlarged Cardiomediastinum')])\n",
    "train_idx.append(df.index[(df['split'] == 'novel_train') & (df['labels'] == 'Lung Lesion')])\n",
    "train_idx.append(df.index[(df['split'] == 'novel_train') & (df['labels'] == 'Pleural Effusion')])\n",
    "\n",
    "# Hook to get the output of the second last layer\n",
    "def feature_hook(model, input, output):\n",
    "    global activation\n",
    "    activation = torch.cat((activation, output), 0)\n",
    "\n",
    "for k in range(30):\n",
    "    # Create Learner\n",
    "    num_sample = [2592, 2850, 388, 1926, 328, 4327, 4000, 762, 823, 1860]\n",
    "    max_sample = max(num_sample)\n",
    "    weight = torch.FloatTensor([max_sample / x for x in num_sample]).cuda()\n",
    "\n",
    "    arch = models.densenet161\n",
    "    learn = cnn_learner(data_base, arch, metrics=[error_rate, accuracy],\n",
    "                        loss_func=torch.nn.CrossEntropyLoss(weight=weight))\n",
    "\n",
    "    # Load Model and convert last layer from 10 classes to 3\n",
    "    model_path = f\"bestmodel_{k}\"\n",
    "    learn = learn.load(model_path)\n",
    "    learn.model[-1][-1] = nn.Linear(512, 3)\n",
    "    learn.save('stage2')\n",
    "    \n",
    "    # Use custom header to replace classifier layers\n",
    "    custom_head = create_head(4416, 3, lin_ftrs=[512])\n",
    "    custom_head[-1] = CosineSimilarityLayer(512, 3)\n",
    "\n",
    "    # Create Learner\n",
    "    arch = models.densenet161\n",
    "    learn = cnn_learner(data, arch, metrics=[accuracy],\n",
    "                        loss_func=torch.nn.CrossEntropyLoss(),\n",
    "                        custom_head=custom_head)\n",
    "    \n",
    "    # Load model\n",
    "    learn = learn.load('stage2', strict=False)\n",
    "    hook = learn.model[-1][-2].register_forward_hook(feature_hook)\n",
    "\n",
    "    data_novel = []\n",
    "    weight = torch.FloatTensor([]).cuda()\n",
    "    for i in range(3):\n",
    "        # Create a list containing only samples from each label\n",
    "        data_novel.append((ImageList.from_df(df, path_jpg)\n",
    "                     .split_by_idxs(train_idx[i], train_idx[i])\n",
    "                     .label_from_df(cols='labels')\n",
    "                     .transform(tfms=tfms, size=size, resize_method=ResizeMethod.SQUISH)\n",
    "                     .databunch(bs=bs)\n",
    "                     .normalize(imagenet_stats))\n",
    "                   )\n",
    "\n",
    "        # Get Activations\n",
    "        activation = torch.FloatTensor([]).cuda()\n",
    "        learn.validate(data_novel[i].valid_dl) # Use validation set as the training set is a multiple of the batch size\n",
    "        mean = torch.mean(activation, 0)\n",
    "        weight = torch.cat((weight, mean[None]), 0)\n",
    "        \n",
    "    hook.remove()\n",
    "    # Replace the layer parameters with the new feature average\n",
    "    param = nn.Parameter(weight, True)\n",
    "    learn.layer_groups[-1][-1].weight = param\n",
    "    # Results\n",
    "    results = learn.validate(data.valid_dl)\n",
    "    with open('test.txt', 'a') as f:\n",
    "        print(k, ': Accuracy:', round(results[1].item(),4), file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1333,  0.1624, -0.1139,  ...,  0.0982, -0.0786, -0.0167],\n",
       "        [-0.1461,  0.2213, -0.1593,  ...,  0.0467, -0.1567, -0.0174],\n",
       "        [-0.1403,  0.1544, -0.1227,  ...,  0.0706, -0.0923,  0.0219]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.hooks.RemovableHandle at 0x7f2753026278>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Dropout' object has no attribute 'hooks'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-42528ca9dfa9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/ilu3/destinationPath/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    574\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0;32m--> 576\u001b[0;31m             type(self).__name__, name))\n\u001b[0m\u001b[1;32m    577\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Dropout' object has no attribute 'hooks'"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "learn.model[-1][-2].hooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
