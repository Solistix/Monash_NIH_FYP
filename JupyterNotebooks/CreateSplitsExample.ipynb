{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai import *\n",
    "from fastai.vision import *\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in data\n",
    "path_chexpert = Path('../../../../scratch/rl80/mimic-cxr-jpg-2.0.0.physionet.org/mimic-cxr-2.0.0-chexpert.csv.gz')\n",
    "path_negbio = Path('../../../../scratch/rl80/mimic-cxr-jpg-2.0.0.physionet.org/mimic-cxr-2.0.0-negbio.csv.gz')\n",
    "path_metadata = Path('../../../../scratch/rl80/mimic-cxr-jpg-2.0.0.physionet.org/mimic-cxr-2.0.0-metadata.csv.gz')\n",
    "\n",
    "df_chexpert = pd.read_csv(path_chexpert)\n",
    "df_negbio = pd.read_csv(path_negbio)\n",
    "df_metadata = pd.read_csv(path_metadata)\n",
    "\n",
    "k_shot = 50\n",
    "novel_labels = ['Lung Lesion', 'Enlarged Cardiomediastinum', 'Pleural Effusion']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dicom_id</th>\n",
       "      <th>subject_id</th>\n",
       "      <th>study_id</th>\n",
       "      <th>ViewPosition</th>\n",
       "      <th>Atelectasis</th>\n",
       "      <th>Cardiomegaly</th>\n",
       "      <th>Consolidation</th>\n",
       "      <th>Edema</th>\n",
       "      <th>Enlarged Cardiomediastinum</th>\n",
       "      <th>Fracture</th>\n",
       "      <th>...</th>\n",
       "      <th>Enlarged Cardiomediastinum_cx</th>\n",
       "      <th>Fracture_cx</th>\n",
       "      <th>Lung Lesion_cx</th>\n",
       "      <th>Lung Opacity_cx</th>\n",
       "      <th>No Finding_cx</th>\n",
       "      <th>Pleural Effusion_cx</th>\n",
       "      <th>Pleural Other_cx</th>\n",
       "      <th>Pneumonia_cx</th>\n",
       "      <th>Pneumothorax_cx</th>\n",
       "      <th>Support Devices_cx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>02aa804e-bde0afdd-112c0b34-7bc16630-4e384014</td>\n",
       "      <td>10000032</td>\n",
       "      <td>50414267</td>\n",
       "      <td>PA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>174413ec-4ec4c1f7-34ea26b7-c5f994f8-79ef1962</td>\n",
       "      <td>10000032</td>\n",
       "      <td>50414267</td>\n",
       "      <td>LATERAL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2a2277a9-b0ded155-c0de8eb9-c124d10e-82c5caab</td>\n",
       "      <td>10000032</td>\n",
       "      <td>53189527</td>\n",
       "      <td>PA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>e084de3b-be89b11e-20fe3f9f-9c8d8dfe-4cfd202c</td>\n",
       "      <td>10000032</td>\n",
       "      <td>53189527</td>\n",
       "      <td>LATERAL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>68b5c4b1-227d0485-9cc38c3f-7b84ab51-4b472714</td>\n",
       "      <td>10000032</td>\n",
       "      <td>53911762</td>\n",
       "      <td>AP</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       dicom_id  subject_id  study_id  \\\n",
       "0  02aa804e-bde0afdd-112c0b34-7bc16630-4e384014    10000032  50414267   \n",
       "1  174413ec-4ec4c1f7-34ea26b7-c5f994f8-79ef1962    10000032  50414267   \n",
       "2  2a2277a9-b0ded155-c0de8eb9-c124d10e-82c5caab    10000032  53189527   \n",
       "3  e084de3b-be89b11e-20fe3f9f-9c8d8dfe-4cfd202c    10000032  53189527   \n",
       "4  68b5c4b1-227d0485-9cc38c3f-7b84ab51-4b472714    10000032  53911762   \n",
       "\n",
       "  ViewPosition  Atelectasis  Cardiomegaly  Consolidation  Edema  \\\n",
       "0           PA          NaN           NaN            NaN    NaN   \n",
       "1      LATERAL          NaN           NaN            NaN    NaN   \n",
       "2           PA          NaN           NaN            NaN    NaN   \n",
       "3      LATERAL          NaN           NaN            NaN    NaN   \n",
       "4           AP          NaN           NaN            NaN    NaN   \n",
       "\n",
       "   Enlarged Cardiomediastinum  Fracture  ...  Enlarged Cardiomediastinum_cx  \\\n",
       "0                         NaN       NaN  ...                            NaN   \n",
       "1                         NaN       NaN  ...                            NaN   \n",
       "2                         NaN       NaN  ...                            NaN   \n",
       "3                         NaN       NaN  ...                            NaN   \n",
       "4                         NaN       NaN  ...                            NaN   \n",
       "\n",
       "   Fracture_cx  Lung Lesion_cx  Lung Opacity_cx  No Finding_cx  \\\n",
       "0          NaN             NaN              NaN            1.0   \n",
       "1          NaN             NaN              NaN            1.0   \n",
       "2          NaN             NaN              NaN            1.0   \n",
       "3          NaN             NaN              NaN            1.0   \n",
       "4          NaN             NaN              NaN            1.0   \n",
       "\n",
       "   Pleural Effusion_cx  Pleural Other_cx  Pneumonia_cx  Pneumothorax_cx  \\\n",
       "0                  NaN               NaN           NaN              NaN   \n",
       "1                  NaN               NaN           NaN              NaN   \n",
       "2                  NaN               NaN           NaN              NaN   \n",
       "3                  NaN               NaN           NaN              NaN   \n",
       "4                  NaN               NaN           NaN              NaN   \n",
       "\n",
       "   Support Devices_cx  \n",
       "0                 NaN  \n",
       "1                 NaN  \n",
       "2                 NaN  \n",
       "3                 NaN  \n",
       "4                 NaN  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merge relevant metadata, NegBio labels and Chexpert labels\n",
    "df = df_negbio.merge(\n",
    "    df_chexpert,\n",
    "    how='left',\n",
    "    left_on=['subject_id','study_id'], right_on=['subject_id','study_id'],\n",
    "    suffixes=('', '_cx')\n",
    ")\n",
    "\n",
    "df_metadata.drop([\n",
    "    'PerformedProcedureStepDescription', \n",
    "    'Rows', \n",
    "    'Columns', \n",
    "    'StudyDate', \n",
    "    'StudyTime', \n",
    "    'ProcedureCodeSequence_CodeMeaning', \n",
    "    'ViewCodeSequence_CodeMeaning', \n",
    "    'PatientOrientationCodeSequence_CodeMeaning'\n",
    "],axis=1, inplace=True)\n",
    "\n",
    "df = df_metadata.merge(\n",
    "    df,\n",
    "    how='left',\n",
    "    left_on=['subject_id','study_id'], right_on=['subject_id','study_id'],\n",
    ")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dicom_id</th>\n",
       "      <th>subject_id</th>\n",
       "      <th>study_id</th>\n",
       "      <th>Atelectasis</th>\n",
       "      <th>Cardiomegaly</th>\n",
       "      <th>Consolidation</th>\n",
       "      <th>Edema</th>\n",
       "      <th>Enlarged Cardiomediastinum</th>\n",
       "      <th>Fracture</th>\n",
       "      <th>Lung Lesion</th>\n",
       "      <th>Lung Opacity</th>\n",
       "      <th>No Finding</th>\n",
       "      <th>Pleural Effusion</th>\n",
       "      <th>Pneumonia</th>\n",
       "      <th>Pneumothorax</th>\n",
       "      <th>Support Devices</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>68b5c4b1-227d0485-9cc38c3f-7b84ab51-4b472714</td>\n",
       "      <td>10000032</td>\n",
       "      <td>53911762</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No Finding</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>fffabebf-74fd3a1f-673b6b41-96ec0ac9-2ab69818</td>\n",
       "      <td>10000032</td>\n",
       "      <td>53911762</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No Finding</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ea030e7a-2e3b1346-bc518786-7a8fd698-f673b44c</td>\n",
       "      <td>10000032</td>\n",
       "      <td>56699142</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No Finding</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>096052b7-d256dc40-453a102b-fa7d01c6-1b22c6b4</td>\n",
       "      <td>10000764</td>\n",
       "      <td>57375967</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Consolidation</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>d0b71acc-b5a62046-bbb5f6b8-7b173b85-65cdf738</td>\n",
       "      <td>10000935</td>\n",
       "      <td>50578979</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pleural Effusion</td>\n",
       "      <td>Pneumonia</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        dicom_id  subject_id  study_id  \\\n",
       "4   68b5c4b1-227d0485-9cc38c3f-7b84ab51-4b472714    10000032  53911762   \n",
       "5   fffabebf-74fd3a1f-673b6b41-96ec0ac9-2ab69818    10000032  53911762   \n",
       "6   ea030e7a-2e3b1346-bc518786-7a8fd698-f673b44c    10000032  56699142   \n",
       "7   096052b7-d256dc40-453a102b-fa7d01c6-1b22c6b4    10000764  57375967   \n",
       "15  d0b71acc-b5a62046-bbb5f6b8-7b173b85-65cdf738    10000935  50578979   \n",
       "\n",
       "   Atelectasis Cardiomegaly  Consolidation Edema Enlarged Cardiomediastinum  \\\n",
       "4          NaN          NaN            NaN   NaN                        NaN   \n",
       "5          NaN          NaN            NaN   NaN                        NaN   \n",
       "6          NaN          NaN            NaN   NaN                        NaN   \n",
       "7          NaN          NaN  Consolidation   NaN                        NaN   \n",
       "15         NaN          NaN            NaN   NaN                        NaN   \n",
       "\n",
       "   Fracture Lung Lesion Lung Opacity  No Finding  Pleural Effusion  Pneumonia  \\\n",
       "4       NaN         NaN          NaN  No Finding               NaN        NaN   \n",
       "5       NaN         NaN          NaN  No Finding               NaN        NaN   \n",
       "6       NaN         NaN          NaN  No Finding               NaN        NaN   \n",
       "7       NaN         NaN          NaN         NaN               NaN        NaN   \n",
       "15      NaN         NaN          NaN         NaN  Pleural Effusion  Pneumonia   \n",
       "\n",
       "   Pneumothorax Support Devices  \n",
       "4           NaN             NaN  \n",
       "5           NaN             NaN  \n",
       "6           NaN             NaN  \n",
       "7           NaN             NaN  \n",
       "15          NaN             NaN  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocess data:\n",
    "# Only use data that is a '1.0'\n",
    "# Remove all disagreeing '1.0' data\n",
    "# Remove all Pleural Other findings\n",
    "# Remove all non antero-posterior (AP) data\n",
    "for key in df.columns:\n",
    "    if key in ('dicom_id','subject_id','study_id', 'ViewPosition'):\n",
    "        continue\n",
    "    \n",
    "    if key[-3:] == '_cx':\n",
    "        continue\n",
    "        \n",
    "    # Remove data that is not a '1.0'\n",
    "    df[key] = df[key].map({1:key})\n",
    "    df[key + '_cx'] = df[key + '_cx'].map({1:key})\n",
    "    \n",
    "    # Remove all disagreeing '1.0' data\n",
    "    agree_matrix = df[key].fillna(0) == df[key + '_cx'].fillna(0)\n",
    "    df = df[agree_matrix]\n",
    "\n",
    "# Remove all Pleural Other Data\n",
    "keep = df['Pleural Other'].map({'Pleural Other': False}).fillna(True)\n",
    "df = df[keep]\n",
    "\n",
    "# Remove all non antero-posterior (AP) data\n",
    "keep = df['ViewPosition'].map({'AP': True}).fillna(False)\n",
    "df = df[keep]\n",
    "\n",
    "# Remove Columns\n",
    "df.drop([key for key in df.columns if key[-3:] == '_cx'], axis=1, inplace=True)\n",
    "df.drop(['ViewPosition','Pleural Other'], axis=1, inplace=True)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_path</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>p10/p10000032/s53911762/68b5c4b1-227d0485-9cc3...</td>\n",
       "      <td>No Finding</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>p10/p10000032/s53911762/fffabebf-74fd3a1f-673b...</td>\n",
       "      <td>No Finding</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>p10/p10000032/s56699142/ea030e7a-2e3b1346-bc51...</td>\n",
       "      <td>No Finding</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>p10/p10000764/s57375967/096052b7-d256dc40-453a...</td>\n",
       "      <td>Consolidation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>p10/p10000935/s50578979/d0b71acc-b5a62046-bbb5...</td>\n",
       "      <td>Pleural Effusion,Pneumonia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>377102</th>\n",
       "      <td>p19/p19999442/s58497551/ee9155f3-944c056b-c76c...</td>\n",
       "      <td>Atelectasis,Lung Opacity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>377103</th>\n",
       "      <td>p19/p19999442/s58708861/16b6c70f-6d36bd77-89d2...</td>\n",
       "      <td>No Finding,Support Devices</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>377107</th>\n",
       "      <td>p19/p19999987/s55368167/58766883-376a15ce-3b32...</td>\n",
       "      <td>Atelectasis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>377108</th>\n",
       "      <td>p19/p19999987/s58621812/7ba273af-3d290f8d-e28d...</td>\n",
       "      <td>Atelectasis,Support Devices</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>377109</th>\n",
       "      <td>p19/p19999987/s58971208/1a1fe7e3-cbac5d93-b339...</td>\n",
       "      <td>Atelectasis</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>130272 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                file_path  \\\n",
       "4       p10/p10000032/s53911762/68b5c4b1-227d0485-9cc3...   \n",
       "5       p10/p10000032/s53911762/fffabebf-74fd3a1f-673b...   \n",
       "6       p10/p10000032/s56699142/ea030e7a-2e3b1346-bc51...   \n",
       "7       p10/p10000764/s57375967/096052b7-d256dc40-453a...   \n",
       "15      p10/p10000935/s50578979/d0b71acc-b5a62046-bbb5...   \n",
       "...                                                   ...   \n",
       "377102  p19/p19999442/s58497551/ee9155f3-944c056b-c76c...   \n",
       "377103  p19/p19999442/s58708861/16b6c70f-6d36bd77-89d2...   \n",
       "377107  p19/p19999987/s55368167/58766883-376a15ce-3b32...   \n",
       "377108  p19/p19999987/s58621812/7ba273af-3d290f8d-e28d...   \n",
       "377109  p19/p19999987/s58971208/1a1fe7e3-cbac5d93-b339...   \n",
       "\n",
       "                             labels  \n",
       "4                        No Finding  \n",
       "5                        No Finding  \n",
       "6                        No Finding  \n",
       "7                     Consolidation  \n",
       "15       Pleural Effusion,Pneumonia  \n",
       "...                             ...  \n",
       "377102     Atelectasis,Lung Opacity  \n",
       "377103   No Finding,Support Devices  \n",
       "377107                  Atelectasis  \n",
       "377108  Atelectasis,Support Devices  \n",
       "377109                  Atelectasis  \n",
       "\n",
       "[130272 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Separate columns into path and labels\n",
    "df_labels = df.copy()\n",
    "cols_path = [key for key in df.columns if key in ('dicom_id', 'subject_id', 'study_id')]\n",
    "cols_labels = [key for key in df.columns if key not in ('dicom_id', 'subject_id', 'study_id')]\n",
    "\n",
    "# Combine columns into a file path and labels\n",
    "df_labels['file_path'] = df_labels[cols_path].apply(lambda x: f\"p{str(x.values[1])[:2]}/p{x.values[1]}/s{x.values[2]}/{x.values[0]}.jpg\", axis=1)\n",
    "df_labels['labels'] = df_labels[cols_labels].apply(lambda x: ','.join(x.dropna().values.tolist()), axis=1)\n",
    "df_labels.drop(df.columns, axis=1, inplace=True)\n",
    "\n",
    "# Remove all data that does not have a label\n",
    "df_labels = df_labels[~(df_labels['labels']=='')]\n",
    "df_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Atelectasis': 3240,\n",
       " 'Cardiomegaly': 3562,\n",
       " 'Consolidation': 485,\n",
       " 'Edema': 2407,\n",
       " 'Enlarged Cardiomediastinum': 428,\n",
       " 'Fracture': 410,\n",
       " 'Lung Lesion': 462,\n",
       " 'Lung Opacity': 5409,\n",
       " 'No Finding': 21110,\n",
       " 'Pleural Effusion': 2242,\n",
       " 'Pneumonia': 952,\n",
       " 'Pneumothorax': 1029,\n",
       " 'Support Devices': 2325}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove all multi label data\n",
    "keep = df_labels['labels'].apply(lambda x: ',' not in x)\n",
    "df_single_labels = df_labels[keep]\n",
    "\n",
    "# Get the resulting samples per class to aid in deciding the size of the sets\n",
    "dict_count = {}\n",
    "for label in df_single_labels['labels']:\n",
    "    if label in dict_count.keys():\n",
    "        dict_count[label] += 1\n",
    "    else:\n",
    "        dict_count[label] = 1\n",
    "\n",
    "dict_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_splits = df_single_labels.copy()\n",
    "\n",
    "# Create splits: 80% Training and 20% Validation per base class\n",
    "#                100 training and 300 validation samples per novel class\n",
    "for label in cols_labels:\n",
    "    df_unsplit = df_splits[df_splits['labels'].apply(lambda x: x == label)]\n",
    "    \n",
    "    # Base Classes\n",
    "    if label not in novel_labels:\n",
    "        df_train = df_unsplit.sample(frac=0.8, random_state=1)\n",
    "        df_validate = df_unsplit.drop(df_train.index)\n",
    "        \n",
    "        # Give split designation and merge back into main dataframe\n",
    "        df_train['split'] = 'base_train'\n",
    "        df_validate['split'] = 'base_validate'\n",
    "        df_train.drop(['file_path', 'labels'], axis=1, inplace=True)\n",
    "        df_validate.drop(['file_path', 'labels'], axis=1, inplace=True)\n",
    "    \n",
    "        df_splits = df_splits.merge(\n",
    "            df_train,\n",
    "            how='left',\n",
    "            left_index=True,\n",
    "            right_index=True,\n",
    "            suffixes=('', '_x')\n",
    "        )\n",
    "    \n",
    "        if 'split_x' in df_splits.columns:\n",
    "            df_splits['split'] = df_splits[['split', 'split_x']].apply(lambda x: ''.join(x.dropna().values.tolist()), axis=1)\n",
    "            df_splits.drop('split_x', axis=1, inplace=True)\n",
    "    \n",
    "        df_splits = df_splits.merge(\n",
    "            df_validate,\n",
    "            how='left',\n",
    "            left_index=True,\n",
    "            right_index=True,\n",
    "            suffixes=('', '_x')\n",
    "        )\n",
    "    \n",
    "        if 'split_x' in df_splits.columns:\n",
    "            df_splits['split'] = df_splits[['split', 'split_x']].apply(lambda x: ''.join(x.dropna().values.tolist()), axis=1)\n",
    "            df_splits.drop('split_x', axis=1, inplace=True)\n",
    "    \n",
    "    # Novel Classes\n",
    "    else:\n",
    "        df_unsplit = df_unsplit.sample(n=k_shot+300, random_state=1)\n",
    "    \n",
    "        df_unsplit['split'] = ''\n",
    "        df_unsplit['split'][:k_shot] = 'novel_train'\n",
    "        df_unsplit['split'][k_shot:] = 'novel_validate'\n",
    "        df_unsplit.drop(['file_path', 'labels'], axis=1, inplace=True)\n",
    "\n",
    "        df_splits = df_splits.merge(\n",
    "            df_unsplit,\n",
    "            how='left',\n",
    "            left_index=True,\n",
    "            right_index=True,\n",
    "            suffixes=('', '_x')\n",
    "        )\n",
    "    \n",
    "        if 'split_x' in df_splits.columns:\n",
    "            df_splits['split'] = df_splits[['split', 'split_x']].apply(lambda x: ''.join(x.dropna().values.tolist()), axis=1)\n",
    "            df_splits.drop('split_x', axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show Number of Samples for Debugging\n",
    "dict_train = {}\n",
    "dict_validate = {}\n",
    "for index, row in df_splits.iterrows():\n",
    "    if (row['split'] == 'base_train') or (row['split'] == 'novel_train'):\n",
    "        if row['labels'] in dict_train.keys():\n",
    "            dict_train[row['labels']] += 1\n",
    "        else:\n",
    "            dict_train[row['labels']] = 1\n",
    "    elif (row['split'] == 'base_validate') or (row['split'] == 'novel_validate'):\n",
    "        if row['labels'] in dict_validate.keys():\n",
    "            dict_validate[row['labels']] += 1\n",
    "        else:\n",
    "            dict_validate[row['labels']] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Atelectasis': 2592,\n",
       " 'Cardiomegaly': 2850,\n",
       " 'Consolidation': 388,\n",
       " 'Edema': 1926,\n",
       " 'Enlarged Cardiomediastinum': 50,\n",
       " 'Fracture': 328,\n",
       " 'Lung Lesion': 50,\n",
       " 'Lung Opacity': 4327,\n",
       " 'No Finding': 16888,\n",
       " 'Pleural Effusion': 50,\n",
       " 'Pneumonia': 762,\n",
       " 'Pneumothorax': 823,\n",
       " 'Support Devices': 1860}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Atelectasis': 648,\n",
       " 'Cardiomegaly': 712,\n",
       " 'Consolidation': 97,\n",
       " 'Edema': 481,\n",
       " 'Enlarged Cardiomediastinum': 300,\n",
       " 'Fracture': 82,\n",
       " 'Lung Lesion': 300,\n",
       " 'Lung Opacity': 1082,\n",
       " 'No Finding': 4222,\n",
       " 'Pleural Effusion': 300,\n",
       " 'Pneumonia': 190,\n",
       " 'Pneumothorax': 206,\n",
       " 'Support Devices': 465}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "def create_splits(k_shot, path_splits):\n",
    "    \"\"\"\n",
    "    Create training and validation splits for the MIMIC-CXR-JPG Database. This function also performs the following:\n",
    "        Keeps only affirmative data,\n",
    "        Merges the two set of structured labels\n",
    "        Removes disagreeing samples and multi-class samples\n",
    "        Removes the Pleural Other and Support Devices class\n",
    "        Keeps only Antero-posterior oriented samples\n",
    "        Undersamples the No Finding class to 5000 samples\n",
    "        Exports the splits into a csv file\n",
    "\n",
    "    Input:\n",
    "            k_shot: The number of samples per class for the novel classes\n",
    "    \"\"\"\n",
    "    novel_labels = ['Lung Lesion', 'Enlarged Cardiomediastinum', 'Pleural Effusion']\n",
    "\n",
    "    # Load in data\n",
    "    path_chexpert = Path('../../../../scratch/rl80/mimic-cxr-jpg-2.0.0.physionet.org/mimic-cxr-2.0.0-chexpert.csv.gz')\n",
    "    path_negbio = Path('../../../../scratch/rl80/mimic-cxr-jpg-2.0.0.physionet.org/mimic-cxr-2.0.0-negbio.csv.gz')\n",
    "    path_metadata = Path('../../../../scratch/rl80/mimic-cxr-jpg-2.0.0.physionet.org/mimic-cxr-2.0.0-metadata.csv.gz')\n",
    "\n",
    "    df_chexpert = pd.read_csv(path_chexpert)\n",
    "    df_negbio = pd.read_csv(path_negbio)\n",
    "    df_metadata = pd.read_csv(path_metadata)\n",
    "\n",
    "    # Merge relevant metadata, NegBio labels and Chexpert labels\n",
    "    df = df_negbio.merge(\n",
    "        df_chexpert,\n",
    "        how='left',\n",
    "        left_on=['subject_id','study_id'], right_on=['subject_id','study_id'],\n",
    "        suffixes=('', '_cx')\n",
    "    )\n",
    "\n",
    "    df_metadata.drop([\n",
    "        'PerformedProcedureStepDescription',\n",
    "        'Rows',\n",
    "        'Columns',\n",
    "        'StudyDate',\n",
    "        'StudyTime',\n",
    "        'ProcedureCodeSequence_CodeMeaning',\n",
    "        'ViewCodeSequence_CodeMeaning',\n",
    "        'PatientOrientationCodeSequence_CodeMeaning'\n",
    "    ],axis=1, inplace=True)\n",
    "\n",
    "    df = df_metadata.merge(\n",
    "        df,\n",
    "        how='left',\n",
    "        left_on=['subject_id','study_id'], right_on=['subject_id','study_id'],\n",
    "    )\n",
    "\n",
    "    # Preprocess data:\n",
    "    # Only use data that is a '1.0'\n",
    "    # Remove all disagreeing '1.0' data\n",
    "    # Remove all Pleural Other findings\n",
    "    # Remove all non antero-posterior (AP) data\n",
    "    for key in df.columns:\n",
    "        if key in ('dicom_id', 'subject_id', 'study_id', 'ViewPosition'):\n",
    "            continue\n",
    "\n",
    "        if key[-3:] == '_cx':\n",
    "            continue\n",
    "\n",
    "        # Remove data that is not a '1.0'\n",
    "        df[key] = df[key].map({1: key})\n",
    "        df[key + '_cx'] = df[key + '_cx'].map({1: key})\n",
    "\n",
    "        # Remove all disagreeing '1.0' data\n",
    "        agree_matrix = df[key].fillna(0) == df[key + '_cx'].fillna(0)\n",
    "        df = df[agree_matrix]\n",
    "\n",
    "    # Remove all Pleural Other Data\n",
    "    keep = df['Pleural Other'].map({'Pleural Other': False}).fillna(True)\n",
    "    df = df[keep]\n",
    "    \n",
    "    # Remove all Support Devices Data\n",
    "    keep = df['Support Devices'].map({'Support Devices': False}).fillna(True)\n",
    "    df = df[keep]\n",
    "\n",
    "    # Remove all non antero-posterior (AP) data\n",
    "    keep = df['ViewPosition'].map({'AP': True}).fillna(False)\n",
    "    df = df[keep]\n",
    "\n",
    "    # Remove Columns\n",
    "    df.drop([key for key in df.columns if key[-3:] == '_cx'], axis=1, inplace=True)\n",
    "    df.drop(['ViewPosition', 'Pleural Other', 'Support Devices'], axis=1, inplace=True)\n",
    "\n",
    "    # Separate columns into path and labels\n",
    "    df_labels = df.copy()\n",
    "    cols_path = [key for key in df.columns if key in ('dicom_id', 'subject_id', 'study_id')]\n",
    "    cols_labels = [key for key in df.columns if key not in ('dicom_id', 'subject_id', 'study_id')]\n",
    "\n",
    "    # Combine columns into a file path and labels\n",
    "    df_labels['file_path'] = df_labels[cols_path].apply(\n",
    "        lambda x: f\"p{str(x.values[1])[:2]}/p{x.values[1]}/s{x.values[2]}/{x.values[0]}.jpg\", axis=1)\n",
    "    df_labels['labels'] = df_labels[cols_labels].apply(lambda x: ','.join(x.dropna().values.tolist()), axis=1)\n",
    "    df_labels.drop(df.columns, axis=1, inplace=True)\n",
    "\n",
    "    # Remove all data that does not have a label\n",
    "    df_labels = df_labels[~(df_labels['labels'] == '')]\n",
    "\n",
    "    # Remove all multi label data\n",
    "    keep = df_labels['labels'].apply(lambda x: ',' not in x)\n",
    "    df_single_labels = df_labels[keep]\n",
    "\n",
    "    df_splits = df_single_labels.copy()\n",
    "\n",
    "    # Create splits: 80% Training and 20% Validation per base class\n",
    "    #                100 training and 300 validation samples per novel class\n",
    "    for label in cols_labels:\n",
    "        df_unsplit = df_splits[df_splits['labels'].apply(lambda x: x == label)]\n",
    "\n",
    "        # Base Classes\n",
    "        if label not in novel_labels:\n",
    "\n",
    "            # Undersample the 'No Finding' Class to 5000 samples\n",
    "            if label == 'No Finding':\n",
    "                df_unsplit = df_unsplit.sample(5000, random_state=1)\n",
    "\n",
    "            df_train = df_unsplit.sample(frac=0.8, random_state=1)\n",
    "            df_validate = df_unsplit.drop(df_train.index)\n",
    "\n",
    "            # Give split designation and merge back into main dataframe\n",
    "            df_train['split'] = 'base_train'\n",
    "            df_validate['split'] = 'base_validate'\n",
    "            df_train.drop(['file_path', 'labels'], axis=1, inplace=True)\n",
    "            df_validate.drop(['file_path', 'labels'], axis=1, inplace=True)\n",
    "\n",
    "            df_splits = df_splits.merge(\n",
    "                df_train,\n",
    "                how='left',\n",
    "                left_index=True,\n",
    "                right_index=True,\n",
    "                suffixes=('', '_x')\n",
    "            )\n",
    "\n",
    "            if 'split_x' in df_splits.columns:\n",
    "                df_splits['split'] = df_splits[['split', 'split_x']].apply(\n",
    "                    lambda x: ''.join(x.dropna().values.tolist()), axis=1)\n",
    "                df_splits.drop('split_x', axis=1, inplace=True)\n",
    "\n",
    "            df_splits = df_splits.merge(\n",
    "                df_validate,\n",
    "                how='left',\n",
    "                left_index=True,\n",
    "                right_index=True,\n",
    "                suffixes=('', '_x')\n",
    "            )\n",
    "\n",
    "            if 'split_x' in df_splits.columns:\n",
    "                df_splits['split'] = df_splits[['split', 'split_x']].apply(\n",
    "                    lambda x: ''.join(x.dropna().values.tolist()), axis=1)\n",
    "                df_splits.drop('split_x', axis=1, inplace=True)\n",
    "\n",
    "        # Novel Classes\n",
    "        else:\n",
    "            df_unsplit = df_unsplit.sample(n=k_shot + 300, random_state=1)\n",
    "\n",
    "            df_unsplit['split'] = ''\n",
    "            df_unsplit['split'][:k_shot] = 'novel_train'\n",
    "            df_unsplit['split'][k_shot:] = 'novel_validate'\n",
    "            df_unsplit.drop(['file_path', 'labels'], axis=1, inplace=True)\n",
    "\n",
    "            df_splits = df_splits.merge(\n",
    "                df_unsplit,\n",
    "                how='left',\n",
    "                left_index=True,\n",
    "                right_index=True,\n",
    "                suffixes=('', '_x')\n",
    "            )\n",
    "\n",
    "            if 'split_x' in df_splits.columns:\n",
    "                df_splits['split'] = df_splits[['split', 'split_x']].apply(\n",
    "                    lambda x: ''.join(x.dropna().values.tolist()), axis=1)\n",
    "                df_splits.drop('split_x', axis=1, inplace=True)\n",
    "\n",
    "    df_splits.to_csv(os.path.join(path_splits, f'{k_shot}_shot.csv'), index=False)\n",
    "\n",
    "\n",
    "def check_splits(df_csv):\n",
    "    \"\"\"\n",
    "    Sums up the number of training and validation samples per class\n",
    "\n",
    "    Input:\n",
    "            df_csv: A dataframe containing training validation split data\n",
    "    Output: An array containing two dictionaries stating the amount of training and validation samples\n",
    "    \"\"\"\n",
    "    df_splits = df_csv\n",
    "    dict_train = {}\n",
    "    dict_validate = {}\n",
    "    for index, row in df_splits.iterrows():\n",
    "        if (row['split'] == 'base_train') or (row['split'] == 'novel_train'):\n",
    "            if row['labels'] in dict_train.keys():\n",
    "                dict_train[row['labels']] += 1\n",
    "            else:\n",
    "                dict_train[row['labels']] = 1\n",
    "        elif (row['split'] == 'base_validate') or (row['split'] == 'novel_validate'):\n",
    "            if row['labels'] in dict_validate.keys():\n",
    "                dict_validate[row['labels']] += 1\n",
    "            else:\n",
    "                dict_validate[row['labels']] = 1\n",
    "\n",
    "    return [dict_train, dict_validate]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    shot_list = [20, 10, 5, 3, 1]\n",
    "    for k_shot in shot_list:\n",
    "        create_splits(k_shot, '../splits')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_splits(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "{'Consolidation': 388, 'Lung Opacity': 4327, 'No Finding': 4000, 'Edema': 1926, 'Pneumonia': 762, 'Cardiomegaly': 2850, 'Pneumothorax': 823, 'Atelectasis': 2592, 'Support Devices': 1860, 'Lung Lesion': 20, 'Fracture': 328, 'Enlarged Cardiomediastinum': 20, 'Pleural Effusion': 20}\n",
      "{'Lung Opacity': 1082, 'No Finding': 1000, 'Enlarged Cardiomediastinum': 300, 'Pleural Effusion': 300, 'Consolidation': 97, 'Edema': 481, 'Support Devices': 465, 'Lung Lesion': 300, 'Cardiomegaly': 712, 'Pneumonia': 190, 'Atelectasis': 648, 'Fracture': 82, 'Pneumothorax': 206}\n",
      "10\n",
      "{'Consolidation': 388, 'Lung Opacity': 4327, 'No Finding': 4000, 'Edema': 1926, 'Pneumonia': 762, 'Cardiomegaly': 2850, 'Pneumothorax': 823, 'Atelectasis': 2592, 'Support Devices': 1860, 'Fracture': 328, 'Enlarged Cardiomediastinum': 10, 'Pleural Effusion': 10, 'Lung Lesion': 10}\n",
      "{'Lung Opacity': 1082, 'No Finding': 1000, 'Enlarged Cardiomediastinum': 300, 'Pleural Effusion': 300, 'Consolidation': 97, 'Edema': 481, 'Support Devices': 465, 'Lung Lesion': 300, 'Cardiomegaly': 712, 'Pneumonia': 190, 'Atelectasis': 648, 'Fracture': 82, 'Pneumothorax': 206}\n",
      "5\n",
      "{'Consolidation': 388, 'Lung Opacity': 4327, 'No Finding': 4000, 'Edema': 1926, 'Pneumonia': 762, 'Cardiomegaly': 2850, 'Pneumothorax': 823, 'Atelectasis': 2592, 'Support Devices': 1860, 'Fracture': 328, 'Lung Lesion': 5, 'Enlarged Cardiomediastinum': 5, 'Pleural Effusion': 5}\n",
      "{'Lung Opacity': 1082, 'No Finding': 1000, 'Enlarged Cardiomediastinum': 300, 'Pleural Effusion': 300, 'Consolidation': 97, 'Edema': 481, 'Support Devices': 465, 'Lung Lesion': 300, 'Cardiomegaly': 712, 'Pneumonia': 190, 'Atelectasis': 648, 'Fracture': 82, 'Pneumothorax': 206}\n",
      "3\n",
      "{'Consolidation': 388, 'Lung Opacity': 4327, 'No Finding': 4000, 'Edema': 1926, 'Pneumonia': 762, 'Cardiomegaly': 2850, 'Pneumothorax': 823, 'Atelectasis': 2592, 'Support Devices': 1860, 'Fracture': 328, 'Lung Lesion': 3, 'Pleural Effusion': 3, 'Enlarged Cardiomediastinum': 3}\n",
      "{'Lung Opacity': 1082, 'No Finding': 1000, 'Enlarged Cardiomediastinum': 300, 'Pleural Effusion': 300, 'Consolidation': 97, 'Edema': 481, 'Support Devices': 465, 'Lung Lesion': 300, 'Cardiomegaly': 712, 'Pneumonia': 190, 'Atelectasis': 648, 'Fracture': 82, 'Pneumothorax': 206}\n",
      "1\n",
      "{'Consolidation': 388, 'Lung Opacity': 4327, 'No Finding': 4000, 'Edema': 1926, 'Pneumonia': 762, 'Cardiomegaly': 2850, 'Pneumothorax': 823, 'Atelectasis': 2592, 'Support Devices': 1860, 'Fracture': 328, 'Enlarged Cardiomediastinum': 1, 'Lung Lesion': 1, 'Pleural Effusion': 1}\n",
      "{'Lung Opacity': 1082, 'No Finding': 1000, 'Enlarged Cardiomediastinum': 300, 'Pleural Effusion': 300, 'Consolidation': 97, 'Edema': 481, 'Support Devices': 465, 'Lung Lesion': 300, 'Cardiomegaly': 712, 'Pneumonia': 190, 'Atelectasis': 648, 'Fracture': 82, 'Pneumothorax': 206}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def check_splits(df_csv):\n",
    "    \"\"\"\n",
    "    Sums up the number of training and validation samples per class\n",
    "\n",
    "    Input:\n",
    "            df_csv: A dataframe containing training validation split data\n",
    "    Output: An array containing two dictionaries stating the amount of training and validation samples\n",
    "    \"\"\"\n",
    "    df_splits = df_csv\n",
    "    dict_train = {}\n",
    "    dict_validate = {}\n",
    "    for index, row in df_splits.iterrows():\n",
    "        if (row['split'] == 'base_train') or (row['split'] == 'novel_train'):\n",
    "            if row['labels'] in dict_train.keys():\n",
    "                dict_train[row['labels']] += 1\n",
    "            else:\n",
    "                dict_train[row['labels']] = 1\n",
    "        elif (row['split'] == 'base_validate') or (row['split'] == 'novel_validate'):\n",
    "            if row['labels'] in dict_validate.keys():\n",
    "                dict_validate[row['labels']] += 1\n",
    "            else:\n",
    "                dict_validate[row['labels']] = 1\n",
    "\n",
    "    return [dict_train, dict_validate]\n",
    "\n",
    "for k_shot in [20, 10, 5, 3, 1]:\n",
    "    df = pd.read_csv(f'../splits/{k_shot}_shot.csv')\n",
    "    dict_train, dict_validate = check_splits(df)\n",
    "    print(k_shot)\n",
    "    print(dict_train)\n",
    "    print(dict_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2 3\n"
     ]
    }
   ],
   "source": [
    "print(1,2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
