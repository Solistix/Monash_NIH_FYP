{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import transforms\n",
    "import torch.optim as optim\n",
    "from biobertology import get_biobert, get_tokenizer\n",
    "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from shared.models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MimicCxrMulti(Dataset):\n",
    "    \"\"\"\n",
    "    MIMIC-CXR-JPG Images and MIMIC-CXR Reports\n",
    "    Todo: Insert references to the database here!\n",
    "    Removes '_' from reports\n",
    "    Truncates the reports to 512 tokens by removing the beginning of the report (Usually where the 'wet read' resides)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, root_image, root_text, csv_path, tokenizer, mode, resize=224, max_length=512):\n",
    "        \n",
    "        # Check if mode contains an accepted value\n",
    "        if mode not in ('base_train', 'base_validate', 'novel_train', 'novel_validate'):\n",
    "            raise Exception(\"Selected 'mode' is not valid\")\n",
    "        \n",
    "        # Initialise variables\n",
    "        self.root_text = root_text\n",
    "        self.root_image = root_image\n",
    "        self.resize = resize\n",
    "        self.max_length = max_length\n",
    "        self.transform = transforms.Compose([lambda x: Image.open(x).convert('L'), # Transforms for images\n",
    "                                             transforms.Resize((self.resize, self.resize)),\n",
    "                                             transforms.ToTensor()\n",
    "                                             ])\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "        # Load data\n",
    "        csv_data = pd.read_csv(csv_path)\n",
    "        self.data = csv_data[csv_data.split == mode]\n",
    "        \n",
    "        if mode == 'base_train' or mode == 'base_validate':\n",
    "            self.dict_labels = {\n",
    "                'Atelectasis': 0,\n",
    "                'Cardiomegaly': 1,\n",
    "                'Consolidation': 2,\n",
    "                'Edema': 3,\n",
    "                'Fracture': 4,\n",
    "                'Lung Opacity': 5,\n",
    "                'No Finding': 6,\n",
    "                'Pneumonia': 7,\n",
    "                'Pneumothorax': 8,\n",
    "                'Support Devices': 9\n",
    "            }\n",
    "        else:\n",
    "            self.dict_labels = {\n",
    "                'Enlarged Cardiomediastinum': 0,\n",
    "                'Lung Lesion': 1,\n",
    "                'Pleural Effusion': 2,\n",
    "            }\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Extract CSV data\n",
    "        file_path = self.data.iloc[idx, 0]\n",
    "        label = self.data.iloc[idx, 1]\n",
    "        \n",
    "        # Get image tensor\n",
    "        img_path = os.path.join(self.root_image, file_path) # Absolute file path to the JPG img\n",
    "        img_tensor = self.transform(img_path)\n",
    "        \n",
    "        # Get text tensor and attention mask\n",
    "        text_name = f'{file_path.split(\"/\")[2]}.txt' # Extract the study id to find the report\n",
    "        text_path = Path(os.path.join(self.root_text, text_name))\n",
    "        plain_text = text_path.read_text()\n",
    "        plain_text = plain_text.replace('_','') # Remove all underscores from the text\n",
    "        encoded_text = self.tokenizer.encode(plain_text, add_special_tokens=True)\n",
    "        \n",
    "        # Transform encodings to be of the same size\n",
    "        len_encoding = len(encoded_text)\n",
    "        if len_encoding > self.max_length:\n",
    "            # Truncate to max length\n",
    "            cutoff = len_encoding - self.max_length + 1 # The cutoff for the tokens to be deleted\n",
    "            del encoded_text[1:cutoff]\n",
    "            attention = [1] * self.max_length\n",
    "        elif len_encoding < self.max_length:\n",
    "            # Pad to max length\n",
    "            num_padding = self.max_length - len_encoding\n",
    "            encoded_text.extend([0] * num_padding) # Padding token is 0\n",
    "            attention = [1] * len_encoding\n",
    "            attention.extend([0] * (self.max_length - len_encoding))\n",
    "        else:\n",
    "            # If equal size, create attention matrix\n",
    "            attention = [1] * self.max_length\n",
    "            \n",
    "        text_tensor = torch.tensor(encoded_text)\n",
    "        attention_tensor = torch.tensor(attention)\n",
    "        \n",
    "        return img_tensor, text_tensor, attention_tensor, self.dict_labels[label]\n",
    "\n",
    "class MultiModalNet(nn.Module):\n",
    "    def __init__(self, n_way, path_biobert):\n",
    "        super(MultiModalNet, self).__init__()\n",
    "        self.baseline = BaselineNet(n_way)\n",
    "        self.biobert = get_biobert(model_dir=path_biobert, download=False)\n",
    "        self.concat_linear = nn.Linear(13312, n_way) # 12544 + 768 = 13312 from baseline and biobert respectively\n",
    "    def forward(self, image, text, attention_mask):\n",
    "        _, image = self.baseline(image, extract_features=True) # baseline returns: logits, features\n",
    "        _, text = self.biobert(text, attention_mask=attention_mask) # biobert returns: sequence output, pooled output\n",
    "        x = torch.cat((image,text), 1)\n",
    "        x = self.concat_linear(x)\n",
    "        return x    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, criterion, device, optimizer, freeze=False):\n",
    "    # freeze accepts a list and represents the layers not to freeze\n",
    "    model.train()\n",
    "\n",
    "    # Freeze all layers except those indicated\n",
    "    if freeze:\n",
    "        for name, param in model.named_parameters():\n",
    "            if name not in freeze:\n",
    "                param.requires_grad = False\n",
    "\n",
    "    train_loss = 0\n",
    "    for step, (data_image, data_text, data_attention, data_labels) in enumerate(train_loader):\n",
    "        image_inputs, labels = data_image.to(device), data_labels.to(device)\n",
    "        text_inputs, attention_inputs = data_text.to(device), data_attention.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(image_inputs, text_inputs, attention_inputs)\n",
    "        loss = criterion(pred, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()  # Running training loss\n",
    "\n",
    "    return train_loss / (step+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The layers that will not be frozen are: ['concat_linear.weight', 'concat_linear.bias']\n",
      "2.452433466911316\n",
      "2.0351550579071045\n",
      "1.7757296562194824\n",
      "1.5069501996040344\n",
      "1.313522219657898\n"
     ]
    }
   ],
   "source": [
    "# Variables\n",
    "num_epochs = 5\n",
    "num_workers = 3\n",
    "bs = 32\n",
    "n_way = 3\n",
    "root_image = '../../../../scratch/rl80/mimic-cxr-jpg-2.0.0.physionet.org/files'\n",
    "root_text = '../../../../scratch/rl80/mimic-cxr-2.0.0.physionet.org'\n",
    "path_splits = '../splits/20_shot.csv'\n",
    "path_biobert = './'\n",
    "path_pretrained = '../results/basic/basic_36.pth' # Pretrained image model\n",
    "freeze = ['concat_linear.weight', 'concat_linear.bias']\n",
    "\n",
    "# Check for GPU device\n",
    "torch.cuda.set_device(0)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load in model\n",
    "model = MultiModalNet(n_way, path_biobert).to(device)\n",
    "pretrained_dict = torch.load(path_pretrained)\n",
    "\n",
    "# Convert image model to work with the multi modal model\n",
    "multi_dict = {}\n",
    "del pretrained_dict['linear.weight']  # Pretrained model is for 10-way, remove last layer for 3-way\n",
    "del pretrained_dict['linear.bias']\n",
    "for key, value in pretrained_dict.items():\n",
    "    multi_dict[f'baseline.{key}'] = pretrained_dict[key] # The model has 'baseline.' in front of every image model key\n",
    "model_dict = model.state_dict()\n",
    "model_dict.update(multi_dict)\n",
    "model.load_state_dict(model_dict)\n",
    "\n",
    "# Check if the layers to be unfrozen are in the model\n",
    "if type(freeze) != bool:\n",
    "    check = all(item in model_dict for item in freeze)\n",
    "    if check:\n",
    "        print(f'The layers that will not be frozen are: {freeze}')\n",
    "    else:\n",
    "        raise Exception('Not all elements to stay unfrozen are in the model')\n",
    "\n",
    "# Load in dataset\n",
    "tokenizer = get_tokenizer()\n",
    "train_dataset = MimicCxrMulti(root_image, root_text, path_splits, tokenizer, mode='novel_train')\n",
    "train_loader = DataLoader(train_dataset, batch_size=bs, shuffle=True, num_workers=num_workers)\n",
    "\n",
    "# Training\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    #train_loss = train(model, train_loader, criterion, device, optimizer)\n",
    "        # freeze accepts a list and represents the layers not to freeze\n",
    "    model.train()\n",
    "\n",
    "    # Freeze all layers except those indicated\n",
    "    if freeze:\n",
    "        for name, param in model.named_parameters():\n",
    "            if name not in freeze:\n",
    "                param.requires_grad = False\n",
    "\n",
    "    train_loss = 0\n",
    "    for step, (data_image, data_text, data_attention, data_labels) in enumerate(train_loader):\n",
    "        image_inputs, labels = data_image.to(device), data_labels.to(device)\n",
    "        text_inputs, attention_inputs = data_text.to(device), data_attention.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(image_inputs, text_inputs, attention_inputs)\n",
    "        loss = criterion(pred, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()  # Running training loss\n",
    "    #val_loss, acc, m_acc, macro_f1, class_f1 = test(model, test_loader, criterion, device, n_way)\n",
    "\n",
    "    #if (save_models):\n",
    "    #    torch.save(model.state_dict(), os.path.join(path_models, f'basic_{epoch + 1}.pth'))  # Save the model\n",
    "\n",
    "    # Append and report results\n",
    "    #df_results.loc[epoch] = [epoch + 1, train_loss, val_loss, acc, m_acc, macro_f1] + class_f1\n",
    "    #print(\n",
    "    #    f'[{epoch + 1}] t_loss: {train_loss:.5f} v_loss: {val_loss:.5f} val_acc: {acc:.5f} '\n",
    "    #    f'val_m_acc: {m_acc:.5f} f1: {macro_f1:.5f}')\n",
    "    print(train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1, 224, 224])\n",
      "torch.Size([32, 1, 512])\n",
      "torch.Size([32, 1, 512])\n",
      "torch.Size([32])\n",
      "torch.Size([1, 224, 224])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'size'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-6c7f597ab736>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest4\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute 'size'"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "print(image_inputs.size())\n",
    "print(text_inputs.size())\n",
    "print(attention_inputs.size())\n",
    "print(labels.size())\n",
    "test1, test2, test3, test4 = train_dataset[1]\n",
    "print(test1.size())\n",
    "print(test2.size())\n",
    "print(test3.size())\n",
    "print(test4.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biobert = get_biobert('./',download=False)\n",
    "biobert.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "number of dims don't match in permute",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-364040233a23>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#biobert(text_inputs, attention_mask=attention_inputs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mbiobert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/ilu3/destinationPath/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ilu3/destinationPath/lib/python3.6/site-packages/pytorch_transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, token_type_ids, attention_mask, position_ids, head_mask)\u001b[0m\n\u001b[1;32m    708\u001b[0m         encoder_outputs = self.encoder(embedding_output,\n\u001b[1;32m    709\u001b[0m                                        \u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 710\u001b[0;31m                                        head_mask=head_mask)\n\u001b[0m\u001b[1;32m    711\u001b[0m         \u001b[0msequence_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m         \u001b[0mpooled_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpooler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ilu3/destinationPath/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ilu3/destinationPath/lib/python3.6/site-packages/pytorch_transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask)\u001b[0m\n\u001b[1;32m    429\u001b[0m                 \u001b[0mall_hidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_hidden_states\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 431\u001b[0;31m             \u001b[0mlayer_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    432\u001b[0m             \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ilu3/destinationPath/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ilu3/destinationPath/lib/python3.6/site-packages/pytorch_transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask)\u001b[0m\n\u001b[1;32m    407\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 409\u001b[0;31m         \u001b[0mattention_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    410\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m         \u001b[0mintermediate_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ilu3/destinationPath/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ilu3/destinationPath/lib/python3.6/site-packages/pytorch_transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_tensor, attention_mask, head_mask)\u001b[0m\n\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 366\u001b[0;31m         \u001b[0mself_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    367\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# add attentions if we output them\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ilu3/destinationPath/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ilu3/destinationPath/lib/python3.6/site-packages/pytorch_transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask)\u001b[0m\n\u001b[1;32m    288\u001b[0m         \u001b[0mmixed_value_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 290\u001b[0;31m         \u001b[0mquery_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose_for_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmixed_query_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    291\u001b[0m         \u001b[0mkey_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose_for_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmixed_key_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m         \u001b[0mvalue_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose_for_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmixed_value_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ilu3/destinationPath/lib/python3.6/site-packages/pytorch_transformers/modeling_bert.py\u001b[0m in \u001b[0;36mtranspose_for_scores\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    281\u001b[0m         \u001b[0mnew_x_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_attention_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention_head_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnew_x_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 283\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: number of dims don't match in permute"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "#biobert(text_inputs, attention_mask=attention_inputs)\n",
    "biobert(text_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-1c539d610526>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m          \u001b[0;31m# print(os.path.join(directory, filename))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mabs_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mencoded_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mabs_path\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mhold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhold\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/python/3.6.2-static/lib/python3.6/pathlib.py\u001b[0m in \u001b[0;36mread_text\u001b[0;34m(self, encoding, errors)\u001b[0m\n\u001b[1;32m   1173\u001b[0m         \"\"\"\n\u001b[1;32m   1174\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1175\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrite_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/python/3.6.2-static/lib/python3.6/codecs.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, input, final)\u001b[0m\n\u001b[1;32m    316\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m         \u001b[0;31m# decode input (taking the buffer into account)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "import os\n",
    "count = 0\n",
    "for filename in os.listdir(root_text):\n",
    "    if filename.endswith(\".txt\"):\n",
    "         # print(os.path.join(directory, filename))\n",
    "        abs_path = Path(os.path.join(root_text, filename))\n",
    "        encoded_text = tokenizer.encode(abs_path.read_text(), add_special_tokens=True)\n",
    "        hold = len(encoded_text)\n",
    "        if hold > count:\n",
    "            count = hold\n",
    "        continue\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_data = pd.read_csv(path_splits)\n",
    "#data = csv_data[csv_data.split == 'novel_train']\n",
    "data = csv_data[csv_data.split == 'novel_validate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p16/p16104236/s57336365/caea42f2-5abeb0d6-c6753361-0252cae8-66323dcb.jpg 535\n",
      "p17/p17673557/s52871653/be675e3a-b4ef502b-816a850e-4e47a8b6-cc00cf55.jpg 849\n"
     ]
    }
   ],
   "source": [
    "for index, row in data.iterrows():\n",
    "    file_path = row['file_path']\n",
    "    # Get paths for both images and text\n",
    "    text_name = f'{file_path.split(\"/\")[2]}.txt' # Extract the study id to find the report\n",
    "    abs_path = Path(os.path.join(root_text, text_name))\n",
    "    text = abs_path.read_text()\n",
    "    text = text.replace('_','')\n",
    "    encoded_text = tokenizer.encode(text, add_special_tokens=True)\n",
    "    if len(encoded_text) > 511:\n",
    "        hold=encoded_text\n",
    "        print(file_path, len(encoded_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([126])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tokenizer.encode(cleaned_text, add_special_tokens=True, max_length=512)\n",
    "new_tensor = torch.tensor(encoded_text)\n",
    "new_tensor.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\" WET READ : 8 : 38 AM ONE COULD ARGUE THAT AREAS OF LUCENCY WITHIN THE RIGHT MEDIASTINAL BORDER ARE MORE PROMINENT ON TODAY'S EXAMINATION, AS COMPARED TO THE MOST RECENT PRIOR XRAY, COMPATIBLE WITH KNOWN PNEUMOMEDIASTINUM AS SEEN ON PRIOR CHEST CT FROM. LUNG VOLUMES ARE DECREASED, ACCENTUATING BRONCHOVASCULAR STRUCTURES AND CAUSING INCREASED CROWDING OF THE LUNG BASES BILATERALLY. AGAIN SEEN ARE SMALL BILATERAL PLEURAL EFFUSIONS WITH INTERSTITIAL ABNORMALITY, MOST PRONOUNCED AT THE LEFT MID LUNG ZONE, SUGGESTIVE OF CHRONIC INTERSTITIAL LUNG DISEASE. FINDINGS DISCUSSED WITH DR BY NSR VIA PHONE ON AT 6 : 15 PM. WET READ VERSION # 1 6 : 21 PM ONE COULD ARGUE THAT AREAS OF LUCENCY WITHIN THE RIGHT MEDIASTINAL BORDER ARE MORE PROMINENT ON TODAY'S EXAMINATION, AS COMPARED TO THE MOST RECENT PRIOR XRAY, COMPATIBLE WITH KNOWN PNEUMOMEDIASTINUM AS SEEN ON PRIOR CHEST CT FROM. LUNG VOLUMES ARE DECREASED, ACCENTUATING BRONCHOVASCULAR STRUCTURES AND CAUSING INCREASED CROWDING OF THE LUNG BASES BILATERALLY. AGAIN SEEN ARE SMALL BILATERAL PLEURAL EFFUSIONS WITH INTERSTITIAL ABNORMALITY, MOST PRONOUNCED AT THE LEFT MID LUNG ZONE, SUGGESTIVE OF CHRONIC INTERSTITIAL LUNG DISEASE. FINDINGS DISCUSSED WITH DR BY NSR VIA PHONE ON AT 6 : 15 PM. FINAL REPORT EXAMINATION : CHEST ( PORTABLE AP ) CHEST ( PORTABLE AP ) i INDICATION : year old woman with esophageal perforation / / interval change? COMPARISON : Chest radiograph. Read in conjunction with chest CT one. IMPRESSION : The upper mediastinum is slightly wider today than on and there may be pneumo mediastinum, as noted in the preliminary reading and discussed by Dr. with Dr. on at 18 : 15. If the patient has referable symptomatology, CT scanning would be reasonable to determine if pneumomediastinum has increased since, and whether there is a new mediastinal fluid collection. Right lung is clear. Interstitial abnormality in the left lung appears more pronounced, probably because of lower lung volumes. There is no pleural effusion or pneumothorax. Heart size is normal. \"]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(hold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "849\n",
      "512\n"
     ]
    }
   ],
   "source": [
    "max_length = 512\n",
    "print(len(hold))\n",
    "if len(hold) > max_length:\n",
    "    cutoff = len(hold) - max_length + 1 # The cutoff for the tokens to be deleted\n",
    "    del hold[1:cutoff]\n",
    "print(len(hold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "126\n",
      "512\n"
     ]
    }
   ],
   "source": [
    "print(len(encoded_text))\n",
    "if len(encoded_text) < max_length:\n",
    "    num_padding = max_length - len(encoded_text)\n",
    "    encoded_text.extend([0] * num_padding)\n",
    "    print(len(encoded_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.0343,  0.0601, -0.1038,  ..., -0.0026,  0.1650,  0.0292],\n",
       "          [-0.4967,  0.4419, -0.3455,  ...,  0.4246, -0.6533,  0.1716],\n",
       "          [-0.4041, -0.0194,  0.0374,  ...,  0.4303, -0.2325, -0.3277],\n",
       "          ...,\n",
       "          [ 0.0118, -0.1907, -0.0836,  ...,  0.1552,  0.1065,  0.0623],\n",
       "          [ 0.2091, -0.2067, -0.1239,  ..., -0.0492, -0.0890,  0.0951],\n",
       "          [ 0.1902, -0.5259, -0.6845,  ..., -0.4168,  0.0479, -0.3069]]],\n",
       "        grad_fn=<NativeLayerNormBackward>),\n",
       " tensor([[ 3.4797e-02,  6.9386e-02,  9.5684e-01, -9.9999e-01,  9.9998e-01,\n",
       "           3.8128e-01,  2.6741e-02,  8.5040e-01, -3.2890e-02, -2.0456e-02,\n",
       "           8.7006e-01,  9.9974e-01,  8.1255e-02, -8.8358e-01,  6.9053e-02,\n",
       "           1.6002e-02,  9.9999e-01,  1.1773e-02, -9.9771e-01, -4.4217e-02,\n",
       "          -7.1061e-03, -9.5842e-01,  1.5264e-01,  9.7861e-01, -4.8262e-02,\n",
       "           7.2457e-02,  9.9969e-01,  9.9737e-01,  3.3668e-02, -1.1860e-02,\n",
       "           5.8690e-03, -9.9999e-01,  9.9294e-01, -9.9969e-01,  2.1293e-01,\n",
       "          -5.1981e-03, -1.2173e-01, -1.7372e-01,  7.4592e-01, -9.9622e-01,\n",
       "           4.6262e-02,  2.4015e-01, -4.2718e-03, -1.1987e-01,  9.9656e-01,\n",
       "           4.3947e-02,  1.2981e-01,  6.1870e-02, -5.3667e-02,  7.3625e-01,\n",
       "           8.1508e-02,  9.7526e-01,  6.5096e-01,  9.9343e-01,  9.9920e-01,\n",
       "          -7.8051e-02,  9.9999e-01,  3.1073e-02,  8.4405e-01,  7.4274e-02,\n",
       "           9.9997e-01, -1.1269e-01,  5.5621e-02, -1.4825e-01, -2.1246e-01,\n",
       "           1.4112e-01, -8.5507e-01, -5.2159e-02,  1.7916e-02,  1.5116e-02,\n",
       "          -1.5714e-02, -9.9320e-02,  9.9613e-01, -9.9965e-01, -1.4814e-01,\n",
       "           2.8291e-02,  4.6122e-02, -9.5219e-01,  9.9858e-01,  9.9967e-01,\n",
       "          -1.0505e-01, -9.9703e-01,  9.9999e-01, -1.5598e-01,  7.6262e-02,\n",
       "          -1.1022e-01,  8.3716e-01, -9.9957e-01,  1.2537e-02,  5.0207e-02,\n",
       "           6.3648e-01, -9.9967e-01, -4.2231e-02, -4.9014e-01,  9.9757e-01,\n",
       "           5.0465e-01, -1.2264e-01, -4.0898e-02,  4.1418e-01, -8.9344e-02,\n",
       "          -6.6209e-03,  9.8296e-01, -3.2390e-01,  7.9887e-01, -2.7261e-01,\n",
       "           5.7602e-02,  5.1489e-02, -9.6220e-02, -1.2394e-01, -7.6890e-02,\n",
       "          -4.8871e-02,  1.0381e-01, -1.4993e-01,  1.4608e-02,  9.9139e-01,\n",
       "           7.7128e-01,  9.9998e-01,  5.3883e-01,  6.5420e-02,  9.9858e-01,\n",
       "          -6.3119e-02,  9.6987e-01,  9.9962e-01,  4.8095e-02, -3.4148e-02,\n",
       "           3.9952e-03,  2.9971e-02,  9.9757e-01, -6.3035e-02, -4.9495e-02,\n",
       "          -4.7513e-02, -9.9957e-01, -9.9549e-01,  9.9711e-01, -1.1209e-01,\n",
       "           9.9978e-01, -9.9942e-01,  7.8793e-01, -9.7212e-01, -2.6752e-01,\n",
       "          -4.7006e-01, -1.0329e-02, -9.5569e-01,  7.6067e-02,  9.9958e-01,\n",
       "           5.3734e-02, -9.9707e-01, -1.3085e-01, -4.5545e-02, -8.8618e-02,\n",
       "          -8.0645e-02,  6.7572e-02, -6.0044e-02,  9.8719e-01,  7.8110e-01,\n",
       "           9.9605e-01,  9.9705e-01,  2.4622e-02,  1.7112e-02,  6.2247e-01,\n",
       "          -8.6342e-02, -9.9869e-01, -1.9734e-01, -9.9638e-01,  9.9329e-01,\n",
       "           9.9973e-01, -7.2015e-02,  8.5586e-01,  9.9926e-01, -6.5049e-02,\n",
       "           6.1189e-02,  4.9807e-06,  3.3488e-02,  7.9706e-01,  1.1062e-01,\n",
       "          -6.9687e-02, -2.1262e-02, -2.2062e-02, -9.9904e-01, -9.9997e-02,\n",
       "           8.2916e-01, -1.5361e-02,  1.1009e-01, -8.5028e-01, -9.9998e-01,\n",
       "           2.3189e-02, -9.9780e-01,  1.4653e-01, -5.0057e-03,  4.1914e-02,\n",
       "           1.4962e-02,  9.9875e-01, -7.7777e-01, -1.0207e-01, -7.3238e-02,\n",
       "           3.5221e-02,  9.9572e-01, -6.4855e-02,  9.9915e-01,  5.4429e-02,\n",
       "          -9.9992e-01, -8.9059e-01, -8.5416e-02, -7.9202e-02, -7.0743e-02,\n",
       "          -1.4447e-01,  6.9743e-02, -7.9288e-02, -7.5267e-01, -9.9153e-01,\n",
       "          -8.8377e-01, -1.6638e-01,  7.6070e-02, -1.7238e-03, -3.1317e-02,\n",
       "          -1.2569e-02, -1.4297e-01,  1.2788e-01,  6.8550e-02,  9.9838e-01,\n",
       "          -9.9967e-01, -5.5087e-02, -2.8880e-02, -9.9983e-01, -9.9991e-01,\n",
       "          -3.7604e-02, -8.7311e-02,  1.8072e-01, -9.2038e-02, -8.2754e-01,\n",
       "          -1.8231e-02,  9.3290e-01,  1.0000e+00, -9.8463e-03,  1.0431e-01,\n",
       "          -9.9767e-01,  7.1793e-01,  1.1323e-01, -2.0517e-01, -1.1551e-01,\n",
       "          -8.9635e-02, -1.9698e-02,  1.0620e-01, -7.9712e-01, -3.5971e-02,\n",
       "          -1.1965e-01, -5.8992e-01, -7.0738e-03,  1.5675e-01,  3.0491e-01,\n",
       "           5.2973e-02,  6.0607e-01, -2.3261e-03,  9.9997e-01, -9.8948e-01,\n",
       "           6.0249e-01,  9.8219e-01, -9.9972e-01,  2.6173e-02,  7.8688e-01,\n",
       "          -4.2688e-02, -9.8784e-01, -8.6868e-02, -8.3183e-02, -1.4965e-02,\n",
       "          -6.5865e-02,  9.9989e-01,  1.7974e-01, -2.3204e-02, -4.7012e-02,\n",
       "          -9.9576e-01, -1.0133e-01, -7.6592e-02,  9.9995e-01,  2.6392e-01,\n",
       "           9.1935e-02, -5.2748e-02,  3.9162e-01, -9.9999e-01, -9.2548e-01,\n",
       "           3.0110e-01,  9.3031e-01, -1.0000e+00, -9.3738e-02,  9.9859e-01,\n",
       "           9.3745e-01,  8.6791e-02, -9.9782e-01, -5.6510e-02, -9.9973e-01,\n",
       "          -1.1088e-01, -7.7577e-02,  4.6137e-02, -1.0638e-01, -9.5925e-03,\n",
       "           1.1353e-01,  9.9870e-01,  9.9531e-01, -9.5235e-02,  6.4811e-02,\n",
       "          -5.8147e-02, -9.9999e-01, -9.7733e-01, -1.5641e-01,  2.9044e-02,\n",
       "          -9.9171e-01,  1.0000e+00, -9.8406e-01,  9.9213e-01,  9.9382e-01,\n",
       "           7.2879e-01,  1.3542e-01, -2.3612e-02, -9.9310e-01,  1.2175e-01,\n",
       "           9.9627e-01,  9.9814e-01, -1.0916e-01, -3.6821e-02,  9.1338e-01,\n",
       "           7.0898e-02, -4.8858e-02, -3.3904e-02,  1.1081e-01,  1.1489e-01,\n",
       "          -1.8325e-02,  5.2505e-01, -6.3451e-02, -8.9177e-01,  5.5725e-01,\n",
       "           3.8970e-02, -3.4348e-03, -4.8263e-01, -3.4680e-02, -1.9407e-02,\n",
       "          -2.4341e-02, -1.0294e-02,  4.7179e-02, -6.0350e-01,  3.2251e-02,\n",
       "           6.3256e-02,  5.5746e-01,  1.2832e-01,  8.8761e-01,  9.2837e-02,\n",
       "          -9.8832e-01,  9.9841e-01, -5.2300e-02,  3.1949e-01,  8.9366e-01,\n",
       "           9.9606e-01, -9.9873e-01,  1.9272e-02, -2.4422e-02,  5.3584e-02,\n",
       "          -1.8311e-02,  9.9999e-01, -3.1475e-01,  3.8239e-01,  2.9534e-01,\n",
       "          -1.1171e-01, -4.1112e-02, -5.4820e-03,  5.5052e-02,  1.7262e-02,\n",
       "           7.8505e-01,  2.6310e-02,  9.9297e-01, -8.1847e-01, -1.9092e-01,\n",
       "          -7.9760e-02, -6.3081e-01, -2.0655e-01, -1.5450e-01, -4.5484e-01,\n",
       "          -1.2092e-01, -7.8215e-01,  4.8089e-02, -7.2886e-01,  1.1188e-03,\n",
       "           9.9562e-01, -4.5770e-02,  8.0193e-02,  5.2071e-01,  2.1754e-02,\n",
       "           9.9979e-01, -9.9995e-01, -7.7107e-02,  9.9972e-01, -9.0216e-02,\n",
       "          -4.0343e-02,  4.9980e-02, -7.9536e-01,  6.0391e-04, -6.6126e-02,\n",
       "          -9.9350e-01,  4.4508e-02, -4.8684e-02, -6.9933e-02, -1.3290e-02,\n",
       "           1.2053e-01, -7.4670e-02,  6.4924e-01, -3.2669e-02, -1.1438e-01,\n",
       "           1.6318e-01, -2.1297e-01, -6.9683e-02, -3.5914e-02,  7.6913e-03,\n",
       "          -6.0732e-02, -1.1541e-01, -8.7056e-02, -2.3012e-02,  1.2508e-01,\n",
       "           9.8576e-01,  9.8174e-02, -9.9786e-01,  8.7379e-01, -1.4695e-01,\n",
       "          -9.8919e-01, -8.3432e-03, -9.9808e-01,  9.9998e-01,  9.9479e-01,\n",
       "           7.6077e-01,  8.3240e-01, -9.9784e-01, -9.9464e-01, -2.7026e-01,\n",
       "          -4.4546e-03, -1.4529e-01, -1.3948e-01,  8.1159e-01,  6.4781e-02,\n",
       "           7.3536e-02, -3.6994e-02,  7.1281e-02,  2.2944e-02,  8.8968e-01,\n",
       "           1.6061e-01, -9.9919e-01,  8.8122e-03,  8.2754e-01,  6.0683e-01,\n",
       "          -8.3930e-01, -2.4064e-02,  7.8445e-01,  6.7532e-02,  2.4566e-02,\n",
       "          -1.0939e-01, -1.8564e-02,  4.0944e-03, -9.7369e-01, -9.0462e-02,\n",
       "           5.6038e-02,  6.0687e-02, -1.5956e-01,  3.6398e-02,  8.9521e-01,\n",
       "          -9.1081e-01, -9.0584e-03, -7.6419e-02, -2.5560e-01, -3.9115e-01,\n",
       "          -9.4082e-01, -7.0942e-01, -8.3452e-01,  3.9847e-02,  9.0371e-02,\n",
       "          -9.9863e-01, -1.6622e-02,  9.3788e-01,  9.8580e-01,  9.9807e-01,\n",
       "          -1.1131e-01, -5.3048e-02, -5.3644e-02,  1.3914e-01, -9.9846e-01,\n",
       "           7.0160e-03,  5.8414e-02,  7.8486e-02, -4.8292e-01, -1.2630e-02,\n",
       "           2.2888e-03,  5.7185e-01, -9.9757e-01,  9.9260e-01, -9.6269e-02,\n",
       "          -1.3132e-01, -7.2042e-02, -5.9872e-02,  6.5838e-02, -3.9521e-02,\n",
       "          -9.9865e-01,  1.6254e-01,  1.0000e+00,  8.8483e-01,  9.7894e-01,\n",
       "           7.5268e-01, -7.5778e-01,  6.9640e-02, -9.8554e-03, -9.9459e-01,\n",
       "          -9.8572e-01,  1.3932e-01, -9.9540e-02, -9.9930e-01,  9.6829e-01,\n",
       "          -9.9547e-01, -5.6000e-02, -1.0101e-01,  9.9638e-01,  9.9865e-01,\n",
       "          -6.2469e-02, -9.9869e-01, -9.9930e-01, -9.0760e-01,  1.5013e-02,\n",
       "           1.4175e-02,  3.9050e-02, -2.5771e-02, -5.5303e-02, -5.0913e-02,\n",
       "           9.3617e-01,  1.7478e-01,  4.8526e-02,  6.0309e-01,  9.9990e-01,\n",
       "           4.2100e-02, -9.9038e-01,  1.4397e-01,  3.9650e-02, -2.6370e-01,\n",
       "          -1.4683e-01,  9.9643e-01, -1.6709e-01,  8.3553e-01,  9.9651e-01,\n",
       "          -9.9855e-01,  9.6510e-01, -9.9914e-01,  7.6412e-01,  9.9817e-01,\n",
       "          -9.9999e-01,  4.1770e-02, -1.0000e+00, -8.1126e-01,  1.7337e-02,\n",
       "           5.9752e-02, -7.3078e-02,  7.7393e-01, -1.0000e+00, -9.9926e-01,\n",
       "           3.6290e-03,  6.1612e-02, -9.4206e-01,  7.5761e-01,  2.0716e-02,\n",
       "           6.4048e-02, -5.2327e-03, -6.3983e-02, -1.5981e-03, -4.1222e-01,\n",
       "           9.9792e-01, -1.4425e-01,  2.1138e-01, -9.9999e-01,  9.9079e-01,\n",
       "          -9.2187e-02,  3.3791e-02,  9.9998e-01, -1.1917e-01,  5.2204e-02,\n",
       "           1.7005e-02, -9.9996e-01,  6.1772e-02, -1.6253e-01, -8.7191e-01,\n",
       "           7.0377e-01, -7.6792e-02,  9.0425e-02,  9.0670e-02,  1.6666e-02,\n",
       "          -9.6135e-01, -6.4403e-02, -9.9995e-01,  9.9994e-01, -9.0112e-01,\n",
       "          -4.0432e-02, -4.8710e-02,  6.7989e-02, -9.1326e-03,  9.7381e-01,\n",
       "           1.0000e+00, -9.9494e-01,  1.2121e-01,  9.9998e-01,  6.3021e-02,\n",
       "          -4.4897e-02, -9.9983e-01, -4.4879e-02, -5.4723e-01,  5.4066e-02,\n",
       "           9.9999e-01,  2.3438e-02, -2.5710e-02,  9.9741e-01, -1.0000e+00,\n",
       "          -6.5084e-01, -2.5196e-02, -5.5039e-02,  5.4633e-02, -9.9999e-01,\n",
       "           9.3102e-02, -9.1971e-01,  7.7814e-02, -9.9996e-01,  9.7233e-01,\n",
       "          -1.0000e+00, -1.0105e-02,  9.9989e-01,  9.2200e-01,  9.9812e-01,\n",
       "           6.2143e-02, -1.3369e-01, -1.0210e-01, -9.9966e-01,  4.5653e-01,\n",
       "           1.1652e-01, -1.9417e-02, -9.6215e-01, -8.8894e-01, -5.1232e-02,\n",
       "           7.1592e-01, -9.6707e-01,  1.0438e-01, -4.2600e-01, -1.2263e-01,\n",
       "           1.8545e-02,  1.6106e-02,  1.6669e-01, -7.0950e-01,  6.4330e-02,\n",
       "           6.3150e-02,  1.5816e-01,  1.5262e-01, -1.4789e-01,  1.5076e-02,\n",
       "           9.8578e-01, -9.9999e-01,  9.1424e-02, -9.9897e-01,  1.8609e-02,\n",
       "           2.3453e-02,  5.1781e-03,  9.0442e-02, -1.5452e-01,  2.5748e-01,\n",
       "          -9.9719e-01,  1.0000e+00, -9.9994e-01, -9.9987e-01,  9.9992e-01,\n",
       "           2.4003e-02, -9.5249e-01, -2.2341e-04, -8.3380e-03, -6.8064e-02,\n",
       "           4.7028e-02,  1.2258e-01,  6.6469e-02,  1.5224e-01, -9.6727e-01,\n",
       "           3.8303e-01,  2.5273e-02,  1.2062e-01, -2.0217e-02,  9.8453e-02,\n",
       "          -9.7559e-01,  9.9999e-01,  9.9999e-01,  9.9763e-01, -9.9946e-01,\n",
       "          -1.1182e-01, -3.4374e-02,  9.9994e-01, -2.5505e-02, -1.0448e-01,\n",
       "           9.8577e-01,  9.8166e-01,  5.9839e-02, -8.5727e-01,  1.6110e-02,\n",
       "          -1.0237e-01, -1.0129e-01,  2.5532e-02, -8.5224e-01, -9.9693e-01,\n",
       "          -4.1437e-02, -9.9879e-01, -9.7294e-01,  9.6899e-01, -6.9776e-02,\n",
       "           9.9999e-01,  5.8274e-02,  1.1026e-01,  4.2234e-03, -3.2762e-01,\n",
       "          -9.9680e-01,  1.6555e-01, -9.9112e-01,  3.1410e-03, -9.9957e-01,\n",
       "          -9.9996e-01, -5.3201e-03, -1.2501e-01, -9.8989e-01,  2.2696e-01,\n",
       "          -4.0473e-02, -9.9142e-01, -8.5995e-01, -9.9742e-01,  5.0619e-01,\n",
       "          -7.8801e-01, -9.1654e-01,  7.8494e-03,  6.9026e-01,  3.9386e-02,\n",
       "           6.9726e-03, -3.8604e-01, -2.7448e-01,  6.5068e-01,  4.8398e-02,\n",
       "          -2.8399e-02, -1.9780e-02, -9.9525e-01,  2.2583e-01, -9.9998e-01,\n",
       "           9.8975e-01, -9.9999e-01,  6.1415e-02,  9.9997e-01,  9.9997e-01,\n",
       "           2.3447e-02, -9.9967e-01,  9.9756e-01, -1.4974e-01,  9.9999e-01,\n",
       "          -7.4224e-02, -9.9565e-01, -9.9969e-01,  5.6973e-02, -5.0304e-02,\n",
       "           9.9999e-01, -7.7253e-02,  8.8873e-01, -6.6773e-02,  5.2994e-03,\n",
       "          -5.2309e-02,  3.6770e-02,  1.5533e-02, -8.6250e-03, -4.4843e-02,\n",
       "           9.8435e-01,  1.0945e-02,  9.9999e-01]], grad_fn=<TanhBackward>))"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biobert(torch.tensor([encoded_text]), attention_mask = torch.tensor([attention]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "length = 126\n",
    "attention = [1]* length\n",
    "attention.extend([0] * (max_length - length))\n",
    "len(attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'attention' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-125-f2e50d503396>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mattention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'attention' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "    # Training Loop\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = train(model, train_loader, criterion, device, optimizer)\n",
    "        val_loss, acc, m_acc, macro_f1, class_f1 = test(model, test_loader, criterion, device, n_way)\n",
    "\n",
    "        if (save_models):\n",
    "            torch.save(model.state_dict(), os.path.join(path_models, f'basic_{epoch + 1}.pth'))  # Save the model\n",
    "\n",
    "        # Append and report results\n",
    "        df_results.loc[epoch] = [epoch + 1, train_loss, val_loss, acc, m_acc, macro_f1] + class_f1\n",
    "        print(\n",
    "            f'[{epoch + 1}] t_loss: {train_loss:.5f} v_loss: {val_loss:.5f} val_acc: {acc:.5f} '\n",
    "            f'val_m_acc: {m_acc:.5f} f1: {macro_f1:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables\n",
    "num_epochs = 5\n",
    "num_workers = 3\n",
    "bs = 64\n",
    "n_way = 3\n",
    "root_image = '../../../../scratch/rl80/mimic-cxr-jpg-2.0.0.physionet.org/files'\n",
    "root_text = '../../../../scratch/rl80/mimic-cxr-2.0.0.physionet.org'\n",
    "path_splits = '../splits/20_shot.csv'\n",
    "path_biobert = './'\n",
    "path_pretrained = '../results/basic/basic_36.pth' # Pretrained image model\n",
    "\n",
    "# Check for GPU device\n",
    "torch.cuda.set_device(0)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load in model\n",
    "model = MultiModalNet(n_way, path_biobert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline.block1.0.weight\n",
      "baseline.block1.0.bias\n",
      "baseline.block1.1.weight\n",
      "baseline.block1.1.bias\n",
      "baseline.block2.0.weight\n",
      "baseline.block2.0.bias\n",
      "baseline.block2.1.weight\n",
      "baseline.block2.1.bias\n",
      "baseline.block3.0.weight\n",
      "baseline.block3.0.bias\n",
      "baseline.block3.1.weight\n",
      "baseline.block3.1.bias\n",
      "baseline.block4.0.weight\n",
      "baseline.block4.0.bias\n",
      "baseline.block4.1.weight\n",
      "baseline.block4.1.bias\n",
      "baseline.linear.weight\n",
      "baseline.linear.bias\n",
      "biobert.embeddings.word_embeddings.weight\n",
      "biobert.embeddings.position_embeddings.weight\n",
      "biobert.embeddings.token_type_embeddings.weight\n",
      "biobert.embeddings.LayerNorm.weight\n",
      "biobert.embeddings.LayerNorm.bias\n",
      "biobert.encoder.layer.0.attention.self.query.weight\n",
      "biobert.encoder.layer.0.attention.self.query.bias\n",
      "biobert.encoder.layer.0.attention.self.key.weight\n",
      "biobert.encoder.layer.0.attention.self.key.bias\n",
      "biobert.encoder.layer.0.attention.self.value.weight\n",
      "biobert.encoder.layer.0.attention.self.value.bias\n",
      "biobert.encoder.layer.0.attention.output.dense.weight\n",
      "biobert.encoder.layer.0.attention.output.dense.bias\n",
      "biobert.encoder.layer.0.attention.output.LayerNorm.weight\n",
      "biobert.encoder.layer.0.attention.output.LayerNorm.bias\n",
      "biobert.encoder.layer.0.intermediate.dense.weight\n",
      "biobert.encoder.layer.0.intermediate.dense.bias\n",
      "biobert.encoder.layer.0.output.dense.weight\n",
      "biobert.encoder.layer.0.output.dense.bias\n",
      "biobert.encoder.layer.0.output.LayerNorm.weight\n",
      "biobert.encoder.layer.0.output.LayerNorm.bias\n",
      "biobert.encoder.layer.1.attention.self.query.weight\n",
      "biobert.encoder.layer.1.attention.self.query.bias\n",
      "biobert.encoder.layer.1.attention.self.key.weight\n",
      "biobert.encoder.layer.1.attention.self.key.bias\n",
      "biobert.encoder.layer.1.attention.self.value.weight\n",
      "biobert.encoder.layer.1.attention.self.value.bias\n",
      "biobert.encoder.layer.1.attention.output.dense.weight\n",
      "biobert.encoder.layer.1.attention.output.dense.bias\n",
      "biobert.encoder.layer.1.attention.output.LayerNorm.weight\n",
      "biobert.encoder.layer.1.attention.output.LayerNorm.bias\n",
      "biobert.encoder.layer.1.intermediate.dense.weight\n",
      "biobert.encoder.layer.1.intermediate.dense.bias\n",
      "biobert.encoder.layer.1.output.dense.weight\n",
      "biobert.encoder.layer.1.output.dense.bias\n",
      "biobert.encoder.layer.1.output.LayerNorm.weight\n",
      "biobert.encoder.layer.1.output.LayerNorm.bias\n",
      "biobert.encoder.layer.2.attention.self.query.weight\n",
      "biobert.encoder.layer.2.attention.self.query.bias\n",
      "biobert.encoder.layer.2.attention.self.key.weight\n",
      "biobert.encoder.layer.2.attention.self.key.bias\n",
      "biobert.encoder.layer.2.attention.self.value.weight\n",
      "biobert.encoder.layer.2.attention.self.value.bias\n",
      "biobert.encoder.layer.2.attention.output.dense.weight\n",
      "biobert.encoder.layer.2.attention.output.dense.bias\n",
      "biobert.encoder.layer.2.attention.output.LayerNorm.weight\n",
      "biobert.encoder.layer.2.attention.output.LayerNorm.bias\n",
      "biobert.encoder.layer.2.intermediate.dense.weight\n",
      "biobert.encoder.layer.2.intermediate.dense.bias\n",
      "biobert.encoder.layer.2.output.dense.weight\n",
      "biobert.encoder.layer.2.output.dense.bias\n",
      "biobert.encoder.layer.2.output.LayerNorm.weight\n",
      "biobert.encoder.layer.2.output.LayerNorm.bias\n",
      "biobert.encoder.layer.3.attention.self.query.weight\n",
      "biobert.encoder.layer.3.attention.self.query.bias\n",
      "biobert.encoder.layer.3.attention.self.key.weight\n",
      "biobert.encoder.layer.3.attention.self.key.bias\n",
      "biobert.encoder.layer.3.attention.self.value.weight\n",
      "biobert.encoder.layer.3.attention.self.value.bias\n",
      "biobert.encoder.layer.3.attention.output.dense.weight\n",
      "biobert.encoder.layer.3.attention.output.dense.bias\n",
      "biobert.encoder.layer.3.attention.output.LayerNorm.weight\n",
      "biobert.encoder.layer.3.attention.output.LayerNorm.bias\n",
      "biobert.encoder.layer.3.intermediate.dense.weight\n",
      "biobert.encoder.layer.3.intermediate.dense.bias\n",
      "biobert.encoder.layer.3.output.dense.weight\n",
      "biobert.encoder.layer.3.output.dense.bias\n",
      "biobert.encoder.layer.3.output.LayerNorm.weight\n",
      "biobert.encoder.layer.3.output.LayerNorm.bias\n",
      "biobert.encoder.layer.4.attention.self.query.weight\n",
      "biobert.encoder.layer.4.attention.self.query.bias\n",
      "biobert.encoder.layer.4.attention.self.key.weight\n",
      "biobert.encoder.layer.4.attention.self.key.bias\n",
      "biobert.encoder.layer.4.attention.self.value.weight\n",
      "biobert.encoder.layer.4.attention.self.value.bias\n",
      "biobert.encoder.layer.4.attention.output.dense.weight\n",
      "biobert.encoder.layer.4.attention.output.dense.bias\n",
      "biobert.encoder.layer.4.attention.output.LayerNorm.weight\n",
      "biobert.encoder.layer.4.attention.output.LayerNorm.bias\n",
      "biobert.encoder.layer.4.intermediate.dense.weight\n",
      "biobert.encoder.layer.4.intermediate.dense.bias\n",
      "biobert.encoder.layer.4.output.dense.weight\n",
      "biobert.encoder.layer.4.output.dense.bias\n",
      "biobert.encoder.layer.4.output.LayerNorm.weight\n",
      "biobert.encoder.layer.4.output.LayerNorm.bias\n",
      "biobert.encoder.layer.5.attention.self.query.weight\n",
      "biobert.encoder.layer.5.attention.self.query.bias\n",
      "biobert.encoder.layer.5.attention.self.key.weight\n",
      "biobert.encoder.layer.5.attention.self.key.bias\n",
      "biobert.encoder.layer.5.attention.self.value.weight\n",
      "biobert.encoder.layer.5.attention.self.value.bias\n",
      "biobert.encoder.layer.5.attention.output.dense.weight\n",
      "biobert.encoder.layer.5.attention.output.dense.bias\n",
      "biobert.encoder.layer.5.attention.output.LayerNorm.weight\n",
      "biobert.encoder.layer.5.attention.output.LayerNorm.bias\n",
      "biobert.encoder.layer.5.intermediate.dense.weight\n",
      "biobert.encoder.layer.5.intermediate.dense.bias\n",
      "biobert.encoder.layer.5.output.dense.weight\n",
      "biobert.encoder.layer.5.output.dense.bias\n",
      "biobert.encoder.layer.5.output.LayerNorm.weight\n",
      "biobert.encoder.layer.5.output.LayerNorm.bias\n",
      "biobert.encoder.layer.6.attention.self.query.weight\n",
      "biobert.encoder.layer.6.attention.self.query.bias\n",
      "biobert.encoder.layer.6.attention.self.key.weight\n",
      "biobert.encoder.layer.6.attention.self.key.bias\n",
      "biobert.encoder.layer.6.attention.self.value.weight\n",
      "biobert.encoder.layer.6.attention.self.value.bias\n",
      "biobert.encoder.layer.6.attention.output.dense.weight\n",
      "biobert.encoder.layer.6.attention.output.dense.bias\n",
      "biobert.encoder.layer.6.attention.output.LayerNorm.weight\n",
      "biobert.encoder.layer.6.attention.output.LayerNorm.bias\n",
      "biobert.encoder.layer.6.intermediate.dense.weight\n",
      "biobert.encoder.layer.6.intermediate.dense.bias\n",
      "biobert.encoder.layer.6.output.dense.weight\n",
      "biobert.encoder.layer.6.output.dense.bias\n",
      "biobert.encoder.layer.6.output.LayerNorm.weight\n",
      "biobert.encoder.layer.6.output.LayerNorm.bias\n",
      "biobert.encoder.layer.7.attention.self.query.weight\n",
      "biobert.encoder.layer.7.attention.self.query.bias\n",
      "biobert.encoder.layer.7.attention.self.key.weight\n",
      "biobert.encoder.layer.7.attention.self.key.bias\n",
      "biobert.encoder.layer.7.attention.self.value.weight\n",
      "biobert.encoder.layer.7.attention.self.value.bias\n",
      "biobert.encoder.layer.7.attention.output.dense.weight\n",
      "biobert.encoder.layer.7.attention.output.dense.bias\n",
      "biobert.encoder.layer.7.attention.output.LayerNorm.weight\n",
      "biobert.encoder.layer.7.attention.output.LayerNorm.bias\n",
      "biobert.encoder.layer.7.intermediate.dense.weight\n",
      "biobert.encoder.layer.7.intermediate.dense.bias\n",
      "biobert.encoder.layer.7.output.dense.weight\n",
      "biobert.encoder.layer.7.output.dense.bias\n",
      "biobert.encoder.layer.7.output.LayerNorm.weight\n",
      "biobert.encoder.layer.7.output.LayerNorm.bias\n",
      "biobert.encoder.layer.8.attention.self.query.weight\n",
      "biobert.encoder.layer.8.attention.self.query.bias\n",
      "biobert.encoder.layer.8.attention.self.key.weight\n",
      "biobert.encoder.layer.8.attention.self.key.bias\n",
      "biobert.encoder.layer.8.attention.self.value.weight\n",
      "biobert.encoder.layer.8.attention.self.value.bias\n",
      "biobert.encoder.layer.8.attention.output.dense.weight\n",
      "biobert.encoder.layer.8.attention.output.dense.bias\n",
      "biobert.encoder.layer.8.attention.output.LayerNorm.weight\n",
      "biobert.encoder.layer.8.attention.output.LayerNorm.bias\n",
      "biobert.encoder.layer.8.intermediate.dense.weight\n",
      "biobert.encoder.layer.8.intermediate.dense.bias\n",
      "biobert.encoder.layer.8.output.dense.weight\n",
      "biobert.encoder.layer.8.output.dense.bias\n",
      "biobert.encoder.layer.8.output.LayerNorm.weight\n",
      "biobert.encoder.layer.8.output.LayerNorm.bias\n",
      "biobert.encoder.layer.9.attention.self.query.weight\n",
      "biobert.encoder.layer.9.attention.self.query.bias\n",
      "biobert.encoder.layer.9.attention.self.key.weight\n",
      "biobert.encoder.layer.9.attention.self.key.bias\n",
      "biobert.encoder.layer.9.attention.self.value.weight\n",
      "biobert.encoder.layer.9.attention.self.value.bias\n",
      "biobert.encoder.layer.9.attention.output.dense.weight\n",
      "biobert.encoder.layer.9.attention.output.dense.bias\n",
      "biobert.encoder.layer.9.attention.output.LayerNorm.weight\n",
      "biobert.encoder.layer.9.attention.output.LayerNorm.bias\n",
      "biobert.encoder.layer.9.intermediate.dense.weight\n",
      "biobert.encoder.layer.9.intermediate.dense.bias\n",
      "biobert.encoder.layer.9.output.dense.weight\n",
      "biobert.encoder.layer.9.output.dense.bias\n",
      "biobert.encoder.layer.9.output.LayerNorm.weight\n",
      "biobert.encoder.layer.9.output.LayerNorm.bias\n",
      "biobert.encoder.layer.10.attention.self.query.weight\n",
      "biobert.encoder.layer.10.attention.self.query.bias\n",
      "biobert.encoder.layer.10.attention.self.key.weight\n",
      "biobert.encoder.layer.10.attention.self.key.bias\n",
      "biobert.encoder.layer.10.attention.self.value.weight\n",
      "biobert.encoder.layer.10.attention.self.value.bias\n",
      "biobert.encoder.layer.10.attention.output.dense.weight\n",
      "biobert.encoder.layer.10.attention.output.dense.bias\n",
      "biobert.encoder.layer.10.attention.output.LayerNorm.weight\n",
      "biobert.encoder.layer.10.attention.output.LayerNorm.bias\n",
      "biobert.encoder.layer.10.intermediate.dense.weight\n",
      "biobert.encoder.layer.10.intermediate.dense.bias\n",
      "biobert.encoder.layer.10.output.dense.weight\n",
      "biobert.encoder.layer.10.output.dense.bias\n",
      "biobert.encoder.layer.10.output.LayerNorm.weight\n",
      "biobert.encoder.layer.10.output.LayerNorm.bias\n",
      "biobert.encoder.layer.11.attention.self.query.weight\n",
      "biobert.encoder.layer.11.attention.self.query.bias\n",
      "biobert.encoder.layer.11.attention.self.key.weight\n",
      "biobert.encoder.layer.11.attention.self.key.bias\n",
      "biobert.encoder.layer.11.attention.self.value.weight\n",
      "biobert.encoder.layer.11.attention.self.value.bias\n",
      "biobert.encoder.layer.11.attention.output.dense.weight\n",
      "biobert.encoder.layer.11.attention.output.dense.bias\n",
      "biobert.encoder.layer.11.attention.output.LayerNorm.weight\n",
      "biobert.encoder.layer.11.attention.output.LayerNorm.bias\n",
      "biobert.encoder.layer.11.intermediate.dense.weight\n",
      "biobert.encoder.layer.11.intermediate.dense.bias\n",
      "biobert.encoder.layer.11.output.dense.weight\n",
      "biobert.encoder.layer.11.output.dense.bias\n",
      "biobert.encoder.layer.11.output.LayerNorm.weight\n",
      "biobert.encoder.layer.11.output.LayerNorm.bias\n",
      "biobert.pooler.dense.weight\n",
      "biobert.pooler.dense.bias\n",
      "concat_linear.weight\n",
      "concat_linear.bias\n"
     ]
    }
   ],
   "source": [
    "for key in model.named_parameters():\n",
    "    print(key[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'baseline.block1.0.weight'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode_plus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables\n",
    "num_epochs = 5\n",
    "num_workers = 3\n",
    "bs = 64\n",
    "n_way = 3\n",
    "root_image = '../../../../scratch/rl80/mimic-cxr-jpg-2.0.0.physionet.org/files'\n",
    "root_text = '../../../../scratch/rl80/mimic-cxr-2.0.0.physionet.org'\n",
    "path_splits = '../splits/20_shot.csv'\n",
    "path_biobert = ',/'\n",
    "path_pretrained = '../results/basic/basic_36.pth' # Pretrained image model\n",
    "\n",
    "# Check for GPU device\n",
    "torch.cuda.set_device(0)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load in model\n",
    "model = MultiModalNet(n_way, path_biobert)\n",
    "pretrained_dict = torch.load(path_pretrained)\n",
    "#model_dict = model.state_dict()\n",
    "#model_dict.update(pretrained_dict)\n",
    "#model.load_state_dict(model_dict)\n",
    "model.load_state_dict(torch.load(path_pretrained))\n",
    "\n",
    "# Load in model and dataset\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "tokenizer = get_tokenizer()\n",
    "train_dataset = MimicCxrMulti(root_image, root_text, csv_path, tokenizer, mode='novel_train')\n",
    "train_loader = DataLoader(train_dataset, batch_size=bs, shuffle=True, num_workers=num_workers)\n",
    "\n",
    "\n",
    "#biobert = get_biobert(model_dir=None, download=True)\n",
    "#biobert = get_biobert(model_dir='./', download=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
