{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.transforms import transforms\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "class MimicCxrReportsEpisodes(Dataset):\n",
    "    \"\"\"\n",
    "    MIMIC-CXR Reports Only\n",
    "    Todo: Insert references to the database here!\n",
    "    Removes '_' from reports\n",
    "    Truncates the reports to 512 tokens by removing the beginning of the report (Usually where the 'wet read' resides)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, root_text, csv_path, tokenizer, n_way, k_shot, k_query, num_episodes, mode, max_length=512):\n",
    "\n",
    "        # Check if mode contains an accepted value\n",
    "        if mode not in ('base', 'novel'):\n",
    "            raise Exception(\"Selected 'mode' is not valid\")\n",
    "\n",
    "        # Initialise variables\n",
    "        self.root_text = root_text\n",
    "        self.max_length = max_length\n",
    "        self.tokenizer = tokenizer\n",
    "        self.n_way = n_way\n",
    "        self.k_shot = k_shot\n",
    "        self.k_query = k_query\n",
    "        self.num_episodes = num_episodes\n",
    "        \n",
    "        # Load data\n",
    "        csv_data = pd.read_csv(csv_path)\n",
    "\n",
    "        if mode == 'base':\n",
    "            self.dict_labels = {\n",
    "                'Atelectasis': 0,\n",
    "                'Cardiomegaly': 1,\n",
    "                'Consolidation': 2,\n",
    "                'Edema': 3,\n",
    "                'No Finding': 4,\n",
    "                'Pneumonia': 5,\n",
    "            }\n",
    "            # Filters for novel classes\n",
    "            data = csv_data[(csv_data.split == \"base_train\") | (csv_data.split == \"base_validate\")]\n",
    "\n",
    "        else:\n",
    "            self.dict_labels = {\n",
    "                'Enlarged Cardiomediastinum': 0,\n",
    "                'Fracture': 1,\n",
    "                'Lung Lesion': 2,\n",
    "                'Lung Opacity': 3,\n",
    "                'Pleural Effusion': 4,\n",
    "                'Pneumothorax': 5\n",
    "            }\n",
    "            data = csv_data[csv_data.split == \"novel\"]  # Filters for novel classes\n",
    "            \n",
    "        # Converts classes to numeric values\n",
    "        self.data = data.assign(labels=data[\"labels\"].apply(lambda x: self.dict_labels[x]))\n",
    "            \n",
    "        # Create Episodes\n",
    "        self.support_episodes = []  # List of training episodes (support set)\n",
    "        self.query_episodes = []  # List of testing episodes (query set)\n",
    "        for i in range(self.num_episodes):  # for each batch\n",
    "            # 1.select n_way classes randomly\n",
    "            selected_cls = np.random.choice(len(self.dict_labels), self.n_way, False)  # no duplicate\n",
    "            np.random.shuffle(selected_cls)\n",
    "            df_support = pd.DataFrame()\n",
    "            df_query = pd.DataFrame()\n",
    "            for cls in selected_cls:\n",
    "                df_cls = self.data[self.data.labels == cls]\n",
    "                # 2. select k_shot + k_query for each class\n",
    "                selected_idx = np.random.choice(len(df_cls), self.k_shot + self.k_query, False)\n",
    "                np.random.shuffle(selected_idx)\n",
    "\n",
    "                # Index of samples for the support and query set\n",
    "                support_idx = selected_idx[:self.k_shot]\n",
    "                query_idx = selected_idx[self.k_shot:]\n",
    "\n",
    "                df_support = df_support.append(df_cls.iloc[support_idx])\n",
    "                df_query = df_query.append(df_cls.iloc[query_idx])\n",
    "\n",
    "            # Shuffle the indexes so that it is no longer ordered by class\n",
    "            df_support = df_support.sample(frac=1)\n",
    "            df_query = df_query.sample(frac=1)\n",
    "\n",
    "            self.support_episodes.append(df_support)\n",
    "            self.query_episodes.append(df_query)\n",
    "            \n",
    "    def __len__(self):\n",
    "        return self.num_episodes\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get a single episode\n",
    "        support_set = self.support_episodes[idx]\n",
    "        query_set = self.query_episodes[idx]\n",
    "        \n",
    "        # Labels ranging from 0 to (number of classes -1)\n",
    "        support_labels = support_set.labels.tolist()\n",
    "        query_labels = query_set.labels.tolist()\n",
    "\n",
    "        # Convert labels to range from 0 to (n way-1) for loss calculation\n",
    "        unique_labels = np.unique(support_labels)  # Unique labels are the same for support and query set\n",
    "        converted_support_labels = support_labels\n",
    "        converted_query_labels = query_labels\n",
    "        for idx, val in enumerate(unique_labels):\n",
    "            # Get indexes of labels that are equal to the iterated val\n",
    "            idx_support = [x for x, label in enumerate(support_labels) if label == val]\n",
    "            idx_query = [x for x, label in enumerate(query_labels) if label == val]\n",
    "\n",
    "            # Replace old labels with new labels\n",
    "            for idx_change in range(len(idx_support)):\n",
    "                converted_support_labels[idx_support[idx_change]] = idx\n",
    "\n",
    "            for idx_change in range(len(idx_query)):\n",
    "                converted_query_labels[idx_query[idx_change]] = idx\n",
    "        \n",
    "        # Get the support set of texts and masks as tensors\n",
    "        support_texts = torch.Tensor()\n",
    "        support_masks = torch.Tensor()\n",
    "        for i in range(len(support_set)):\n",
    "            # Extract CSV data\n",
    "            file_path = support_set.iloc[idx, 0]\n",
    "\n",
    "            # Get text tensor and attention mask\n",
    "            text_name = f'{file_path.split(\"/\")[2]}.txt'  # Extract the study id to find the report\n",
    "            text_path = Path(os.path.join(self.root_text, text_name))\n",
    "            plain_text = text_path.read_text()\n",
    "            plain_text = plain_text.replace('_', '')  # Remove all underscores from the text\n",
    "            encoded_text = self.tokenizer.encode(plain_text, add_special_tokens=True)\n",
    "            len_encoding = len(encoded_text)\n",
    "            \n",
    "            # Transform encodings to be of the same size\n",
    "            if len_encoding > self.max_length:\n",
    "                # Truncate to max length\n",
    "                cutoff = len_encoding - self.max_length + 1  # The cutoff for the tokens to be deleted\n",
    "                del encoded_text[1:cutoff]\n",
    "                attention = [1] * self.max_length\n",
    "            elif len_encoding < self.max_length:\n",
    "                # Pad to max length\n",
    "                num_padding = self.max_length - len_encoding\n",
    "                encoded_text.extend([0] * num_padding)  # Padding token is 0\n",
    "                attention = [1] * len_encoding\n",
    "                attention.extend([0] * (self.max_length - len_encoding))\n",
    "            else:\n",
    "                # If equal size, create attention matrix\n",
    "                attention = [1] * self.max_length\n",
    "                \n",
    "            # Append texts and attention masks to the tensor to be outputted\n",
    "            torch.cat((support_texts, torch.tensor(encoded_text)))\n",
    "            torch.cat((support_masks, torch.tensor(attention)))\n",
    "        \n",
    "        # Get the query set of texts and masks as tensors\n",
    "        query_texts = torch.Tensor()\n",
    "        query_masks = torch.Tensor()\n",
    "        for i in range(len(query_set)):\n",
    "            # Extract CSV data\n",
    "            file_path = query_set.iloc[idx, 0]\n",
    "\n",
    "            # Get text tensor and attention mask\n",
    "            text_name = f'{file_path.split(\"/\")[2]}.txt'  # Extract the study id to find the report\n",
    "            text_path = Path(os.path.join(self.root_text, text_name))\n",
    "            plain_text = text_path.read_text()\n",
    "            plain_text = plain_text.replace('_', '')  # Remove all underscores from the text\n",
    "            encoded_text = self.tokenizer.encode(plain_text, add_special_tokens=True)\n",
    "            len_encoding = len(encoded_text)\n",
    "            \n",
    "            # Transform encodings to be of the same size\n",
    "            if len_encoding > self.max_length:\n",
    "                # Truncate to max length\n",
    "                cutoff = len_encoding - self.max_length + 1  # The cutoff for the tokens to be deleted\n",
    "                del encoded_text[1:cutoff]\n",
    "                attention = [1] * self.max_length\n",
    "            elif len_encoding < self.max_length:\n",
    "                # Pad to max length\n",
    "                num_padding = self.max_length - len_encoding\n",
    "                encoded_text.extend([0] * num_padding)  # Padding token is 0\n",
    "                attention = [1] * len_encoding\n",
    "                attention.extend([0] * (self.max_length - len_encoding))\n",
    "            else:\n",
    "                # If equal size, create attention matrix\n",
    "                attention = [1] * self.max_length\n",
    "                \n",
    "            # Append texts and attention masks to the tensor to be outputted\n",
    "            torch.cat((query_texts, torch.Tensor(encoded_text)))\n",
    "            torch.cat((query_masks, torch.tensor(attention)))\n",
    "\n",
    "        return support_texts, support_masks, torch.LongTensor(support_labels), \\\n",
    "                    query_texts, query_masks, torch.LongTensor(query_labels)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Caught TypeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/ilu3/destinationPath/lib/python3.6/site-packages/torch/utils/data/_utils/worker.py\", line 185, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/ilu3/destinationPath/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/ilu3/destinationPath/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"<ipython-input-1-34be384eb770>\", line 152, in __getitem__\n    torch.cat((support_texts, encoded_text))\nTypeError: expected Tensor as element 1 in argument 0, but got list\n",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-83ebe706db23>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-83ebe706db23>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0;31m# Iterate through batched episodes. One episode is one experiment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msupport_imgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msupport_masks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msupport_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery_imgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery_masks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery_labels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m         \u001b[0;31m# Convert Tensors to appropriate device\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0mbatch_support_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_support_masks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_support_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_query_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_query_masks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_query_y\u001b[0m \u001b[0;34m=\u001b[0m             \u001b[0msupport_imgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msupport_masks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msupport_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m             \u001b[0mquery_imgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery_masks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ilu3/destinationPath/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ilu3/destinationPath/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    987\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 989\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    990\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    991\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ilu3/destinationPath/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1012\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1015\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ilu3/destinationPath/lib/python3.6/site-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    393\u001b[0m             \u001b[0;31m# (https://bugs.python.org/issue2651), so we work around it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKeyErrorMessage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 395\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: Caught TypeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/ilu3/destinationPath/lib/python3.6/site-packages/torch/utils/data/_utils/worker.py\", line 185, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/ilu3/destinationPath/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/ilu3/destinationPath/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"<ipython-input-1-34be384eb770>\", line 152, in __getitem__\n    torch.cat((support_texts, encoded_text))\nTypeError: expected Tensor as element 1 in argument 0, but got list\n"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from biobertology import get_tokenizer\n",
    "import sys\n",
    "\n",
    "sys.path.append('..')\n",
    "from shared.models import *\n",
    "# from shared.datasets import *\n",
    "from shared.metrics import *\n",
    "\n",
    "\n",
    "def train(text_inputs, attention_inputs, labels, model, criterion, device, optimizer, freeze=False):\n",
    "    # Training loop\n",
    "    model.train()\n",
    "\n",
    "    # Freeze all layers except those indicated\n",
    "    if freeze:\n",
    "        for name, param in model.named_parameters():\n",
    "            if name not in freeze:\n",
    "                param.requires_grad = False\n",
    "\n",
    "    # Train the entire support set in one batch\n",
    "    optimizer.zero_grad()\n",
    "    pred = model(text_inputs, attention_inputs)\n",
    "    loss = criterion(pred, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    train_loss = loss.item()  # Running training loss\n",
    "\n",
    "    return train_loss\n",
    "\n",
    "\n",
    "def test(text_inputs, attention_inputs, labels, model, criterion, device, n_way):\n",
    "    # An F1 Score of 0 indicates that it is invalid\n",
    "    model.eval()\n",
    "    true_positive = list(0. for i in range(n_way))  # Number of correctly predicted samples per class\n",
    "    total_truth = list(0. for i in range(n_way))  # Number of ground truths per class\n",
    "    predicted_positive = list(0. for i in range(n_way))  # Number of predicted samples per class\n",
    "    correct_total = 0  # Total correctly predicted samples\n",
    "    total = 0  # Total samples\n",
    "    with torch.no_grad():\n",
    "        # Test the entire query set in one batch\n",
    "        pred = model(text_inputs, attention_inputs)\n",
    "        loss = criterion(pred, labels)\n",
    "        val_loss = loss.item()  # Running validation loss\n",
    "        _, predicted = torch.max(pred, 1)\n",
    "        correct = (predicted == labels).squeeze()  # Samples that are correctly predicted\n",
    "        correct_total += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "        for i in range(len(predicted)):\n",
    "            label = labels[i]\n",
    "            true_positive[label] += correct[i].item()\n",
    "            total_truth[label] += 1\n",
    "            predicted_positive[predicted[i].item()] += 1  # True Positive + False Positive\n",
    "\n",
    "    accuracy, macro_accuracy, f1_score, class_f1 = metrics(true_positive, total_truth,\n",
    "                                                           predicted_positive, correct_total, total)\n",
    "\n",
    "    return val_loss, accuracy, macro_accuracy, f1_score, class_f1\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Set Training Parameters\n",
    "    n_way = 3\n",
    "    k_shot = 20\n",
    "    k_query = 16\n",
    "    num_episodes = 20\n",
    "    num_epochs = 20\n",
    "    num_workers = 12\n",
    "    bs = 4\n",
    "    lr = 1e-4\n",
    "    root_text = '../../../../scratch/rl80/mimic-cxr-2.0.0.physionet.org'\n",
    "    path_biobert = '../results'\n",
    "    path_splits = '../splits/splits.csv'  # Location of preprocessed splits\n",
    "    path_results = '../../results'  # Folder to save the CSV results\n",
    "    freeze = ['linear.weight', 'linear.bias']  # Freeze all layers except linear layers\n",
    "\n",
    "    torch.cuda.set_device(0)\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Training tools\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    tokenizer = get_tokenizer()\n",
    "\n",
    "    # Load in data\n",
    "    dataset = MimicCxrReportsEpisodes(root_text, path_splits, tokenizer, n_way, k_shot, k_query, num_episodes, 'novel')\n",
    "    loader = DataLoader(dataset, batch_size=bs, shuffle=True, num_workers=num_workers)\n",
    "\n",
    "    # Create Dataframe to export results to CSV\n",
    "    df_results = pd.DataFrame(columns=['Epoch', 'Training Loss', 'Validation Loss', 'Accuracy', 'Macro Accuracy',\n",
    "                                       'Macro-F1 Score'] + [str(x) + ' F1' for x in range(n_way)])\n",
    "\n",
    "    # Iterate through batched episodes. One episode is one experiment\n",
    "    for step, (support_imgs, support_masks, support_labels, query_imgs, query_masks, query_labels) in enumerate(loader):\n",
    "        # Convert Tensors to appropriate device\n",
    "        batch_support_x, batch_support_masks, batch_support_y, batch_query_x, batch_query_masks, batch_query_y = \\\n",
    "            support_imgs.to(device), support_masks.to(device), support_labels.to(device), \\\n",
    "            query_imgs.to(device), query_masks.to(device), query_labels.to(device)\n",
    "\n",
    "        # [num_batch, training_sz, channels, height, width] = support_x.size()\n",
    "        # num_batch = num of episodes\n",
    "        # training_sz = size of support or query set\n",
    "        num_batch = batch_support_x.size(0) # Number of episodes in the batch\n",
    "\n",
    "        # Break down the batch of episodes into single episodes\n",
    "        for i in range(num_batch):\n",
    "            # Load in model and reset weights every episode/experiment\n",
    "            model = SemanticNet(n_way, path_biobert).to(device)\n",
    "\n",
    "            # Reset optimizer with model parameters\n",
    "            optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "            # Break down the sets into individual episodes\n",
    "            support_x, support_y = batch_support_x[i], batch_support_y[i]\n",
    "            query_x, query_y = batch_query_x[i], batch_query_y[i]\n",
    "\n",
    "            # Variables for best epoch per experiment\n",
    "            best_score = 0\n",
    "            best_epoch = 0\n",
    "            df_best = pd.DataFrame(columns=['Epoch', 'Training Loss', 'Validation Loss', 'Accuracy', 'Macro Accuracy',\n",
    "                                       'Macro-F1 Score'] + [str(x) + ' F1' for x in range(n_way)]) # Track best epoch\n",
    "            # Training and testing for specified epochs\n",
    "            for epoch in range(num_epochs):\n",
    "                # Training\n",
    "                train_loss = train(support_x, support_y, model, criterion, device, optimizer, freeze=freeze)\n",
    "\n",
    "                # Testing\n",
    "                val_loss, acc, m_acc, macro_f1, class_f1 = test(query_x, query_y, model, criterion, device, n_way)\n",
    "\n",
    "                # Find best epoch\n",
    "                score = 0.5*acc + 0.5*macro_f1\n",
    "                if score > best_score:\n",
    "                    best_score = score\n",
    "                    df_best.loc[0] = [epoch + 1, train_loss, val_loss, acc, m_acc, macro_f1] + class_f1\n",
    "\n",
    "            # Print the best results per experiment\n",
    "            print(\n",
    "                f'[{int(df_best.iloc[0,0])}] t_loss: {df_best.iloc[0,1]} v_loss: {df_best.iloc[0,2]} '\n",
    "                f'val_acc: {df_best.iloc[0,3]} f1: {df_best.iloc[0,5]}')\n",
    "\n",
    "            # Record the best epoch to be saved into a CSV\n",
    "            df_results = df_results.append(df_best.loc[0], ignore_index=True)\n",
    "\n",
    "    # Create results folder if it does not exist\n",
    "    if not os.path.exists(path_results):\n",
    "        os.makedirs(path_results)\n",
    "\n",
    "    # Export results to a CSV file\n",
    "    df_results.to_csv(os.path.join(path_results, f'{k_shot}shot_semantic.csv'), index=False)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1000])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b, c, d, e, f, g = \\\n",
    "1, 2, 3, 4, 5, 6, 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
