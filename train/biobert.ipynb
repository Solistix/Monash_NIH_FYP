{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] t_loss: 1.10848 v_loss: 1.09739 val_acc: 0.31250 val_m_acc: 0.31250 f1: 0.00000\n",
      "[2] t_loss: 1.10093 v_loss: 1.09857 val_acc: 0.35417 val_m_acc: 0.35417 f1: 0.00000\n",
      "[3] t_loss: 1.10490 v_loss: 1.09632 val_acc: 0.41667 val_m_acc: 0.41667 f1: 0.00000\n",
      "[4] t_loss: 1.08619 v_loss: 1.09937 val_acc: 0.37500 val_m_acc: 0.37500 f1: 0.25428\n",
      "[5] t_loss: 1.07951 v_loss: 1.10369 val_acc: 0.33333 val_m_acc: 0.33333 f1: 0.27403\n",
      "[6] t_loss: 1.07948 v_loss: 1.10590 val_acc: 0.33333 val_m_acc: 0.33333 f1: 0.26903\n",
      "[7] t_loss: 1.07405 v_loss: 1.10673 val_acc: 0.31250 val_m_acc: 0.31250 f1: 0.26413\n",
      "[8] t_loss: 1.07881 v_loss: 1.10916 val_acc: 0.27083 val_m_acc: 0.27083 f1: 0.20168\n",
      "[9] t_loss: 1.06343 v_loss: 1.11292 val_acc: 0.33333 val_m_acc: 0.33333 f1: 0.00000\n",
      "[10] t_loss: 1.05573 v_loss: 1.11611 val_acc: 0.35417 val_m_acc: 0.35417 f1: 0.00000\n",
      "[11] t_loss: 1.05835 v_loss: 1.11555 val_acc: 0.31250 val_m_acc: 0.31250 f1: 0.00000\n",
      "[12] t_loss: 1.04984 v_loss: 1.11379 val_acc: 0.33333 val_m_acc: 0.33333 f1: 0.00000\n",
      "[13] t_loss: 1.05731 v_loss: 1.11255 val_acc: 0.45833 val_m_acc: 0.45833 f1: 0.45143\n",
      "[14] t_loss: 1.04109 v_loss: 1.11310 val_acc: 0.39583 val_m_acc: 0.39583 f1: 0.38930\n",
      "[15] t_loss: 1.03185 v_loss: 1.11328 val_acc: 0.37500 val_m_acc: 0.37500 f1: 0.36718\n",
      "[16] t_loss: 1.03487 v_loss: 1.11256 val_acc: 0.39583 val_m_acc: 0.39583 f1: 0.38620\n",
      "[17] t_loss: 1.03541 v_loss: 1.11103 val_acc: 0.41667 val_m_acc: 0.41667 f1: 0.41590\n",
      "[18] t_loss: 1.02372 v_loss: 1.11006 val_acc: 0.47917 val_m_acc: 0.47917 f1: 0.47558\n",
      "[19] t_loss: 1.01783 v_loss: 1.10925 val_acc: 0.39583 val_m_acc: 0.39583 f1: 0.38164\n",
      "[20] t_loss: 1.01679 v_loss: 1.10742 val_acc: 0.39583 val_m_acc: 0.39583 f1: 0.38164\n",
      "[21] t_loss: 1.01810 v_loss: 1.10419 val_acc: 0.45833 val_m_acc: 0.45833 f1: 0.45319\n",
      "[22] t_loss: 1.00393 v_loss: 1.10065 val_acc: 0.50000 val_m_acc: 0.50000 f1: 0.48935\n",
      "[23] t_loss: 1.00128 v_loss: 1.09799 val_acc: 0.47917 val_m_acc: 0.47917 f1: 0.45308\n",
      "[24] t_loss: 1.01078 v_loss: 1.09553 val_acc: 0.45833 val_m_acc: 0.45833 f1: 0.43767\n",
      "[25] t_loss: 0.99589 v_loss: 1.09296 val_acc: 0.39583 val_m_acc: 0.39583 f1: 0.38342\n",
      "[26] t_loss: 0.99282 v_loss: 1.09068 val_acc: 0.41667 val_m_acc: 0.41667 f1: 0.40217\n",
      "[27] t_loss: 0.98410 v_loss: 1.08995 val_acc: 0.43750 val_m_acc: 0.43750 f1: 0.42253\n",
      "[28] t_loss: 0.99149 v_loss: 1.08933 val_acc: 0.52083 val_m_acc: 0.52083 f1: 0.51490\n",
      "[29] t_loss: 0.97065 v_loss: 1.08860 val_acc: 0.52083 val_m_acc: 0.52083 f1: 0.50970\n",
      "[30] t_loss: 0.97833 v_loss: 1.08762 val_acc: 0.54167 val_m_acc: 0.54167 f1: 0.53112\n",
      "[31] t_loss: 0.97171 v_loss: 1.08662 val_acc: 0.50000 val_m_acc: 0.50000 f1: 0.47976\n",
      "[32] t_loss: 0.97269 v_loss: 1.08654 val_acc: 0.47917 val_m_acc: 0.47917 f1: 0.46313\n",
      "[33] t_loss: 0.96179 v_loss: 1.08681 val_acc: 0.45833 val_m_acc: 0.45833 f1: 0.43608\n",
      "[34] t_loss: 0.97080 v_loss: 1.08685 val_acc: 0.45833 val_m_acc: 0.45833 f1: 0.43608\n",
      "[35] t_loss: 0.96317 v_loss: 1.08657 val_acc: 0.45833 val_m_acc: 0.45833 f1: 0.43608\n",
      "[36] t_loss: 0.94389 v_loss: 1.08635 val_acc: 0.47917 val_m_acc: 0.47917 f1: 0.46313\n",
      "[37] t_loss: 0.95130 v_loss: 1.08611 val_acc: 0.47917 val_m_acc: 0.47917 f1: 0.46313\n",
      "[38] t_loss: 0.93780 v_loss: 1.08630 val_acc: 0.47917 val_m_acc: 0.47917 f1: 0.46515\n",
      "[39] t_loss: 0.95216 v_loss: 1.08611 val_acc: 0.47917 val_m_acc: 0.47917 f1: 0.46515\n",
      "[40] t_loss: 0.93118 v_loss: 1.08608 val_acc: 0.43750 val_m_acc: 0.43750 f1: 0.43089\n",
      "[41] t_loss: 0.94599 v_loss: 1.08563 val_acc: 0.47917 val_m_acc: 0.47917 f1: 0.46515\n",
      "[42] t_loss: 0.93004 v_loss: 1.08492 val_acc: 0.47917 val_m_acc: 0.47917 f1: 0.46515\n",
      "[43] t_loss: 0.93650 v_loss: 1.08499 val_acc: 0.50000 val_m_acc: 0.50000 f1: 0.47976\n",
      "[44] t_loss: 0.93146 v_loss: 1.08474 val_acc: 0.47917 val_m_acc: 0.47917 f1: 0.45823\n",
      "[45] t_loss: 0.92925 v_loss: 1.08439 val_acc: 0.50000 val_m_acc: 0.50000 f1: 0.47976\n",
      "[46] t_loss: 0.91207 v_loss: 1.08402 val_acc: 0.47917 val_m_acc: 0.47917 f1: 0.46515\n",
      "[47] t_loss: 0.91852 v_loss: 1.08427 val_acc: 0.45833 val_m_acc: 0.45833 f1: 0.45261\n",
      "[48] t_loss: 0.91034 v_loss: 1.08508 val_acc: 0.45833 val_m_acc: 0.45833 f1: 0.45261\n",
      "[49] t_loss: 0.90251 v_loss: 1.08534 val_acc: 0.45833 val_m_acc: 0.45833 f1: 0.45124\n",
      "[50] t_loss: 0.90096 v_loss: 1.08612 val_acc: 0.47917 val_m_acc: 0.47917 f1: 0.46515\n",
      "[51] t_loss: 0.91346 v_loss: 1.08628 val_acc: 0.52083 val_m_acc: 0.52083 f1: 0.50794\n",
      "[52] t_loss: 0.90563 v_loss: 1.08562 val_acc: 0.52083 val_m_acc: 0.52083 f1: 0.50794\n",
      "[53] t_loss: 0.91446 v_loss: 1.08509 val_acc: 0.50000 val_m_acc: 0.50000 f1: 0.49019\n",
      "[54] t_loss: 0.90874 v_loss: 1.08373 val_acc: 0.47917 val_m_acc: 0.47917 f1: 0.46515\n",
      "[55] t_loss: 0.90668 v_loss: 1.08241 val_acc: 0.47917 val_m_acc: 0.47917 f1: 0.46515\n",
      "[56] t_loss: 0.89444 v_loss: 1.08191 val_acc: 0.45833 val_m_acc: 0.45833 f1: 0.45261\n",
      "[57] t_loss: 0.85629 v_loss: 1.08164 val_acc: 0.45833 val_m_acc: 0.45833 f1: 0.45261\n",
      "[58] t_loss: 0.88985 v_loss: 1.08027 val_acc: 0.47917 val_m_acc: 0.47917 f1: 0.46313\n",
      "[59] t_loss: 0.87393 v_loss: 1.07909 val_acc: 0.47917 val_m_acc: 0.47917 f1: 0.46313\n",
      "[60] t_loss: 0.85946 v_loss: 1.07828 val_acc: 0.52083 val_m_acc: 0.52083 f1: 0.50634\n",
      "[61] t_loss: 0.86730 v_loss: 1.07601 val_acc: 0.52083 val_m_acc: 0.52083 f1: 0.50634\n",
      "[62] t_loss: 0.86521 v_loss: 1.07358 val_acc: 0.50000 val_m_acc: 0.50000 f1: 0.49019\n",
      "[63] t_loss: 0.86519 v_loss: 1.07224 val_acc: 0.50000 val_m_acc: 0.50000 f1: 0.49470\n",
      "[64] t_loss: 0.87787 v_loss: 1.07165 val_acc: 0.47917 val_m_acc: 0.47917 f1: 0.47710\n",
      "[65] t_loss: 0.87587 v_loss: 1.07085 val_acc: 0.50000 val_m_acc: 0.50000 f1: 0.49019\n",
      "[66] t_loss: 0.85743 v_loss: 1.07100 val_acc: 0.50000 val_m_acc: 0.50000 f1: 0.48878\n",
      "[67] t_loss: 0.84427 v_loss: 1.07052 val_acc: 0.52083 val_m_acc: 0.52083 f1: 0.50634\n",
      "[68] t_loss: 0.86596 v_loss: 1.06895 val_acc: 0.52083 val_m_acc: 0.52083 f1: 0.50838\n",
      "[69] t_loss: 0.84020 v_loss: 1.06842 val_acc: 0.52083 val_m_acc: 0.52083 f1: 0.51197\n",
      "[70] t_loss: 0.86503 v_loss: 1.06771 val_acc: 0.47917 val_m_acc: 0.47917 f1: 0.47694\n",
      "[71] t_loss: 0.87222 v_loss: 1.06663 val_acc: 0.52083 val_m_acc: 0.52083 f1: 0.51270\n",
      "[72] t_loss: 0.84595 v_loss: 1.06581 val_acc: 0.52083 val_m_acc: 0.52083 f1: 0.51270\n",
      "[73] t_loss: 0.83429 v_loss: 1.06619 val_acc: 0.52083 val_m_acc: 0.52083 f1: 0.51270\n",
      "[74] t_loss: 0.84055 v_loss: 1.06635 val_acc: 0.50000 val_m_acc: 0.50000 f1: 0.49160\n",
      "[75] t_loss: 0.84230 v_loss: 1.06744 val_acc: 0.52083 val_m_acc: 0.52083 f1: 0.51270\n",
      "[76] t_loss: 0.83627 v_loss: 1.06762 val_acc: 0.52083 val_m_acc: 0.52083 f1: 0.51270\n",
      "[77] t_loss: 0.84843 v_loss: 1.06751 val_acc: 0.50000 val_m_acc: 0.50000 f1: 0.48765\n",
      "[78] t_loss: 0.83398 v_loss: 1.06713 val_acc: 0.52083 val_m_acc: 0.52083 f1: 0.51270\n",
      "[79] t_loss: 0.80249 v_loss: 1.06663 val_acc: 0.52083 val_m_acc: 0.52083 f1: 0.50794\n",
      "[80] t_loss: 0.84690 v_loss: 1.06492 val_acc: 0.52083 val_m_acc: 0.52083 f1: 0.51270\n",
      "[81] t_loss: 0.81685 v_loss: 1.06466 val_acc: 0.52083 val_m_acc: 0.52083 f1: 0.51270\n",
      "[82] t_loss: 0.84217 v_loss: 1.06431 val_acc: 0.52083 val_m_acc: 0.52083 f1: 0.51270\n",
      "[83] t_loss: 0.83290 v_loss: 1.06308 val_acc: 0.54167 val_m_acc: 0.54167 f1: 0.53086\n",
      "[84] t_loss: 0.78767 v_loss: 1.06145 val_acc: 0.54167 val_m_acc: 0.54167 f1: 0.53081\n",
      "[85] t_loss: 0.82009 v_loss: 1.06047 val_acc: 0.54167 val_m_acc: 0.54167 f1: 0.53081\n",
      "[86] t_loss: 0.83243 v_loss: 1.05943 val_acc: 0.54167 val_m_acc: 0.54167 f1: 0.53081\n",
      "[87] t_loss: 0.81633 v_loss: 1.05803 val_acc: 0.54167 val_m_acc: 0.54167 f1: 0.53081\n",
      "[88] t_loss: 0.80082 v_loss: 1.05777 val_acc: 0.56250 val_m_acc: 0.56250 f1: 0.54923\n",
      "[89] t_loss: 0.79570 v_loss: 1.05852 val_acc: 0.56250 val_m_acc: 0.56250 f1: 0.54923\n",
      "[90] t_loss: 0.78894 v_loss: 1.05819 val_acc: 0.56250 val_m_acc: 0.56250 f1: 0.54923\n",
      "[91] t_loss: 0.79719 v_loss: 1.05782 val_acc: 0.56250 val_m_acc: 0.56250 f1: 0.54923\n",
      "[92] t_loss: 0.79682 v_loss: 1.05628 val_acc: 0.56250 val_m_acc: 0.56250 f1: 0.54923\n",
      "[93] t_loss: 0.77375 v_loss: 1.05532 val_acc: 0.56250 val_m_acc: 0.56250 f1: 0.54923\n",
      "[94] t_loss: 0.81624 v_loss: 1.05500 val_acc: 0.56250 val_m_acc: 0.56250 f1: 0.54923\n",
      "[95] t_loss: 0.79389 v_loss: 1.05505 val_acc: 0.56250 val_m_acc: 0.56250 f1: 0.54923\n",
      "[96] t_loss: 0.75243 v_loss: 1.05526 val_acc: 0.56250 val_m_acc: 0.56250 f1: 0.54923\n",
      "[97] t_loss: 0.80640 v_loss: 1.05426 val_acc: 0.56250 val_m_acc: 0.56250 f1: 0.54923\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[98] t_loss: 0.76456 v_loss: 1.05394 val_acc: 0.56250 val_m_acc: 0.56250 f1: 0.54923\n",
      "[99] t_loss: 0.79496 v_loss: 1.05225 val_acc: 0.56250 val_m_acc: 0.56250 f1: 0.54923\n",
      "[100] t_loss: 0.75829 v_loss: 1.05100 val_acc: 0.54167 val_m_acc: 0.54167 f1: 0.53086\n",
      "[88] t_loss: 0.8008180260658264 v_loss: 1.057766318321228 val_acc: 0.5625 f1: 0.549227098246706\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from biobertology import get_tokenizer\n",
    "import sys\n",
    "\n",
    "sys.path.append('..')\n",
    "from shared.models import *\n",
    "from shared.datasets import *\n",
    "from shared.metrics import *\n",
    "\n",
    "\n",
    "def train(text_inputs, attention_inputs, labels, model, criterion, device, optimizer, freeze=False):\n",
    "    # Training loop\n",
    "    model.train()\n",
    "\n",
    "    # Freeze all layers except those indicated\n",
    "    if freeze:\n",
    "        for name, param in model.named_parameters():\n",
    "            if name not in freeze:\n",
    "                param.requires_grad = False\n",
    "    \n",
    "    # Train the entire support set in one batch\n",
    "    optimizer.zero_grad()\n",
    "    pred = model(text_inputs, attention_inputs)\n",
    "    loss = criterion(pred, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    train_loss = loss.item()  # Running training loss\n",
    "\n",
    "    return train_loss\n",
    "\n",
    "\n",
    "def test(text_inputs, attention_inputs, labels, model, criterion, device, n_way):\n",
    "    # An F1 Score of 0 indicates that it is invalid\n",
    "    model.eval()\n",
    "    true_positive = list(0. for i in range(n_way))  # Number of correctly predicted samples per class\n",
    "    total_truth = list(0. for i in range(n_way))  # Number of ground truths per class\n",
    "    predicted_positive = list(0. for i in range(n_way))  # Number of predicted samples per class\n",
    "    correct_total = 0  # Total correctly predicted samples\n",
    "    total = 0  # Total samples\n",
    "    with torch.no_grad():\n",
    "        # Test the entire query set in one batch\n",
    "        pred = model(text_inputs, attention_inputs)\n",
    "        loss = criterion(pred, labels)\n",
    "        val_loss = loss.item()  # Running validation loss\n",
    "        _, predicted = torch.max(pred, 1)\n",
    "        correct = (predicted == labels).squeeze()  # Samples that are correctly predicted\n",
    "        correct_total += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "        for i in range(len(predicted)):\n",
    "            label = labels[i]\n",
    "            true_positive[label] += correct[i].item()\n",
    "            total_truth[label] += 1\n",
    "            predicted_positive[predicted[i].item()] += 1  # True Positive + False Positive\n",
    "\n",
    "    accuracy, macro_accuracy, f1_score, class_f1 = metrics(true_positive, total_truth,\n",
    "                                                           predicted_positive, correct_total, total)\n",
    "\n",
    "    return val_loss, accuracy, macro_accuracy, f1_score, class_f1\n",
    "\n",
    "\n",
    "def main(k_shot):\n",
    "    # Set Training Parameters\n",
    "    n_way = 3\n",
    "    k_query = 16\n",
    "    num_episodes = 200\n",
    "    num_epochs = 100\n",
    "    num_workers = 12\n",
    "    bs = 4\n",
    "    lr = 1e-3\n",
    "    root_text = '../../../../scratch/rl80/mimic-cxr-2.0.0.physionet.org'\n",
    "    path_biobert = '../results'\n",
    "    path_splits = '../splits/splits.csv'  # Location of preprocessed splits\n",
    "    path_results = '../../results'  # Folder to save the CSV results\n",
    "    freeze = ['linear.weight', 'linear.bias']  # Freeze all layers except linear layers\n",
    "\n",
    "    torch.cuda.set_device(0)\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Training tools\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    tokenizer = get_tokenizer()\n",
    "\n",
    "    # Load in data\n",
    "    dataset = MimicCxrReportsEpisodes(root_text, path_splits, tokenizer, n_way, k_shot, k_query, num_episodes, 'novel')\n",
    "    loader = DataLoader(dataset, batch_size=bs, shuffle=True, num_workers=num_workers)\n",
    "\n",
    "    # Create Dataframe to export results to CSV\n",
    "    df_results = pd.DataFrame(columns=['Epoch', 'Training Loss', 'Validation Loss', 'Accuracy', 'Macro Accuracy',\n",
    "                                       'Macro-F1 Score'] + [str(x) + ' F1' for x in range(n_way)])\n",
    "\n",
    "    # Iterate through batched episodes. One episode is one experiment\n",
    "    for step, (support_texts, support_masks, support_labels, query_texts, query_masks, query_labels) in enumerate(loader):\n",
    "        # Convert Tensors to appropriate device\n",
    "        batch_support_x, batch_support_masks, batch_support_y, batch_query_x, batch_query_masks, batch_query_y = \\\n",
    "            support_texts.to(device), support_masks.to(device), support_labels.to(device), \\\n",
    "            query_texts.to(device), query_masks.to(device), query_labels.to(device)\n",
    "\n",
    "        # [num_batch, training_sz, channels, height, width] = support_x.size()\n",
    "        # num_batch = num of episodes\n",
    "        # training_sz = size of support or query set\n",
    "        num_batch = batch_support_x.size(0) # Number of episodes in the batch\n",
    "\n",
    "        # Break down the batch of episodes into single episodes\n",
    "        for i in range(num_batch):\n",
    "            # Load in model and reset weights every episode/experiment\n",
    "            model = SemanticNet(n_way, path_biobert).to(device)\n",
    "\n",
    "            # Reset optimizer with model parameters\n",
    "            optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "            # Break down the sets into individual episodes\n",
    "            support_x, support_m, support_y, query_x, query_m, query_y = \\\n",
    "                batch_support_x[i], batch_support_masks[i], batch_support_y[i], \\\n",
    "                batch_query_x[i], batch_query_masks[i], batch_query_y[i]\n",
    "\n",
    "            # Variables for best epoch per experiment\n",
    "            best_score = 0\n",
    "            best_epoch = 0\n",
    "            df_best = pd.DataFrame(columns=['Epoch', 'Training Loss', 'Validation Loss', 'Accuracy', 'Macro Accuracy',\n",
    "                                       'Macro-F1 Score'] + [str(x) + ' F1' for x in range(n_way)]) # Track best epoch\n",
    "            # Training and testing for specified epochs\n",
    "            for epoch in range(num_epochs):\n",
    "                # Training\n",
    "                train_loss = train(support_x, support_m, support_y, model, criterion, device, optimizer, freeze=freeze)\n",
    "\n",
    "                # Testing\n",
    "                val_loss, acc, m_acc, macro_f1, class_f1 = test(query_x, query_m, query_y, \n",
    "                                                                model, criterion, device, n_way)\n",
    "\n",
    "                # Find best epoch\n",
    "                score = 0.5*acc + 0.5*macro_f1\n",
    "                if score > best_score:\n",
    "                    best_score = score\n",
    "                    df_best.loc[0] = [epoch + 1, train_loss, val_loss, acc, m_acc, macro_f1] + class_f1\n",
    "\n",
    "            # Print the best results per experiment\n",
    "            print(\n",
    "                f'[{int(df_best.iloc[0,0])}] t_loss: {df_best.iloc[0,1]} v_loss: {df_best.iloc[0,2]} '\n",
    "                f'val_acc: {df_best.iloc[0,3]} f1: {df_best.iloc[0,5]}')\n",
    "\n",
    "            # Record the best epoch to be saved into a CSV\n",
    "            df_results = df_results.append(df_best.loc[0], ignore_index=True)\n",
    "\n",
    "    # Create results folder if it does not exist\n",
    "    if not os.path.exists(path_results):\n",
    "        os.makedirs(path_results)\n",
    "\n",
    "    # Export results to a CSV file\n",
    "    df_results.to_csv(os.path.join(path_results, f'{k_shot}shot_semantic.csv'), index=False)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print(f'Semantic Training {sys.argv[1]} shot')\n",
    "    main(int(sys.argv[1]))  # Get the k_shot variable from command line\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 2, 1, 2, 2, 2, 1, 2, 1, 2, 0, 0, 1, 2, 0, 1, 0, 1, 2, 1, 1, 1, 0, 1,\n",
       "        2, 2, 0, 2, 1, 2, 1, 2, 0, 2, 0, 0, 1, 0, 0, 2, 2, 1, 0, 1, 1, 1, 0, 0,\n",
       "        0, 1, 2, 2, 0, 0, 2, 2, 0, 0, 1, 1], device='cuda:0')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "support_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Training Parameters\n",
    "n_way = 3\n",
    "k_shot = 20\n",
    "k_query = 16\n",
    "num_episodes = 1\n",
    "num_epochs = 100\n",
    "num_workers = 12\n",
    "bs = 4\n",
    "lr = 1e-4\n",
    "root_text = '../../../../scratch/rl80/mimic-cxr-2.0.0.physionet.org'\n",
    "path_biobert = '../results'\n",
    "path_splits = '../splits/splits.csv'  # Location of preprocessed splits\n",
    "path_results = '../../results'  # Folder to save the CSV results\n",
    "freeze = ['linear.weight', 'linear.bias']  # Freeze all layers except linear layers\n",
    "\n",
    "torch.cuda.set_device(0)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = SemanticNet(n_way, path_biobert).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(param.requires_grad)\n",
    "    #if name not in freeze:\n",
    "       # param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if name not in freeze:\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([     0,      0,      0,      0,      0,      0,      0,      0,     -3,\n",
       "         -3068,  -7118,   9538,  10158,  24674, -11989,   9606,   8162,   1825,\n",
       "        -21697,   -952,   7262,   9040,  13203,  -6195,   7949, -27337,  11089,\n",
       "         -7603,  11225,  17944,  -1187, -10866,    109, -10552,  -3106,     10,\n",
       "           913,   1962,     71, -16649, -10305,  -8244,  -5153, -10187,  26469,\n",
       "          4644,   9736, -20112,  15853,   4911, -16257,   1089,    971,  -1111,\n",
       "          5880,  -7596,    632,   4732,  -4522,  -1940,   -990,  -2861,  23868,\n",
       "         22643,    -85, -14060,   -967,   3143,  20785, -12290, -18698,  17616,\n",
       "         11215,  -4258,   5144,    395,  -1102,  -6471,    816,  15837,  11377,\n",
       "           367,  -6402,  10859,  24935,  12810,  -9243,  15523,     36,   -128,\n",
       "          2870,  -5135,    995,   1032,  -3447,  -3513,   -500,    946,  -1018,\n",
       "        -21423,  -3819,  11492,  -2825,   2982,   3353,   5874,  -2092, -23861,\n",
       "         -6688,  -7049,  -8281,  11366,  23024,     -3,    459,  -2869,  -8892,\n",
       "            75,  17243,   1336,   -458,     94,    862,   5206, -11078,   1467,\n",
       "        -11770,   3965,      2,    -29,  24824,  25929, -14158, -18160,   1010,\n",
       "         13456,    928,   -410, -10373,   1896,  12140, -16161,     19,   4567,\n",
       "          6997,   -704, -10087, -25514,   1127,   -214,  14258, -11217,   -900,\n",
       "          1288,  16781, -10520, -22154, -15709,   7930,   3791,   -150,  -6057,\n",
       "          1167,   2959,  12816,  -7637, -11160,  -9257,    987,   -199,  -8737,\n",
       "         -2487,  -1113, -13224, -12393,  -7467,   1058, -21728,  21338,   4065,\n",
       "         -5783,   2652, -24051, -25382,  -5761,  -4568,  -1381,  24585,   5294,\n",
       "          -260,  -4666,     -3,    -33,     50,   6086,   4841,  -1023, -10802,\n",
       "          7181,   1091,  13178,   1174,    117,   1231,  11192,   5800,   1880,\n",
       "          1114,   4892,   2229,  16899,    152,   9046,  16899,   1126,  10712,\n",
       "          8944,   1110,   1106,   1129,   1737,   1191,   7300,   1193,   4668,\n",
       "           119,    102,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
