{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import transforms\n",
    "import torch.optim as optim\n",
    "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification\n",
    "\n",
    "\n",
    "def test(model, test_loader, criterion, device, n_way):\n",
    "    # An F1 Score of 0 indicates that it is invalid\n",
    "    model.eval()\n",
    "    true_positive = list(0. for i in range(n_way))  # Number of correctly predicted samples per class\n",
    "    total_truth = list(0. for i in range(n_way))  # Number of ground truths per class\n",
    "    predicted_positive = list(0. for i in range(n_way))  # Number of predicted samples per class\n",
    "    precision = list(0. for i in range(n_way))\n",
    "    recall = list(0. for i in range(n_way))\n",
    "    class_f1 = list(0. for i in range(n_way))\n",
    "    val_loss = 0\n",
    "    correct_total = 0  # Total correctly predicted samples\n",
    "    total = 0  # Total samples\n",
    "    f1_flag = 0  # Flag for invalid F1 score\n",
    "    with torch.no_grad():\n",
    "        for step, (data_inputs, data_labels) in enumerate(test_loader):\n",
    "            inputs, labels = data_inputs.to(device), data_labels.to(device)\n",
    "            pred = model(inputs)\n",
    "            loss = criterion(pred, labels)\n",
    "            val_loss += loss.item()  # Running validation loss\n",
    "            _, predicted = torch.max(pred, 1)\n",
    "            correct = (predicted == labels).squeeze()  # Samples that are correctly predicted\n",
    "            correct_total += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "            for i in range(len(predicted)):\n",
    "                label = labels[i]\n",
    "                true_positive[label] += correct[i].item()\n",
    "                total_truth[label] += 1\n",
    "                predicted_positive[predicted[i].item()] += 1  # True Positive + False Positive\n",
    "\n",
    "        # Find class accuracy, precision and recall\n",
    "        for j in range(n_way):\n",
    "            if (predicted_positive[j] != 0 and true_positive[j] != 0):  # Check if F1 score is valid\n",
    "                precision[j] = true_positive[j] / predicted_positive[j]\n",
    "                recall[j] = true_positive[j] / total_truth[j]  # Recall is the same as per class accuracy\n",
    "                class_f1[j] = 2 * precision[j] * recall[j] / (precision[j] + recall[j])\n",
    "            else:\n",
    "                f1_flag = 1\n",
    "\n",
    "        # Find Accuracy, Macro Accuracy and Macro F1 Score\n",
    "        macro_acc_sum = 0\n",
    "        f1_sum = 0\n",
    "        for k in range(n_way):\n",
    "            macro_acc_sum += recall[k]\n",
    "            if f1_flag == 0:  # Check for invalid f1 score\n",
    "                f1_sum += class_f1[k]\n",
    "\n",
    "        accuracy = correct_total / total\n",
    "        macro_accuracy = macro_acc_sum / n_way\n",
    "        f1_score = f1_sum / n_way\n",
    "\n",
    "    return val_loss / (step+1), accuracy, macro_accuracy, f1_score, class_f1\n",
    "\n",
    "class MimicCxrReports(Dataset):\n",
    "    \"\"\"\n",
    "    MIMIC-CXR Database, Reports Only\n",
    "    Todo: Insert references to the database here!\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, root, csv_path, tokenizer, mode):\n",
    "        \n",
    "        # Check if mode contains an accepted value\n",
    "        if mode not in ('base_train', 'base_validate', 'novel_train', 'novel_validate'):\n",
    "            raise Exception(\"Selected 'mode' is not valid\")\n",
    "            \n",
    "        self.root = root\n",
    "        csv_data = pd.read_csv(csv_path)\n",
    "        csv_data = csv_data[csv_data.split == mode]\n",
    "        \n",
    "        if mode == 'base_train' or mode == 'base_validate':\n",
    "            dict_labels = {\n",
    "                'Atelectasis': 0,\n",
    "                'Cardiomegaly': 1,\n",
    "                'Consolidation': 2,\n",
    "                'Edema': 3,\n",
    "                'Fracture': 4,\n",
    "                'Lung Opacity': 5,\n",
    "                'No Finding': 6,\n",
    "                'Pneumonia': 7,\n",
    "                'Pneumothorax': 8,\n",
    "                'Support Devices': 9\n",
    "            }\n",
    "        else:\n",
    "            dict_labels = {\n",
    "                'Enlarged Cardiomediastinum': 0,\n",
    "                'Lung Lesion': 1,\n",
    "                'Pleural Effusion': 2,\n",
    "            }\n",
    "            \n",
    "            # Get text encodings and labels\n",
    "            texts = []\n",
    "            labels = []\n",
    "            for index, row in csv_data.iterrows():\n",
    "                text_name = f'{row[\"file_path\"].split(\"/\")[2]}.txt' # Only the study id is required to find the report\n",
    "                text_path = Path(os.path.join(self.root, text_name))\n",
    "                texts.append(text_path.read_text())\n",
    "                labels.append(dict_labels[row['labels']])\n",
    "            self.labels = labels\n",
    "            self.encodings = tokenizer(texts, truncation=True, padding=True)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "root='../../../../scratch/rl80/mimic-cxr-2.0.0.physionet.org'\n",
    "csv_path = '../splits/20_shot.csv'\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "train_dataset = MimicCxrReports(root, csv_path, tokenizer, mode='novel_train')\n",
    "test_dataset = MimicCxrReports(root, csv_path, tokenizer, mode='novel_validate')\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=3)\n",
    "model.to(device)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=True)\n",
    "#optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "for epoch in range(3):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for step, batch in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs[0]\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()  # Running training loss\n",
    "#model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'loss'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-05c7d67f0a9c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'loss'"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "outputs.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 FINAL REPORT\n",
      " HISTORY:  Evaluate for the pneumothorax, pigtail catheter connected to\n",
      " Pleurovac now with leak.\n",
      " \n",
      " COMPARISON:  ___ at 7:14.\n",
      " \n",
      " FINDINGS:\n",
      " \n",
      " The left pigtail catheter, right chest port and AICD leads are in unchanged\n",
      " position.  A lucency along the left mediastinum could represent medial\n",
      " pneumothorax, not significantly changed from earlier exam.  Otherwise, no\n",
      " significant change in bilateral pleural effusions.  No focal consolidation is\n",
      " present.  No evidence of pulmonary vascular congestion.\n",
      " \n",
      " IMPRESSION:\n",
      " \n",
      " Lucency along the left mediastinum could represent medial pneumothorax, not\n",
      " significantly changed from earlier radiograph.  Otherwise, no significant\n",
      " change from prior radiographs.\n",
      " \n",
      " NOTIFICATION:  Findings discussed with Dr. ___ by Dr. ___ at 14:30\n",
      " on ___.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "texts = []\n",
    "labels = []\n",
    "for index, row in csv_data.iterrows():\n",
    "    text_name = f'{row[\"file_path\"].split(\"/\")[2]}.txt' # Only the study id is required to find the report\n",
    "    text_path = Path(os.path.join(root, text_name))\n",
    "    texts.append(text_path.read_text())\n",
    "    #labels.append(dict_labels[row['labels']])\n",
    "print(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
