{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.transforms import transforms\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "class MimicCxrReportsEpisodes(Dataset):\n",
    "    \"\"\"\n",
    "    MIMIC-CXR Reports Only\n",
    "    Todo: Insert references to the database here!\n",
    "    Removes '_' from reports\n",
    "    Truncates the reports to 512 tokens by removing the beginning of the report (Usually where the 'wet read' resides)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, root_text, csv_path, tokenizer, n_way, k_shot, k_query, num_episodes, mode, max_length=512):\n",
    "\n",
    "        # Check if mode contains an accepted value\n",
    "        if mode not in ('base', 'novel'):\n",
    "            raise Exception(\"Selected 'mode' is not valid\")\n",
    "\n",
    "        # Initialise variables\n",
    "        self.root_text = root_text\n",
    "        self.max_length = max_length\n",
    "        self.tokenizer = tokenizer\n",
    "        self.n_way = n_way\n",
    "        self.k_shot = k_shot\n",
    "        self.k_query = k_query\n",
    "        self.num_episodes = num_episodes\n",
    "        \n",
    "        # Load data\n",
    "        csv_data = pd.read_csv(csv_path)\n",
    "\n",
    "        if mode == 'base':\n",
    "            self.dict_labels = {\n",
    "                'Atelectasis': 0,\n",
    "                'Cardiomegaly': 1,\n",
    "                'Consolidation': 2,\n",
    "                'Edema': 3,\n",
    "                'No Finding': 4,\n",
    "                'Pneumonia': 5,\n",
    "            }\n",
    "            # Filters for novel classes\n",
    "            data = csv_data[(csv_data.split == \"base_train\") | (csv_data.split == \"base_validate\")]\n",
    "\n",
    "        else:\n",
    "            self.dict_labels = {\n",
    "                'Enlarged Cardiomediastinum': 0,\n",
    "                'Fracture': 1,\n",
    "                'Lung Lesion': 2,\n",
    "                'Lung Opacity': 3,\n",
    "                'Pleural Effusion': 4,\n",
    "                'Pneumothorax': 5\n",
    "            }\n",
    "            data = csv_data[csv_data.split == \"novel\"]  # Filters for novel classes\n",
    "            \n",
    "        # Converts classes to numeric values\n",
    "        self.data = data.assign(labels=data[\"labels\"].apply(lambda x: self.dict_labels[x]))\n",
    "            \n",
    "        # Create Episodes\n",
    "        self.support_episodes = []  # List of training episodes (support set)\n",
    "        self.query_episodes = []  # List of testing episodes (query set)\n",
    "        for i in range(self.num_episodes):  # for each batch\n",
    "            # 1.select n_way classes randomly\n",
    "            selected_cls = np.random.choice(len(self.dict_labels), self.n_way, False)  # no duplicate\n",
    "            np.random.shuffle(selected_cls)\n",
    "            df_support = pd.DataFrame()\n",
    "            df_query = pd.DataFrame()\n",
    "            for cls in selected_cls:\n",
    "                df_cls = self.data[self.data.labels == cls]\n",
    "                # 2. select k_shot + k_query for each class\n",
    "                selected_idx = np.random.choice(len(df_cls), self.k_shot + self.k_query, False)\n",
    "                np.random.shuffle(selected_idx)\n",
    "\n",
    "                # Index of samples for the support and query set\n",
    "                support_idx = selected_idx[:self.k_shot]\n",
    "                query_idx = selected_idx[self.k_shot:]\n",
    "\n",
    "                df_support = df_support.append(df_cls.iloc[support_idx])\n",
    "                df_query = df_query.append(df_cls.iloc[query_idx])\n",
    "\n",
    "            # Shuffle the indexes so that it is no longer ordered by class\n",
    "            df_support = df_support.sample(frac=1)\n",
    "            df_query = df_query.sample(frac=1)\n",
    "\n",
    "            self.support_episodes.append(df_support)\n",
    "            self.query_episodes.append(df_query)\n",
    "            \n",
    "    def __len__(self):\n",
    "        return self.num_episodes\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get a single episode\n",
    "        support_set = self.support_episodes[idx]\n",
    "        query_set = self.query_episodes[idx]\n",
    "        \n",
    "        # Labels ranging from 0 to (number of classes -1)\n",
    "        support_labels = support_set.labels.tolist()\n",
    "        query_labels = query_set.labels.tolist()\n",
    "\n",
    "        # Convert labels to range from 0 to (n way-1) for loss calculation\n",
    "        unique_labels = np.unique(support_labels)  # Unique labels are the same for support and query set\n",
    "        converted_support_labels = support_labels\n",
    "        converted_query_labels = query_labels\n",
    "        for idx, val in enumerate(unique_labels):\n",
    "            # Get indexes of labels that are equal to the iterated val\n",
    "            idx_support = [x for x, label in enumerate(support_labels) if label == val]\n",
    "            idx_query = [x for x, label in enumerate(query_labels) if label == val]\n",
    "\n",
    "            # Replace old labels with new labels\n",
    "            for idx_change in range(len(idx_support)):\n",
    "                converted_support_labels[idx_support[idx_change]] = idx\n",
    "\n",
    "            for idx_change in range(len(idx_query)):\n",
    "                converted_query_labels[idx_query[idx_change]] = idx\n",
    "        \n",
    "        # Get the support set of texts and masks as tensors\n",
    "        support_texts = torch.LongTensor() # Bert inputs need to be LongTensor\n",
    "        support_masks = torch.Tensor() # Bert masks need to be FloatTensor\n",
    "        for i in range(len(support_set)):\n",
    "            # Extract CSV data\n",
    "            file_path = support_set.iloc[idx, 0]\n",
    "\n",
    "            # Get text tensor and attention mask\n",
    "            text_name = f'{file_path.split(\"/\")[2]}.txt'  # Extract the study id to find the report\n",
    "            text_path = Path(os.path.join(self.root_text, text_name))\n",
    "            plain_text = text_path.read_text()\n",
    "            plain_text = plain_text.replace('_', '')  # Remove all underscores from the text\n",
    "            encoded_text = self.tokenizer.encode(plain_text, add_special_tokens=True)\n",
    "            len_encoding = len(encoded_text)\n",
    "            \n",
    "            # Transform encodings to be of the same size\n",
    "            if len_encoding > self.max_length:\n",
    "                # Truncate to max length\n",
    "                cutoff = len_encoding - self.max_length + 1  # The cutoff for the tokens to be deleted\n",
    "                del encoded_text[1:cutoff]\n",
    "                attention = [1] * self.max_length\n",
    "            elif len_encoding < self.max_length:\n",
    "                # Pad to max length\n",
    "                num_padding = self.max_length - len_encoding\n",
    "                encoded_text.extend([0] * num_padding)  # Padding token is 0\n",
    "                attention = [1] * len_encoding\n",
    "                attention.extend([0] * (self.max_length - len_encoding))\n",
    "            else:\n",
    "                # If equal size, create attention matrix\n",
    "                attention = [1] * self.max_length\n",
    "                \n",
    "            # Append texts and attention masks to the tensor to be outputted\n",
    "            support_texts = torch.cat((support_texts, torch.LongTensor(encoded_text)[None]))\n",
    "            support_masks = torch.cat((support_masks, torch.tensor(attention)[None]))\n",
    "        \n",
    "        # Get the query set of texts and masks as tensors\n",
    "        query_texts = torch.LongTensor() # Bert Inputs need to be LongTensor\n",
    "        query_masks = torch.Tensor() # Bert Masks need to be FloatTensor\n",
    "        for i in range(len(query_set)):\n",
    "            # Extract CSV data\n",
    "            file_path = query_set.iloc[idx, 0]\n",
    "\n",
    "            # Get text tensor and attention mask\n",
    "            text_name = f'{file_path.split(\"/\")[2]}.txt'  # Extract the study id to find the report\n",
    "            text_path = Path(os.path.join(self.root_text, text_name))\n",
    "            plain_text = text_path.read_text()\n",
    "            plain_text = plain_text.replace('_', '')  # Remove all underscores from the text\n",
    "            encoded_text = self.tokenizer.encode(plain_text, add_special_tokens=True)\n",
    "            len_encoding = len(encoded_text)\n",
    "            \n",
    "            # Transform encodings to be of the same size\n",
    "            if len_encoding > self.max_length:\n",
    "                # Truncate to max length\n",
    "                cutoff = len_encoding - self.max_length + 1  # The cutoff for the tokens to be deleted\n",
    "                del encoded_text[1:cutoff]\n",
    "                attention = [1] * self.max_length\n",
    "            elif len_encoding < self.max_length:\n",
    "                # Pad to max length\n",
    "                num_padding = self.max_length - len_encoding\n",
    "                encoded_text.extend([0] * num_padding)  # Padding token is 0\n",
    "                attention = [1] * len_encoding\n",
    "                attention.extend([0] * (self.max_length - len_encoding))\n",
    "            else:\n",
    "                # If equal size, create attention matrix\n",
    "                attention = [1] * self.max_length\n",
    "                \n",
    "            # Append texts and attention masks to the tensor to be outputted\n",
    "            query_texts = torch.cat((query_texts, torch.LongTensor(encoded_text)[None]))\n",
    "            query_masks = torch.cat((query_masks, torch.tensor(attention)[None]))\n",
    "\n",
    "        return support_texts, support_masks, torch.LongTensor(support_labels), \\\n",
    "                    query_texts, query_masks, torch.LongTensor(query_labels)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] t_loss: 1.1614248752593994 v_loss: 1.1491694450378418 val_acc: 0.3333333333333333 f1: 0.0\n",
      "[1] t_loss: 1.1108742952346802 v_loss: 1.1099640130996704 val_acc: 0.3333333333333333 f1: 0.0\n",
      "[1] t_loss: 1.1164581775665283 v_loss: 1.1016627550125122 val_acc: 0.3333333333333333 f1: 0.0\n",
      "[1] t_loss: 1.1169180870056152 v_loss: 1.1159577369689941 val_acc: 0.3333333333333333 f1: 0.0\n",
      "[1] t_loss: 1.1644023656845093 v_loss: 1.1466538906097412 val_acc: 0.3333333333333333 f1: 0.0\n",
      "[1] t_loss: 1.1183087825775146 v_loss: 1.1099356412887573 val_acc: 0.3333333333333333 f1: 0.0\n",
      "[1] t_loss: 1.1101338863372803 v_loss: 1.1063259840011597 val_acc: 0.3333333333333333 f1: 0.0\n",
      "[1] t_loss: 1.0991480350494385 v_loss: 1.1031323671340942 val_acc: 0.3333333333333333 f1: 0.0\n",
      "[1] t_loss: 1.1364350318908691 v_loss: 1.130818247795105 val_acc: 0.3333333333333333 f1: 0.0\n",
      "[1] t_loss: 1.1044405698776245 v_loss: 1.109404444694519 val_acc: 0.3333333333333333 f1: 0.0\n",
      "[1] t_loss: 1.2097285985946655 v_loss: 1.1860020160675049 val_acc: 0.3333333333333333 f1: 0.0\n",
      "[1] t_loss: 1.1036350727081299 v_loss: 1.1046239137649536 val_acc: 0.3333333333333333 f1: 0.0\n",
      "[1] t_loss: 1.0965262651443481 v_loss: 1.0995110273361206 val_acc: 0.3333333333333333 f1: 0.0\n",
      "[1] t_loss: 1.1767808198928833 v_loss: 1.1696535348892212 val_acc: 0.3333333333333333 f1: 0.0\n",
      "[1] t_loss: 1.1525980234146118 v_loss: 1.1733183860778809 val_acc: 0.3333333333333333 f1: 0.0\n",
      "[1] t_loss: 1.102684736251831 v_loss: 1.1007856130599976 val_acc: 0.3333333333333333 f1: 0.0\n",
      "[1] t_loss: 1.1218525171279907 v_loss: 1.1193052530288696 val_acc: 0.3333333333333333 f1: 0.0\n",
      "[1] t_loss: 1.1112060546875 v_loss: 1.1191481351852417 val_acc: 0.3333333333333333 f1: 0.0\n",
      "[1] t_loss: 1.1031583547592163 v_loss: 1.1010807752609253 val_acc: 0.3333333333333333 f1: 0.0\n",
      "[1] t_loss: 1.1380152702331543 v_loss: 1.1616193056106567 val_acc: 0.3333333333333333 f1: 0.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from biobertology import get_tokenizer\n",
    "import sys\n",
    "\n",
    "sys.path.append('..')\n",
    "from shared.models import *\n",
    "# from shared.datasets import *\n",
    "from shared.metrics import *\n",
    "\n",
    "\n",
    "def train(text_inputs, attention_inputs, labels, model, criterion, device, optimizer, freeze=False):\n",
    "    # Training loop\n",
    "    model.train()\n",
    "\n",
    "    # Freeze all layers except those indicated\n",
    "    if freeze:\n",
    "        for name, param in model.named_parameters():\n",
    "            if name not in freeze:\n",
    "                param.requires_grad = False\n",
    "    \n",
    "    # Train the entire support set in one batch\n",
    "    optimizer.zero_grad()\n",
    "    pred = model(text_inputs, attention_inputs)\n",
    "    loss = criterion(pred, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    train_loss = loss.item()  # Running training loss\n",
    "\n",
    "    return train_loss\n",
    "\n",
    "\n",
    "def test(text_inputs, attention_inputs, labels, model, criterion, device, n_way):\n",
    "    # An F1 Score of 0 indicates that it is invalid\n",
    "    model.eval()\n",
    "    true_positive = list(0. for i in range(n_way))  # Number of correctly predicted samples per class\n",
    "    total_truth = list(0. for i in range(n_way))  # Number of ground truths per class\n",
    "    predicted_positive = list(0. for i in range(n_way))  # Number of predicted samples per class\n",
    "    correct_total = 0  # Total correctly predicted samples\n",
    "    total = 0  # Total samples\n",
    "    with torch.no_grad():\n",
    "        # Test the entire query set in one batch\n",
    "        pred = model(text_inputs, attention_inputs)\n",
    "        loss = criterion(pred, labels)\n",
    "        val_loss = loss.item()  # Running validation loss\n",
    "        _, predicted = torch.max(pred, 1)\n",
    "        correct = (predicted == labels).squeeze()  # Samples that are correctly predicted\n",
    "        correct_total += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "        for i in range(len(predicted)):\n",
    "            label = labels[i]\n",
    "            true_positive[label] += correct[i].item()\n",
    "            total_truth[label] += 1\n",
    "            predicted_positive[predicted[i].item()] += 1  # True Positive + False Positive\n",
    "\n",
    "    accuracy, macro_accuracy, f1_score, class_f1 = metrics(true_positive, total_truth,\n",
    "                                                           predicted_positive, correct_total, total)\n",
    "\n",
    "    return val_loss, accuracy, macro_accuracy, f1_score, class_f1\n",
    "\n",
    "\n",
    "#def main():\n",
    "# Set Training Parameters\n",
    "n_way = 3\n",
    "k_shot = 20\n",
    "k_query = 16\n",
    "num_episodes = 20\n",
    "num_epochs = 20\n",
    "num_workers = 12\n",
    "bs = 4\n",
    "lr = 1e-4\n",
    "root_text = '../../../../scratch/rl80/mimic-cxr-2.0.0.physionet.org'\n",
    "path_biobert = '../results'\n",
    "path_splits = '../splits/splits.csv'  # Location of preprocessed splits\n",
    "path_results = '../../results'  # Folder to save the CSV results\n",
    "freeze = ['linear.weight', 'linear.bias']  # Freeze all layers except linear layers\n",
    "\n",
    "torch.cuda.set_device(0)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Training tools\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "tokenizer = get_tokenizer()\n",
    "\n",
    "# Load in data\n",
    "dataset = MimicCxrReportsEpisodes(root_text, path_splits, tokenizer, n_way, k_shot, k_query, num_episodes, 'novel')\n",
    "loader = DataLoader(dataset, batch_size=bs, shuffle=True, num_workers=num_workers)\n",
    "\n",
    "# Create Dataframe to export results to CSV\n",
    "df_results = pd.DataFrame(columns=['Epoch', 'Training Loss', 'Validation Loss', 'Accuracy', 'Macro Accuracy',\n",
    "                                   'Macro-F1 Score'] + [str(x) + ' F1' for x in range(n_way)])\n",
    "\n",
    "# Iterate through batched episodes. One episode is one experiment\n",
    "for step, (support_texts, support_masks, support_labels, query_texts, query_masks, query_labels) in enumerate(loader):\n",
    "    # Convert Tensors to appropriate device\n",
    "    batch_support_x, batch_support_masks, batch_support_y, batch_query_x, batch_query_masks, batch_query_y = \\\n",
    "        support_texts.to(device), support_masks.to(device), support_labels.to(device), \\\n",
    "        query_texts.to(device), query_masks.to(device), query_labels.to(device)\n",
    "\n",
    "    # [num_batch, training_sz, channels, height, width] = support_x.size()\n",
    "    # num_batch = num of episodes\n",
    "    # training_sz = size of support or query set\n",
    "    num_batch = batch_support_x.size(0) # Number of episodes in the batch\n",
    "\n",
    "    # Break down the batch of episodes into single episodes\n",
    "    for i in range(num_batch):\n",
    "        # Load in model and reset weights every episode/experiment\n",
    "        model = SemanticNet(n_way, path_biobert).to(device)\n",
    "\n",
    "        # Reset optimizer with model parameters\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "        # Break down the sets into individual episodes\n",
    "        support_x, support_m, support_y, query_x, query_m, query_y = \\\n",
    "            batch_support_x[i], batch_support_masks[i], batch_support_y[i], \\\n",
    "            batch_query_x[i], batch_query_masks[i], batch_query_y[i]\n",
    "\n",
    "        # Variables for best epoch per experiment\n",
    "        best_score = 0\n",
    "        best_epoch = 0\n",
    "        df_best = pd.DataFrame(columns=['Epoch', 'Training Loss', 'Validation Loss', 'Accuracy', 'Macro Accuracy',\n",
    "                                   'Macro-F1 Score'] + [str(x) + ' F1' for x in range(n_way)]) # Track best epoch\n",
    "        # Training and testing for specified epochs\n",
    "        for epoch in range(num_epochs):\n",
    "            # Training\n",
    "            train_loss = train(support_x, support_m, support_y, model, criterion, device, optimizer, freeze=freeze)\n",
    "\n",
    "            # Testing\n",
    "            val_loss, acc, m_acc, macro_f1, class_f1 = test(query_x, query_m, query_y, \n",
    "                                                            model, criterion, device, n_way)\n",
    "\n",
    "            # Find best epoch\n",
    "            score = 0.5*acc + 0.5*macro_f1\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                df_best.loc[0] = [epoch + 1, train_loss, val_loss, acc, m_acc, macro_f1] + class_f1\n",
    "\n",
    "        # Print the best results per experiment\n",
    "        print(\n",
    "            f'[{int(df_best.iloc[0,0])}] t_loss: {df_best.iloc[0,1]} v_loss: {df_best.iloc[0,2]} '\n",
    "            f'val_acc: {df_best.iloc[0,3]} f1: {df_best.iloc[0,5]}')\n",
    "\n",
    "        # Record the best epoch to be saved into a CSV\n",
    "        df_results = df_results.append(df_best.loc[0], ignore_index=True)\n",
    "\n",
    "# Create results folder if it does not exist\n",
    "if not os.path.exists(path_results):\n",
    "    os.makedirs(path_results)\n",
    "\n",
    "# Export results to a CSV file\n",
    "df_results.to_csv(os.path.join(path_results, f'{k_shot}shot_semantic.csv'), index=False)\n",
    "\n",
    "\n",
    "#if __name__ == '__main__':\n",
    "#    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "support_x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "support_m.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'torch.cuda.FloatTensor'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "support_x.type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = torch.LongTensor([0.1111])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "support_y.type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
