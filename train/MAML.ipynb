{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 4 Accuracy: 0.4125 F1-Score: 0.40484990429906764\n",
      "Best Step: 4\n"
     ]
    }
   ],
   "source": [
    "import  torch, os\n",
    "import  pandas as pd\n",
    "import  numpy as np\n",
    "import  scipy.stats\n",
    "from    torch.utils.data import DataLoader\n",
    "import  sys\n",
    "\n",
    "sys.path.append('..')\n",
    "from shared.datasets import *\n",
    "from shared.meta import *\n",
    "\n",
    "\n",
    "def main(k_shot):\n",
    "    n_way = 3\n",
    "    k_query = 16\n",
    "    num_workers = 12\n",
    "    train_num_episodes = 5\n",
    "    test_num_episodes = 5\n",
    "    bs = 1\n",
    "    root = '../../../../scratch/rl80/mimic-cxr-jpg-2.0.0.physionet.org/files'\n",
    "    path_splits = '../splits/splits.csv'  # Location of preprocessed splits\n",
    "    path_results = '../../results'  # Folder to save the CSV results\n",
    "\n",
    "    update_lr = 1e-2 # Learning rate for meta-training\n",
    "    meta_lr = 1e-3 # Learning rate for meta-testing\n",
    "    update_step = 5 # Number of meta-training update steps\n",
    "    update_step_test = 10 # Number of meta-testing update steps\n",
    "    imgsz = 224 # Size of images\n",
    "    imgc = 1 # Initial image channels\n",
    "\n",
    "    # Learner model configuration\n",
    "    config = [\n",
    "        ('conv2d', [64, 1, 3, 3, 1, 1]),\n",
    "        ('relu', [True]),\n",
    "        ('bn', [64]),\n",
    "        ('max_pool2d', [2, 2, 0]),\n",
    "        ('conv2d', [64, 64, 3, 3, 1, 1]),\n",
    "        ('relu', [True]),\n",
    "        ('bn', [64]),\n",
    "        ('max_pool2d', [2, 2, 0]),\n",
    "        ('conv2d', [64, 64, 3, 3, 1, 1]),\n",
    "        ('relu', [True]),\n",
    "        ('bn', [64]),\n",
    "        ('max_pool2d', [2, 2, 0]),\n",
    "        ('conv2d', [64, 64, 3, 3, 1, 1]),\n",
    "        ('relu', [True]),\n",
    "        ('bn', [64]),\n",
    "        ('max_pool2d', [2, 2, 0]),\n",
    "        ('flatten', []),\n",
    "        ('linear', [n_way, 64 * 14 * 14])\n",
    "    ]\n",
    "\n",
    "    torch.cuda.set_device(0)\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    maml = Meta(update_lr, meta_lr, n_way, k_shot, k_query, bs,\n",
    "                update_step, update_step_test, imgc, imgsz, config).to(device)\n",
    "    tmp = filter(lambda x: x.requires_grad, maml.parameters())\n",
    "    num = sum(map(lambda x: np.prod(x.shape), tmp))\n",
    "    \n",
    "    # Create batched episode datasets\n",
    "    mini = MimicCxrJpgEpisodes(root, path_splits, n_way, k_shot, k_query, train_num_episodes, mode=\"base\")\n",
    "    mini_test = MimicCxrJpgEpisodes(root, path_splits, n_way, k_shot, k_query, test_num_episodes, mode=\"novel\")\n",
    "\n",
    "    # fetch meta_batchsz num of episode each time\n",
    "    db = DataLoader(mini, batch_size=bs, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "    # Keep track of best meta-testing results\n",
    "    best_score = 0\n",
    "    best_step = 0\n",
    "\n",
    "    for step, (x_spt, y_spt, x_qry, y_qry) in enumerate(db):\n",
    "\n",
    "        x_spt, y_spt, x_qry, y_qry = x_spt.to(device), y_spt.to(device), x_qry.to(device), y_qry.to(device)\n",
    "\n",
    "        accs = maml(x_spt, y_spt, x_qry, y_qry)\n",
    "\n",
    "        if (step+1) % 5 == 0:  # evaluation\n",
    "            # Create Dataframe containing results of the multiple episodes\n",
    "            df_results = pd.DataFrame(columns=['Step', 'Accuracy', 'Macro Accuracy',\n",
    "                                               'Macro-F1 Score'] + [str(x) + ' F1' for x in range(n_way)])\n",
    "            db_test = DataLoader(mini_test, 1, shuffle=True, num_workers=1, pin_memory=True)\n",
    "            accs_all_test = []\n",
    "\n",
    "            for x_spt, y_spt, x_qry, y_qry in db_test:\n",
    "                x_spt, y_spt = x_spt.squeeze(0).to(device), y_spt.squeeze(0).to(device)\n",
    "                x_qry, y_qry = x_qry.squeeze(0).to(device), y_qry.squeeze(0).to(device)\n",
    "\n",
    "                # Record the best step per episode\n",
    "                df_best = maml.finetunning(x_spt, y_spt, x_qry, y_qry)\n",
    "                df_results = df_results.append(df_best.loc[0], ignore_index=True)\n",
    "\n",
    "            \n",
    "\n",
    "            # Find average accuracy and average f1 score over the experiments\n",
    "            average_accuracy = df_results[\"Accuracy\"].mean()\n",
    "            average_f1 = df_results[\"Macro-F1 Score\"].mean()\n",
    "            print(f'Step: {step} Accuracy: {average_accuracy} F1-Score: {average_f1}') # Print results\n",
    "\n",
    "            # Record best testing results\n",
    "            score = 0.5*average_accuracy + 0.5*average_f1\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_step = step\n",
    "                df_best_test = df_results\n",
    "\n",
    "    print(f\"Best Step: {best_step}\")\n",
    "\n",
    "    # Create results folder if it does not exist\n",
    "    if not os.path.exists(path_results):\n",
    "        os.makedirs(path_results)\n",
    "\n",
    "    df_best_test.to_csv(os.path.join(path_results, f'{k_shot}shot_MAML_{best_step}.csv'), index=False)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    #print(f'MAML Training {sys.argv[1]} shot')\n",
    "    #main(int(sys.argv[1])) # Get the k_shot variable from command line\n",
    "    main(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  torch, os\n",
    "import  pandas as pd\n",
    "import  numpy as np\n",
    "import  scipy.stats\n",
    "from    torch.utils.data import DataLoader\n",
    "import  sys\n",
    "\n",
    "sys.path.append('..')\n",
    "from shared.datasets import *\n",
    "from shared.meta import *\n",
    "\n",
    "\n",
    "def main(k_shot):\n",
    "    n_way = 3\n",
    "    k_query = 16\n",
    "    num_workers = 12\n",
    "    train_num_episodes = 50000\n",
    "    test_num_episodes = 200\n",
    "    bs = 1\n",
    "    root = '../../../../scratch/rl80/mimic-cxr-jpg-2.0.0.physionet.org/files'\n",
    "    path_splits = '../splits/splits.csv'  # Location of preprocessed splits\n",
    "    path_results = '../../results'  # Folder to save the CSV results\n",
    "\n",
    "    update_lr = 1e-2 # Learning rate for meta-training\n",
    "    meta_lr = 1e-3 # Learning rate for meta-testing\n",
    "    update_step = 5 # Number of meta-training update steps\n",
    "    update_step_test = 10 # Number of meta-testing update steps\n",
    "    imgsz = 224 # Size of images\n",
    "    imgc = 1 # Initial image channels\n",
    "\n",
    "    # Learner model configuration\n",
    "    config = [\n",
    "        ('conv2d', [64, 1, 3, 3, 1, 1]),\n",
    "        ('relu', [True]),\n",
    "        ('bn', [64]),\n",
    "        ('max_pool2d', [2, 2, 0]),\n",
    "        ('conv2d', [64, 64, 3, 3, 1, 1]),\n",
    "        ('relu', [True]),\n",
    "        ('bn', [64]),\n",
    "        ('max_pool2d', [2, 2, 0]),\n",
    "        ('conv2d', [64, 64, 3, 3, 1, 1]),\n",
    "        ('relu', [True]),\n",
    "        ('bn', [64]),\n",
    "        ('max_pool2d', [2, 2, 0]),\n",
    "        ('conv2d', [64, 64, 3, 3, 1, 1]),\n",
    "        ('relu', [True]),\n",
    "        ('bn', [64]),\n",
    "        ('max_pool2d', [2, 2, 0]),\n",
    "        ('flatten', []),\n",
    "        ('linear', [n_way, 64 * 14 * 14])\n",
    "    ]\n",
    "\n",
    "    torch.cuda.set_device(0)\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    maml = Meta(update_lr, meta_lr, n_way, k_shot, k_query, bs,\n",
    "                update_step, update_step_test, imgc, imgsz, config).to(device)\n",
    "    tmp = filter(lambda x: x.requires_grad, maml.parameters())\n",
    "    num = sum(map(lambda x: np.prod(x.shape), tmp))\n",
    "    \n",
    "    # Create batched episode datasets\n",
    "    mini = MimicCxrJpgEpisodes(root, path_splits, n_way, k_shot, k_query, train_num_episodes, mode=\"base\")\n",
    "    mini_test = MimicCxrJpgEpisodes(root, path_splits, n_way, k_shot, k_query, test_num_episodes, mode=\"novel\")\n",
    "\n",
    "    # fetch meta_batchsz num of episode each time\n",
    "    db = DataLoader(mini, batch_size=bs, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "    # Keep track of best meta-testing results\n",
    "    best_score = 0\n",
    "    best_step = 0\n",
    "\n",
    "    for step, (x_spt, y_spt, x_qry, y_qry) in enumerate(db):\n",
    "\n",
    "        x_spt, y_spt, x_qry, y_qry = x_spt.to(device), y_spt.to(device), x_qry.to(device), y_qry.to(device)\n",
    "\n",
    "        accs = maml(x_spt, y_spt, x_qry, y_qry)\n",
    "\n",
    "        if (step+1) % 1000 == 0:  # evaluation\n",
    "            # Create Dataframe containing results of the multiple episodes\n",
    "            df_results = pd.DataFrame(columns=['Step', 'Accuracy', 'Macro Accuracy',\n",
    "                                               'Macro-F1 Score'] + [str(x) + ' F1' for x in range(n_way)])\n",
    "            db_test = DataLoader(mini_test, 1, shuffle=True, num_workers=1, pin_memory=True)\n",
    "            accs_all_test = []\n",
    "\n",
    "            for x_spt, y_spt, x_qry, y_qry in db_test:\n",
    "                x_spt, y_spt = x_spt.squeeze(0).to(device), y_spt.squeeze(0).to(device)\n",
    "                x_qry, y_qry = x_qry.squeeze(0).to(device), y_qry.squeeze(0).to(device)\n",
    "\n",
    "                # Record the best step per episode\n",
    "                df_best = maml.finetunning(x_spt, y_spt, x_qry, y_qry)\n",
    "                df_results = df_results.append(df_best.loc[0], ignore_index=True)\n",
    "\n",
    "            # Find average accuracy and average f1 score over the experiments\n",
    "            average_accuracy = df_results[\"Accuracy\"].mean()\n",
    "            average_f1 = df_results[\"Macro-F1 Score\"].mean()\n",
    "            print(f'Step: {step} Accuracy: {average_accuracy} F1-Score: {average_f1}') # Print results\n",
    "\n",
    "            # Record best testing results\n",
    "            score = 0.5*average_accuracy + 0.5*average_f1\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_step = step\n",
    "                df_best_test = df_results\n",
    "\n",
    "    print(f\"Best Step: {best_step}\")\n",
    "\n",
    "    # Create results folder if it does not exist\n",
    "    if not os.path.exists(path_results):\n",
    "        os.makedirs(path_results)\n",
    "\n",
    "    df_best_test.to_csv(os.path.join(path_results, f'{k_shot}shot_MAML_{best_step}.csv'), index=False)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print(f'MAML Training {sys.argv[1]} shot')\n",
    "    main(int(sys.argv[1])) # Get the k_shot variable from command line\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
