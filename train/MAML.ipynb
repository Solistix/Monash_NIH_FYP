{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch import optim\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "# TODO: Need to Reference to https://github.com/dragen1860/MAML-Pytorch\n",
    "class Meta(nn.Module):\n",
    "    \"\"\"\n",
    "    Meta Learner\n",
    "    \"\"\"\n",
    "    def __init__(self, args, config):\n",
    "        \"\"\"\n",
    "\n",
    "        :param args:\n",
    "        \"\"\"\n",
    "        super(Meta, self).__init__()\n",
    "\n",
    "        self.update_lr = args.update_lr\n",
    "        self.meta_lr = args.meta_lr\n",
    "        self.n_way = args.n_way\n",
    "        self.k_spt = args.k_spt\n",
    "        self.k_qry = args.k_qry\n",
    "        self.task_num = args.task_num\n",
    "        self.update_step = args.update_step\n",
    "        self.update_step_test = args.update_step_test\n",
    "\n",
    "        self.net = Learner(config, args.imgc, args.imgsz)\n",
    "        self.meta_optim = optim.Adam(self.net.parameters(), lr=self.meta_lr)\n",
    "\n",
    "    def clip_grad_by_norm_(self, grad, max_norm):\n",
    "        \"\"\"\n",
    "        in-place gradient clipping.\n",
    "        :param grad: list of gradients\n",
    "        :param max_norm: maximum norm allowable\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        total_norm = 0\n",
    "        counter = 0\n",
    "        for g in grad:\n",
    "            param_norm = g.data.norm(2)\n",
    "            total_norm += param_norm.item() ** 2\n",
    "            counter += 1\n",
    "        total_norm = total_norm ** (1. / 2)\n",
    "\n",
    "        clip_coef = max_norm / (total_norm + 1e-6)\n",
    "        if clip_coef < 1:\n",
    "            for g in grad:\n",
    "                g.data.mul_(clip_coef)\n",
    "\n",
    "        return total_norm/counter\n",
    "\n",
    "    def forward(self, x_spt, y_spt, x_qry, y_qry):\n",
    "        \"\"\"\n",
    "\n",
    "        :param x_spt:   [b, setsz, c_, h, w]\n",
    "        :param y_spt:   [b, setsz]\n",
    "        :param x_qry:   [b, querysz, c_, h, w]\n",
    "        :param y_qry:   [b, querysz]\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        task_num, setsz, c_, h, w = x_spt.size()\n",
    "        querysz = x_qry.size(1)\n",
    "\n",
    "        losses_q = [0 for _ in range(self.update_step + 1)]  # losses_q[i] is the loss on step i\n",
    "        corrects = [0 for _ in range(self.update_step + 1)]\n",
    "\n",
    "        for i in range(task_num):\n",
    "\n",
    "            # 1. run the i-th task and compute loss for k=0\n",
    "            logits = self.net(x_spt[i], vars=None, bn_training=True)\n",
    "            loss = F.cross_entropy(logits, y_spt[i])\n",
    "            grad = torch.autograd.grad(loss, self.net.parameters())\n",
    "            fast_weights = list(map(lambda p: p[1] - self.update_lr * p[0], zip(grad, self.net.parameters())))\n",
    "\n",
    "            # this is the loss and accuracy before first update\n",
    "            with torch.no_grad():\n",
    "                # [setsz, nway]\n",
    "                logits_q = self.net(x_qry[i], self.net.parameters(), bn_training=True)\n",
    "                loss_q = F.cross_entropy(logits_q, y_qry[i])\n",
    "                losses_q[0] += loss_q\n",
    "\n",
    "                pred_q = F.softmax(logits_q, dim=1).argmax(dim=1)\n",
    "                correct = torch.eq(pred_q, y_qry[i]).sum().item()\n",
    "                corrects[0] = corrects[0] + correct\n",
    "\n",
    "            # this is the loss and accuracy after the first update\n",
    "            with torch.no_grad():\n",
    "                # [setsz, nway]\n",
    "                logits_q = self.net(x_qry[i], fast_weights, bn_training=True)\n",
    "                loss_q = F.cross_entropy(logits_q, y_qry[i])\n",
    "                losses_q[1] += loss_q\n",
    "                # [setsz]\n",
    "                pred_q = F.softmax(logits_q, dim=1).argmax(dim=1)\n",
    "                correct = torch.eq(pred_q, y_qry[i]).sum().item()\n",
    "                corrects[1] = corrects[1] + correct\n",
    "\n",
    "            for k in range(1, self.update_step):\n",
    "                # 1. run the i-th task and compute loss for k=1~K-1\n",
    "                logits = self.net(x_spt[i], fast_weights, bn_training=True)\n",
    "                loss = F.cross_entropy(logits, y_spt[i])\n",
    "                # 2. compute grad on theta_pi\n",
    "                grad = torch.autograd.grad(loss, fast_weights)\n",
    "                # 3. theta_pi = theta_pi - train_lr * grad\n",
    "                fast_weights = list(map(lambda p: p[1] - self.update_lr * p[0], zip(grad, fast_weights)))\n",
    "\n",
    "                logits_q = self.net(x_qry[i], fast_weights, bn_training=True)\n",
    "                # loss_q will be overwritten and just keep the loss_q on last update step.\n",
    "                loss_q = F.cross_entropy(logits_q, y_qry[i])\n",
    "                losses_q[k + 1] += loss_q\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    pred_q = F.softmax(logits_q, dim=1).argmax(dim=1)\n",
    "                    correct = torch.eq(pred_q, y_qry[i]).sum().item()  # convert to numpy\n",
    "                    corrects[k + 1] = corrects[k + 1] + correct\n",
    "\n",
    "\n",
    "\n",
    "        # end of all tasks\n",
    "        # sum over all losses on query set across all tasks\n",
    "        loss_q = losses_q[-1] / task_num\n",
    "\n",
    "        # optimize theta parameters\n",
    "        self.meta_optim.zero_grad()\n",
    "        loss_q.backward()\n",
    "        # print('meta update')\n",
    "        # for p in self.net.parameters()[:5]:\n",
    "        # \tprint(torch.norm(p).item())\n",
    "        self.meta_optim.step()\n",
    "\n",
    "\n",
    "        accs = np.array(corrects) / (querysz * task_num)\n",
    "\n",
    "        return accs\n",
    "\n",
    "    def finetunning(self, x_spt, y_spt, x_qry, y_qry):\n",
    "        \"\"\"\n",
    "\n",
    "        :param x_spt:   [setsz, c_, h, w]\n",
    "        :param y_spt:   [setsz]\n",
    "        :param x_qry:   [querysz, c_, h, w]\n",
    "        :param y_qry:   [querysz]\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        assert len(x_spt.shape) == 4\n",
    "\n",
    "        querysz = x_qry.size(0)\n",
    "\n",
    "        corrects = [0 for _ in range(self.update_step_test + 1)]\n",
    "\n",
    "        # in order to not ruin the state of running_mean/variance and bn_weight/bias\n",
    "        # we finetunning on the copied model instead of self.net\n",
    "        net = deepcopy(self.net)\n",
    "\n",
    "        # 1. run the i-th task and compute loss for k=0\n",
    "        logits = net(x_spt)\n",
    "        loss = F.cross_entropy(logits, y_spt)\n",
    "        grad = torch.autograd.grad(loss, net.parameters())\n",
    "        fast_weights = list(map(lambda p: p[1] - self.update_lr * p[0], zip(grad, net.parameters())))\n",
    "\n",
    "        # this is the loss and accuracy before first update\n",
    "        with torch.no_grad():\n",
    "            # [setsz, nway]\n",
    "            logits_q = net(x_qry, net.parameters(), bn_training=True)\n",
    "            # [setsz]\n",
    "            pred_q = F.softmax(logits_q, dim=1).argmax(dim=1)\n",
    "            # scalar\n",
    "            correct = torch.eq(pred_q, y_qry).sum().item()\n",
    "            corrects[0] = corrects[0] + correct\n",
    "\n",
    "        # this is the loss and accuracy after the first update\n",
    "        with torch.no_grad():\n",
    "            # [setsz, nway]\n",
    "            logits_q = net(x_qry, fast_weights, bn_training=True)\n",
    "            # [setsz]\n",
    "            pred_q = F.softmax(logits_q, dim=1).argmax(dim=1)\n",
    "            # scalar\n",
    "            correct = torch.eq(pred_q, y_qry).sum().item()\n",
    "            corrects[1] = corrects[1] + correct\n",
    "\n",
    "        for k in range(1, self.update_step_test):\n",
    "            # 1. run the i-th task and compute loss for k=1~K-1\n",
    "            logits = net(x_spt, fast_weights, bn_training=True)\n",
    "            loss = F.cross_entropy(logits, y_spt)\n",
    "            # 2. compute grad on theta_pi\n",
    "            grad = torch.autograd.grad(loss, fast_weights)\n",
    "            # 3. theta_pi = theta_pi - train_lr * grad\n",
    "            fast_weights = list(map(lambda p: p[1] - self.update_lr * p[0], zip(grad, fast_weights)))\n",
    "\n",
    "            logits_q = net(x_qry, fast_weights, bn_training=True)\n",
    "            # loss_q will be overwritten and just keep the loss_q on last update step.\n",
    "            loss_q = F.cross_entropy(logits_q, y_qry)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                pred_q = F.softmax(logits_q, dim=1).argmax(dim=1)\n",
    "                correct = torch.eq(pred_q, y_qry).sum().item()  # convert to numpy\n",
    "                corrects[k + 1] = corrects[k + 1] + correct\n",
    "\n",
    "\n",
    "        del net\n",
    "\n",
    "        accs = np.array(corrects) / querysz\n",
    "\n",
    "        return accs\n",
    "\n",
    "\n",
    "def main():\n",
    "    pass\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import  torch, os\n",
    "import  numpy as np\n",
    "from    MimicCxrJpg import MimicCxrJpg\n",
    "import  scipy.stats\n",
    "from    torch.utils.data import DataLoader\n",
    "import  argparse\n",
    "\n",
    "sys.path.append('..')\n",
    "from shared.meta import *\n",
    "from shared.datasets import *\n",
    "\n",
    "\n",
    "def mean_confidence_interval(accs, confidence=0.95):\n",
    "    n = accs.shape[0]\n",
    "    m, se = np.mean(accs), scipy.stats.sem(accs)\n",
    "    h = se * scipy.stats.t._ppf((1 + confidence) / 2, n - 1)\n",
    "    return m, h\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    torch.manual_seed(222)\n",
    "    torch.cuda.manual_seed_all(222)\n",
    "    np.random.seed(222)\n",
    "\n",
    "    print(args)\n",
    "\n",
    "    config = [\n",
    "        ('conv2d', [32, 1, 3, 3, 1, 0]),\n",
    "        ('relu', [True]),\n",
    "        ('bn', [32]),\n",
    "        ('max_pool2d', [2, 2, 0]),\n",
    "        ('conv2d', [32, 32, 3, 3, 1, 0]),\n",
    "        ('relu', [True]),\n",
    "        ('bn', [32]),\n",
    "        ('max_pool2d', [2, 2, 0]),\n",
    "        ('conv2d', [32, 32, 3, 3, 1, 0]),\n",
    "        ('relu', [True]),\n",
    "        ('bn', [32]),\n",
    "        ('max_pool2d', [2, 2, 0]),\n",
    "        ('conv2d', [32, 32, 3, 3, 1, 0]),\n",
    "        ('relu', [True]),\n",
    "        ('bn', [32]),\n",
    "        ('max_pool2d', [2, 2, 0]),\n",
    "        ('flatten', []),\n",
    "        ('linear', [args.n_way, 16928])\n",
    "    ]\n",
    "\n",
    "    device = torch.device('cuda')\n",
    "    maml = Meta(args, config).to(device)\n",
    "\n",
    "    tmp = filter(lambda x: x.requires_grad, maml.parameters())\n",
    "    num = sum(map(lambda x: np.prod(x.shape), tmp))\n",
    "    print(maml)\n",
    "    print('Total trainable tensors:', num)\n",
    "\n",
    "    # batchsz here means total episode number\n",
    "    mini = MimicCxrJpg('../../../../scratch/rl80/mimic-cxr-jpg-2.0.0.physionet.org/files/',\n",
    "                       mode='train', csv_path='../splits.csv', n_way=args.n_way, k_shot=args.k_spt, k_query=args.k_qry,\n",
    "                       batchsz=10000, resize=args.imgsz)\n",
    "    mini_test = MimicCxrJpg('../../../../scratch/rl80/mimic-cxr-jpg-2.0.0.physionet.org/files/',\n",
    "                            mode='test', csv_path='../splits.csv', n_way=args.n_way, k_shot=args.k_spt, k_query=args.k_qry,\n",
    "                            batchsz=100, resize=args.imgsz)\n",
    "\n",
    "    for epoch in range(args.epoch//10000):\n",
    "        # fetch meta_batchsz num of episode each time\n",
    "        db = DataLoader(mini, args.task_num, shuffle=True, num_workers=1, pin_memory=True)\n",
    "\n",
    "        for step, (x_spt, y_spt, x_qry, y_qry) in enumerate(db):\n",
    "\n",
    "            x_spt, y_spt, x_qry, y_qry = x_spt.to(device), y_spt.to(device), x_qry.to(device), y_qry.to(device)\n",
    "\n",
    "            accs = maml(x_spt, y_spt, x_qry, y_qry)\n",
    "\n",
    "            if step % 30 == 0:\n",
    "                print('step:', step, '\\ttraining acc:', accs)\n",
    "\n",
    "            if step % 100 == 0:  # evaluation\n",
    "                db_test = DataLoader(mini_test, 1, shuffle=True, num_workers=1, pin_memory=True)\n",
    "                accs_all_test = []\n",
    "\n",
    "                for x_spt, y_spt, x_qry, y_qry in db_test:\n",
    "                    x_spt, y_spt, x_qry, y_qry = x_spt.squeeze(0).to(device), y_spt.squeeze(0).to(device), \\\n",
    "                                                 x_qry.squeeze(0).to(device), y_qry.squeeze(0).to(device)\n",
    "\n",
    "                    accs = maml.finetunning(x_spt, y_spt, x_qry, y_qry)\n",
    "                    accs_all_test.append(accs)\n",
    "\n",
    "                # [b, update_step+1]\n",
    "                accs = np.array(accs_all_test).mean(axis=0).astype(np.float16)\n",
    "                print('Test acc:', accs)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    argparser = argparse.ArgumentParser()\n",
    "    argparser.add_argument('--epoch', type=int, help='epoch number', default=40000)\n",
    "    argparser.add_argument('--n_way', type=int, help='n way', default=3)\n",
    "    argparser.add_argument('--k_spt', type=int, help='k shot for support set', default=1)\n",
    "    argparser.add_argument('--k_qry', type=int, help='k shot for query set', default=1)\n",
    "    argparser.add_argument('--imgsz', type=int, help='imgsz', default=224)\n",
    "    argparser.add_argument('--imgc', type=int, help='imgc', default=1)\n",
    "    argparser.add_argument('--task_num', type=int, help='meta batch size, namely task num', default=1)\n",
    "    argparser.add_argument('--meta_lr', type=float, help='meta-level outer learning rate', default=1e-3)\n",
    "    argparser.add_argument('--update_lr', type=float, help='task-level inner update learning rate', default=0.01)\n",
    "    argparser.add_argument('--update_step', type=int, help='task-level inner update steps', default=5)\n",
    "    argparser.add_argument('--update_step_test', type=int, help='update steps for finetunning', default=10)\n",
    "\n",
    "    args, unknown = argparser.parse_known_args()\n",
    "\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Variables\n",
    "    num_epochs = 100\n",
    "    num_workers = 12\n",
    "    bs = 8\n",
    "    n_way = 3\n",
    "    k_shot = 20\n",
    "    lr = 1e-5\n",
    "    save_models = True  # Whether to save the trained models (Occurs every epoch)\n",
    "\n",
    "    root_text = '../../../../scratch/rl80/mimic-cxr-2.0.0.physionet.org'\n",
    "    path_biobert = '../results'\n",
    "    path_splits = f'../splits/{k_shot}_shot.csv'\n",
    "    path_results = f'../../results/{k_shot}shot_semantic.csv'  # Full path to save the CSV results\n",
    "    path_models = f'../../models/semantic/{k_shot}_shot'  # Folder path to save the trained models to\n",
    "    freeze = ['linear.weight', 'linear.bias']\n",
    "\n",
    "    if not os.path.exists(path_models):\n",
    "        os.makedirs(path_models)\n",
    "\n",
    "    # Check for GPU device\n",
    "    torch.cuda.set_device(0)\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Load in model\n",
    "    model = SemanticNet(n_way, path_biobert).to(device)\n",
    "    model_dict = model.state_dict()  # To do layer checks\n",
    "\n",
    "    # Check if the layers to be unfrozen are in the model\n",
    "    if type(freeze) != bool:\n",
    "        check = all(item in model_dict for item in freeze)\n",
    "        if check:\n",
    "            print(f'The layers that will not be frozen are: {freeze}')\n",
    "        else:\n",
    "            raise Exception('Not all elements to stay unfrozen are in the model')\n",
    "\n",
    "    # Load in dataset\n",
    "    tokenizer = get_tokenizer()\n",
    "    train_dataset = MimicCxrReports(root_text, path_splits, tokenizer, mode='novel_train')\n",
    "    test_dataset = MimicCxrReports(root_text, path_splits, tokenizer, mode='novel_validate')\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=bs, shuffle=True, num_workers=num_workers)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=bs, shuffle=True, num_workers=num_workers)\n",
    "\n",
    "    # Create Dataframe to export results to CSV\n",
    "    df_results = pd.DataFrame(columns=['Epoch', 'Training Loss', 'Validation Loss', 'Accuracy', 'Macro Accuracy',\n",
    "                                       'Macro-F1 Score'] + [str(x) + ' F1' for x in range(n_way)])\n",
    "\n",
    "    # Training\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = train(model, train_loader, criterion, device, optimizer)\n",
    "        val_loss, acc, m_acc, macro_f1, class_f1 = test(model, test_loader, criterion, device, n_way)\n",
    "\n",
    "        if (save_models):\n",
    "            torch.save(model.state_dict(), os.path.join(path_models, f'semantic_{epoch + 1}.pth'))  # Save the model\n",
    "\n",
    "        # Append and report results\n",
    "        df_results.loc[epoch] = [epoch + 1, train_loss, val_loss, acc, m_acc, macro_f1] + class_f1\n",
    "\n",
    "        print(\n",
    "            f'[{epoch + 1}] t_loss: {train_loss:.5f} v_loss: {val_loss:.5f} val_acc: {acc:.5f} '\n",
    "            f'val_m_acc: {m_acc:.5f} f1: {macro_f1:.5f}')\n",
    "\n",
    "    df_results.to_csv(path_results, index=False)  # Export results to a CSV file\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Learner(nn.Module):\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config, imgc, imgsz):\n",
    "        \"\"\"\n",
    "\n",
    "        :param config: network config file, type:list of (string, list)\n",
    "        :param imgc: 1 or 3\n",
    "        :param imgsz:  28 or 84\n",
    "        \"\"\"\n",
    "        super(Learner, self).__init__()\n",
    "\n",
    "\n",
    "        self.config = config\n",
    "\n",
    "        # this dict contains all tensors needed to be optimized\n",
    "        self.vars = nn.ParameterList()\n",
    "        # running_mean and running_var\n",
    "        self.vars_bn = nn.ParameterList()\n",
    "\n",
    "        for i, (name, param) in enumerate(self.config):\n",
    "            if name is 'conv2d':\n",
    "                # [ch_out, ch_in, kernelsz, kernelsz]\n",
    "                w = nn.Parameter(torch.ones(*param[:4]))\n",
    "                # gain=1 according to cbfin's implementation\n",
    "                torch.nn.init.kaiming_normal_(w)\n",
    "                self.vars.append(w)\n",
    "                # [ch_out]\n",
    "                self.vars.append(nn.Parameter(torch.zeros(param[0])))\n",
    "\n",
    "            elif name is 'convt2d':\n",
    "                # [ch_in, ch_out, kernelsz, kernelsz, stride, padding]\n",
    "                w = nn.Parameter(torch.ones(*param[:4]))\n",
    "                # gain=1 according to cbfin's implementation\n",
    "                torch.nn.init.kaiming_normal_(w)\n",
    "                self.vars.append(w)\n",
    "                # [ch_in, ch_out]\n",
    "                self.vars.append(nn.Parameter(torch.zeros(param[1])))\n",
    "\n",
    "            elif name is 'linear':\n",
    "                # [ch_out, ch_in]\n",
    "                w = nn.Parameter(torch.ones(*param))\n",
    "                # gain=1 according to cbfinn's implementation\n",
    "                torch.nn.init.kaiming_normal_(w)\n",
    "                self.vars.append(w)\n",
    "                # [ch_out]\n",
    "                self.vars.append(nn.Parameter(torch.zeros(param[0])))\n",
    "\n",
    "            elif name is 'bn':\n",
    "                # [ch_out]\n",
    "                w = nn.Parameter(torch.ones(param[0]))\n",
    "                self.vars.append(w)\n",
    "                # [ch_out]\n",
    "                self.vars.append(nn.Parameter(torch.zeros(param[0])))\n",
    "\n",
    "                # must set requires_grad=False\n",
    "                running_mean = nn.Parameter(torch.zeros(param[0]), requires_grad=False)\n",
    "                running_var = nn.Parameter(torch.ones(param[0]), requires_grad=False)\n",
    "                self.vars_bn.extend([running_mean, running_var])\n",
    "\n",
    "\n",
    "            elif name in ['tanh', 'relu', 'upsample', 'avg_pool2d', 'max_pool2d',\n",
    "                          'flatten', 'reshape', 'leakyrelu', 'sigmoid']:\n",
    "                continue\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "\n",
    "    def extra_repr(self):\n",
    "        info = ''\n",
    "\n",
    "        for name, param in self.config:\n",
    "            if name is 'conv2d':\n",
    "                tmp = 'conv2d:(ch_in:%d, ch_out:%d, k:%dx%d, stride:%d, padding:%d)'\\\n",
    "                      %(param[1], param[0], param[2], param[3], param[4], param[5],)\n",
    "                info += tmp + '\\n'\n",
    "\n",
    "            elif name is 'convt2d':\n",
    "                tmp = 'convTranspose2d:(ch_in:%d, ch_out:%d, k:%dx%d, stride:%d, padding:%d)'\\\n",
    "                      %(param[0], param[1], param[2], param[3], param[4], param[5],)\n",
    "                info += tmp + '\\n'\n",
    "\n",
    "            elif name is 'linear':\n",
    "                tmp = 'linear:(in:%d, out:%d)'%(param[1], param[0])\n",
    "                info += tmp + '\\n'\n",
    "\n",
    "            elif name is 'leakyrelu':\n",
    "                tmp = 'leakyrelu:(slope:%f)'%(param[0])\n",
    "                info += tmp + '\\n'\n",
    "\n",
    "\n",
    "            elif name is 'avg_pool2d':\n",
    "                tmp = 'avg_pool2d:(k:%d, stride:%d, padding:%d)'%(param[0], param[1], param[2])\n",
    "                info += tmp + '\\n'\n",
    "            elif name is 'max_pool2d':\n",
    "                tmp = 'max_pool2d:(k:%d, stride:%d, padding:%d)'%(param[0], param[1], param[2])\n",
    "                info += tmp + '\\n'\n",
    "            elif name in ['flatten', 'tanh', 'relu', 'upsample', 'reshape', 'sigmoid', 'use_logits', 'bn']:\n",
    "                tmp = name + ':' + str(tuple(param))\n",
    "                info += tmp + '\\n'\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "\n",
    "        return info\n",
    "\n",
    "    def forward(self, x, vars=None, bn_training=True):\n",
    "        \"\"\"\n",
    "        This function can be called by finetunning, however, in finetunning, we dont wish to update\n",
    "        running_mean/running_var. Thought weights/bias of bn is updated, it has been separated by fast_weights.\n",
    "        Indeed, to not update running_mean/running_var, we need set update_bn_statistics=False\n",
    "        but weight/bias will be updated and not dirty initial theta parameters via fast_weiths.\n",
    "        :param x: [b, 1, 28, 28]\n",
    "        :param vars:\n",
    "        :param bn_training: set False to not update\n",
    "        :return: x, loss, likelihood, kld\n",
    "        \"\"\"\n",
    "\n",
    "        if vars is None:\n",
    "            vars = self.vars\n",
    "\n",
    "        idx = 0\n",
    "        bn_idx = 0\n",
    "\n",
    "        for name, param in self.config:\n",
    "            if name is 'conv2d':\n",
    "                w, b = vars[idx], vars[idx + 1]\n",
    "                # remember to keep synchrozied of forward_encoder and forward_decoder!\n",
    "                x = F.conv2d(x, w, b, stride=param[4], padding=param[5])\n",
    "                idx += 2\n",
    "                # print(name, param, '\\tout:', x.shape)\n",
    "            elif name is 'convt2d':\n",
    "                w, b = vars[idx], vars[idx + 1]\n",
    "                # remember to keep synchrozied of forward_encoder and forward_decoder!\n",
    "                x = F.conv_transpose2d(x, w, b, stride=param[4], padding=param[5])\n",
    "                idx += 2\n",
    "                # print(name, param, '\\tout:', x.shape)\n",
    "            elif name is 'linear':\n",
    "                w, b = vars[idx], vars[idx + 1]\n",
    "                x = F.linear(x, w, b)\n",
    "                idx += 2\n",
    "                # print('forward:', idx, x.norm().item())\n",
    "            elif name is 'bn':\n",
    "                w, b = vars[idx], vars[idx + 1]\n",
    "                running_mean, running_var = self.vars_bn[bn_idx], self.vars_bn[bn_idx+1]\n",
    "                x = F.batch_norm(x, running_mean, running_var, weight=w, bias=b, training=bn_training)\n",
    "                idx += 2\n",
    "                bn_idx += 2\n",
    "\n",
    "            elif name is 'flatten':\n",
    "                # print(x.shape)\n",
    "                x = x.view(x.size(0), -1)\n",
    "            elif name is 'reshape':\n",
    "                # [b, 8] => [b, 2, 2, 2]\n",
    "                x = x.view(x.size(0), *param)\n",
    "            elif name is 'relu':\n",
    "                x = F.relu(x, inplace=param[0])\n",
    "            elif name is 'leakyrelu':\n",
    "                x = F.leaky_relu(x, negative_slope=param[0], inplace=param[1])\n",
    "            elif name is 'tanh':\n",
    "                x = F.tanh(x)\n",
    "            elif name is 'sigmoid':\n",
    "                x = torch.sigmoid(x)\n",
    "            elif name is 'upsample':\n",
    "                x = F.upsample_nearest(x, scale_factor=param[0])\n",
    "            elif name is 'max_pool2d':\n",
    "                x = F.max_pool2d(x, param[0], param[1], param[2])\n",
    "            elif name is 'avg_pool2d':\n",
    "                x = F.avg_pool2d(x, param[0], param[1], param[2])\n",
    "\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "\n",
    "        # make sure variable is used properly\n",
    "        assert idx == len(vars)\n",
    "        assert bn_idx == len(self.vars_bn)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def zero_grad(self, vars=None):\n",
    "        \"\"\"\n",
    "\n",
    "        :param vars:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            if vars is None:\n",
    "                for p in self.vars:\n",
    "                    if p.grad is not None:\n",
    "                        p.grad.zero_()\n",
    "            else:\n",
    "                for p in vars:\n",
    "                    if p.grad is not None:\n",
    "                        p.grad.zero_()\n",
    "\n",
    "    def parameters(self):\n",
    "        \"\"\"\n",
    "        override this function since initial parameters will return with a generator.\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        return self.vars\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
