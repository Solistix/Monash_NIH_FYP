{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[v_loss: 1.09525 val_acc: 0.41667 val_m_acc: 0.41667 f1: 0.00000\n",
      "[v_loss: 1.10243 val_acc: 0.35417 val_m_acc: 0.35417 f1: 0.33184\n",
      "[v_loss: 1.09105 val_acc: 0.39583 val_m_acc: 0.39583 f1: 0.33434\n",
      "[v_loss: 1.09488 val_acc: 0.43750 val_m_acc: 0.43750 f1: 0.43459\n",
      "[v_loss: 1.09399 val_acc: 0.31250 val_m_acc: 0.31250 f1: 0.28654\n",
      "[v_loss: 1.09397 val_acc: 0.35417 val_m_acc: 0.35417 f1: 0.35659\n",
      "[v_loss: 1.10067 val_acc: 0.33333 val_m_acc: 0.33333 f1: 0.32194\n",
      "[v_loss: 1.10131 val_acc: 0.25000 val_m_acc: 0.25000 f1: 0.00000\n",
      "[v_loss: 1.09945 val_acc: 0.27083 val_m_acc: 0.27083 f1: 0.27666\n",
      "[v_loss: 1.08890 val_acc: 0.37500 val_m_acc: 0.37500 f1: 0.36150\n",
      "[v_loss: 1.10344 val_acc: 0.41667 val_m_acc: 0.41667 f1: 0.37267\n",
      "[v_loss: 1.09031 val_acc: 0.45833 val_m_acc: 0.45833 f1: 0.43407\n",
      "[v_loss: 1.09506 val_acc: 0.45833 val_m_acc: 0.45833 f1: 0.42097\n",
      "[v_loss: 1.09878 val_acc: 0.29167 val_m_acc: 0.29167 f1: 0.28472\n",
      "[v_loss: 1.09857 val_acc: 0.27083 val_m_acc: 0.27083 f1: 0.00000\n",
      "[v_loss: 1.09447 val_acc: 0.39583 val_m_acc: 0.39583 f1: 0.38617\n",
      "[v_loss: 1.10231 val_acc: 0.35417 val_m_acc: 0.35417 f1: 0.00000\n",
      "[v_loss: 1.09566 val_acc: 0.39583 val_m_acc: 0.39583 f1: 0.37472\n",
      "[v_loss: 1.08975 val_acc: 0.47917 val_m_acc: 0.47917 f1: 0.42732\n",
      "[v_loss: 1.09349 val_acc: 0.41667 val_m_acc: 0.41667 f1: 0.39728\n",
      "[v_loss: 1.09001 val_acc: 0.43750 val_m_acc: 0.43750 f1: 0.43603\n",
      "[v_loss: 1.10526 val_acc: 0.27083 val_m_acc: 0.27083 f1: 0.00000\n",
      "[v_loss: 1.09885 val_acc: 0.27083 val_m_acc: 0.27083 f1: 0.00000\n",
      "[v_loss: 1.09719 val_acc: 0.33333 val_m_acc: 0.33333 f1: 0.32147\n",
      "[v_loss: 1.10393 val_acc: 0.31250 val_m_acc: 0.31250 f1: 0.28232\n",
      "[v_loss: 1.09766 val_acc: 0.31250 val_m_acc: 0.31250 f1: 0.00000\n",
      "[v_loss: 1.09673 val_acc: 0.41667 val_m_acc: 0.41667 f1: 0.36480\n",
      "[v_loss: 1.09951 val_acc: 0.35417 val_m_acc: 0.35417 f1: 0.00000\n",
      "[v_loss: 1.09928 val_acc: 0.27083 val_m_acc: 0.27083 f1: 0.27048\n",
      "[v_loss: 1.10069 val_acc: 0.35417 val_m_acc: 0.35417 f1: 0.31054\n",
      "[v_loss: 1.10203 val_acc: 0.29167 val_m_acc: 0.29167 f1: 0.00000\n",
      "[v_loss: 1.09289 val_acc: 0.41667 val_m_acc: 0.41667 f1: 0.38705\n",
      "[v_loss: 1.10544 val_acc: 0.29167 val_m_acc: 0.29167 f1: 0.00000\n",
      "[v_loss: 1.10037 val_acc: 0.33333 val_m_acc: 0.33333 f1: 0.29079\n",
      "[v_loss: 1.08953 val_acc: 0.37500 val_m_acc: 0.37500 f1: 0.31818\n",
      "[v_loss: 1.09948 val_acc: 0.37500 val_m_acc: 0.37500 f1: 0.36372\n",
      "[v_loss: 1.10393 val_acc: 0.29167 val_m_acc: 0.29167 f1: 0.00000\n",
      "[v_loss: 1.09861 val_acc: 0.33333 val_m_acc: 0.33333 f1: 0.31191\n",
      "[v_loss: 1.08932 val_acc: 0.43750 val_m_acc: 0.43750 f1: 0.43585\n",
      "[v_loss: 1.09631 val_acc: 0.31250 val_m_acc: 0.31250 f1: 0.00000\n",
      "[v_loss: 1.10280 val_acc: 0.31250 val_m_acc: 0.31250 f1: 0.00000\n",
      "[v_loss: 1.08759 val_acc: 0.45833 val_m_acc: 0.45833 f1: 0.41667\n",
      "[v_loss: 1.09442 val_acc: 0.31250 val_m_acc: 0.31250 f1: 0.00000\n",
      "[v_loss: 1.10418 val_acc: 0.31250 val_m_acc: 0.31250 f1: 0.26530\n",
      "[v_loss: 1.10698 val_acc: 0.33333 val_m_acc: 0.33333 f1: 0.32906\n",
      "[v_loss: 1.10244 val_acc: 0.29167 val_m_acc: 0.29167 f1: 0.00000\n",
      "[v_loss: 1.09454 val_acc: 0.37500 val_m_acc: 0.37500 f1: 0.36479\n",
      "[v_loss: 1.09840 val_acc: 0.35417 val_m_acc: 0.35417 f1: 0.00000\n",
      "[v_loss: 1.10332 val_acc: 0.25000 val_m_acc: 0.25000 f1: 0.24451\n",
      "[v_loss: 1.09597 val_acc: 0.33333 val_m_acc: 0.33333 f1: 0.33522\n",
      "[v_loss: 1.10730 val_acc: 0.29167 val_m_acc: 0.29167 f1: 0.28222\n",
      "[v_loss: 1.09703 val_acc: 0.33333 val_m_acc: 0.33333 f1: 0.25355\n",
      "[v_loss: 1.10425 val_acc: 0.29167 val_m_acc: 0.29167 f1: 0.29132\n",
      "[v_loss: 1.10046 val_acc: 0.33333 val_m_acc: 0.33333 f1: 0.29529\n",
      "[v_loss: 1.09323 val_acc: 0.41667 val_m_acc: 0.41667 f1: 0.41148\n",
      "[v_loss: 1.10777 val_acc: 0.31250 val_m_acc: 0.31250 f1: 0.32238\n",
      "[v_loss: 1.10429 val_acc: 0.22917 val_m_acc: 0.22917 f1: 0.21763\n",
      "[v_loss: 1.09235 val_acc: 0.39583 val_m_acc: 0.39583 f1: 0.39739\n",
      "[v_loss: 1.09804 val_acc: 0.39583 val_m_acc: 0.39583 f1: 0.39232\n",
      "[v_loss: 1.09727 val_acc: 0.39583 val_m_acc: 0.39583 f1: 0.37967\n",
      "[v_loss: 1.09264 val_acc: 0.47917 val_m_acc: 0.47917 f1: 0.48199\n",
      "[v_loss: 1.09121 val_acc: 0.35417 val_m_acc: 0.35417 f1: 0.32729\n",
      "[v_loss: 1.09091 val_acc: 0.43750 val_m_acc: 0.43750 f1: 0.44746\n",
      "[v_loss: 1.09172 val_acc: 0.41667 val_m_acc: 0.41667 f1: 0.36525\n",
      "[v_loss: 1.09402 val_acc: 0.41667 val_m_acc: 0.41667 f1: 0.41937\n",
      "[v_loss: 1.09576 val_acc: 0.33333 val_m_acc: 0.33333 f1: 0.28515\n",
      "[v_loss: 1.08908 val_acc: 0.45833 val_m_acc: 0.45833 f1: 0.41618\n",
      "[v_loss: 1.09094 val_acc: 0.50000 val_m_acc: 0.50000 f1: 0.00000\n",
      "[v_loss: 1.11059 val_acc: 0.20833 val_m_acc: 0.20833 f1: 0.00000\n",
      "[v_loss: 1.10180 val_acc: 0.33333 val_m_acc: 0.33333 f1: 0.29409\n",
      "[v_loss: 1.10977 val_acc: 0.29167 val_m_acc: 0.29167 f1: 0.00000\n",
      "[v_loss: 1.10201 val_acc: 0.37500 val_m_acc: 0.37500 f1: 0.00000\n",
      "[v_loss: 1.09774 val_acc: 0.35417 val_m_acc: 0.35417 f1: 0.00000\n",
      "[v_loss: 1.10013 val_acc: 0.37500 val_m_acc: 0.37500 f1: 0.37807\n",
      "[v_loss: 1.09349 val_acc: 0.37500 val_m_acc: 0.37500 f1: 0.36264\n",
      "[v_loss: 1.10774 val_acc: 0.35417 val_m_acc: 0.35417 f1: 0.29164\n",
      "[v_loss: 1.10266 val_acc: 0.25000 val_m_acc: 0.25000 f1: 0.00000\n",
      "[v_loss: 1.09997 val_acc: 0.37500 val_m_acc: 0.37500 f1: 0.34634\n",
      "[v_loss: 1.10246 val_acc: 0.29167 val_m_acc: 0.29167 f1: 0.00000\n",
      "[v_loss: 1.08824 val_acc: 0.41667 val_m_acc: 0.41667 f1: 0.36502\n",
      "[v_loss: 1.10467 val_acc: 0.27083 val_m_acc: 0.27083 f1: 0.00000\n",
      "[v_loss: 1.09402 val_acc: 0.31250 val_m_acc: 0.31250 f1: 0.28219\n",
      "[v_loss: 1.09116 val_acc: 0.37500 val_m_acc: 0.37500 f1: 0.36883\n",
      "[v_loss: 1.10604 val_acc: 0.25000 val_m_acc: 0.25000 f1: 0.00000\n",
      "[v_loss: 1.09664 val_acc: 0.29167 val_m_acc: 0.29167 f1: 0.23206\n",
      "[v_loss: 1.09207 val_acc: 0.45833 val_m_acc: 0.45833 f1: 0.45643\n",
      "[v_loss: 1.09609 val_acc: 0.31250 val_m_acc: 0.31250 f1: 0.00000\n",
      "[v_loss: 1.09826 val_acc: 0.31250 val_m_acc: 0.31250 f1: 0.00000\n",
      "[v_loss: 1.10400 val_acc: 0.27083 val_m_acc: 0.27083 f1: 0.25948\n",
      "[v_loss: 1.09814 val_acc: 0.39583 val_m_acc: 0.39583 f1: 0.35284\n",
      "[v_loss: 1.10754 val_acc: 0.29167 val_m_acc: 0.29167 f1: 0.25243\n",
      "[v_loss: 1.10523 val_acc: 0.27083 val_m_acc: 0.27083 f1: 0.26596\n",
      "[v_loss: 1.10208 val_acc: 0.35417 val_m_acc: 0.35417 f1: 0.31238\n",
      "[v_loss: 1.09793 val_acc: 0.22917 val_m_acc: 0.22917 f1: 0.21194\n",
      "[v_loss: 1.09663 val_acc: 0.35417 val_m_acc: 0.35417 f1: 0.00000\n",
      "[v_loss: 1.09509 val_acc: 0.39583 val_m_acc: 0.39583 f1: 0.37796\n",
      "[v_loss: 1.09757 val_acc: 0.33333 val_m_acc: 0.33333 f1: 0.32919\n",
      "[v_loss: 1.10263 val_acc: 0.31250 val_m_acc: 0.31250 f1: 0.30162\n",
      "[v_loss: 1.10505 val_acc: 0.27083 val_m_acc: 0.27083 f1: 0.23915\n",
      "[v_loss: 1.09123 val_acc: 0.43750 val_m_acc: 0.43750 f1: 0.38828\n",
      "[v_loss: 1.09662 val_acc: 0.39583 val_m_acc: 0.39583 f1: 0.34059\n",
      "[v_loss: 1.10183 val_acc: 0.37500 val_m_acc: 0.37500 f1: 0.00000\n",
      "[v_loss: 1.09076 val_acc: 0.45833 val_m_acc: 0.45833 f1: 0.45503\n",
      "[v_loss: 1.09050 val_acc: 0.56250 val_m_acc: 0.56250 f1: 0.56079\n",
      "[v_loss: 1.10078 val_acc: 0.35417 val_m_acc: 0.35417 f1: 0.33059\n",
      "[v_loss: 1.10530 val_acc: 0.37500 val_m_acc: 0.37500 f1: 0.00000\n",
      "[v_loss: 1.09778 val_acc: 0.33333 val_m_acc: 0.33333 f1: 0.31882\n",
      "[v_loss: 1.10209 val_acc: 0.35417 val_m_acc: 0.35417 f1: 0.31020\n",
      "[v_loss: 1.10224 val_acc: 0.35417 val_m_acc: 0.35417 f1: 0.34176\n",
      "[v_loss: 1.09868 val_acc: 0.31250 val_m_acc: 0.31250 f1: 0.27851\n",
      "[v_loss: 1.09608 val_acc: 0.35417 val_m_acc: 0.35417 f1: 0.00000\n",
      "[v_loss: 1.10791 val_acc: 0.33333 val_m_acc: 0.33333 f1: 0.27624\n",
      "[v_loss: 1.09698 val_acc: 0.37500 val_m_acc: 0.37500 f1: 0.00000\n",
      "[v_loss: 1.09677 val_acc: 0.35417 val_m_acc: 0.35417 f1: 0.35764\n",
      "[v_loss: 1.09426 val_acc: 0.33333 val_m_acc: 0.33333 f1: 0.33141\n",
      "[v_loss: 1.10201 val_acc: 0.29167 val_m_acc: 0.29167 f1: 0.25777\n",
      "[v_loss: 1.09661 val_acc: 0.31250 val_m_acc: 0.31250 f1: 0.30541\n",
      "[v_loss: 1.09025 val_acc: 0.37500 val_m_acc: 0.37500 f1: 0.36525\n",
      "[v_loss: 1.09679 val_acc: 0.33333 val_m_acc: 0.33333 f1: 0.00000\n",
      "[v_loss: 1.09217 val_acc: 0.45833 val_m_acc: 0.45833 f1: 0.42856\n",
      "[v_loss: 1.10399 val_acc: 0.33333 val_m_acc: 0.33333 f1: 0.00000\n",
      "[v_loss: 1.09006 val_acc: 0.47917 val_m_acc: 0.47917 f1: 0.45770\n",
      "[v_loss: 1.10782 val_acc: 0.27083 val_m_acc: 0.27083 f1: 0.27080\n",
      "[v_loss: 1.10102 val_acc: 0.29167 val_m_acc: 0.29167 f1: 0.28972\n",
      "[v_loss: 1.09391 val_acc: 0.35417 val_m_acc: 0.35417 f1: 0.00000\n",
      "[v_loss: 1.09835 val_acc: 0.33333 val_m_acc: 0.33333 f1: 0.31570\n",
      "[v_loss: 1.10110 val_acc: 0.37500 val_m_acc: 0.37500 f1: 0.32016\n",
      "[v_loss: 1.10267 val_acc: 0.27083 val_m_acc: 0.27083 f1: 0.25762\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[v_loss: 1.09583 val_acc: 0.39583 val_m_acc: 0.39583 f1: 0.00000\n",
      "[v_loss: 1.09264 val_acc: 0.33333 val_m_acc: 0.33333 f1: 0.27531\n",
      "[v_loss: 1.09435 val_acc: 0.35417 val_m_acc: 0.35417 f1: 0.29860\n",
      "[v_loss: 1.10668 val_acc: 0.31250 val_m_acc: 0.31250 f1: 0.29965\n",
      "[v_loss: 1.09520 val_acc: 0.31250 val_m_acc: 0.31250 f1: 0.30935\n",
      "[v_loss: 1.09867 val_acc: 0.35417 val_m_acc: 0.35417 f1: 0.32961\n",
      "[v_loss: 1.09575 val_acc: 0.35417 val_m_acc: 0.35417 f1: 0.33025\n",
      "[v_loss: 1.10318 val_acc: 0.25000 val_m_acc: 0.25000 f1: 0.22621\n",
      "[v_loss: 1.09763 val_acc: 0.39583 val_m_acc: 0.39583 f1: 0.38038\n",
      "[v_loss: 1.09548 val_acc: 0.31250 val_m_acc: 0.31250 f1: 0.30476\n",
      "[v_loss: 1.10276 val_acc: 0.27083 val_m_acc: 0.27083 f1: 0.26773\n",
      "[v_loss: 1.09518 val_acc: 0.37500 val_m_acc: 0.37500 f1: 0.00000\n",
      "[v_loss: 1.10298 val_acc: 0.27083 val_m_acc: 0.27083 f1: 0.23175\n",
      "[v_loss: 1.09702 val_acc: 0.39583 val_m_acc: 0.39583 f1: 0.32124\n",
      "[v_loss: 1.10210 val_acc: 0.31250 val_m_acc: 0.31250 f1: 0.28864\n",
      "[v_loss: 1.10766 val_acc: 0.25000 val_m_acc: 0.25000 f1: 0.23993\n",
      "[v_loss: 1.10747 val_acc: 0.25000 val_m_acc: 0.25000 f1: 0.24718\n",
      "[v_loss: 1.09536 val_acc: 0.25000 val_m_acc: 0.25000 f1: 0.24317\n",
      "[v_loss: 1.09848 val_acc: 0.27083 val_m_acc: 0.27083 f1: 0.25965\n",
      "[v_loss: 1.10216 val_acc: 0.33333 val_m_acc: 0.33333 f1: 0.32405\n",
      "[v_loss: 1.09234 val_acc: 0.35417 val_m_acc: 0.35417 f1: 0.34721\n",
      "[v_loss: 1.10163 val_acc: 0.31250 val_m_acc: 0.31250 f1: 0.28701\n",
      "[v_loss: 1.09267 val_acc: 0.39583 val_m_acc: 0.39583 f1: 0.37855\n",
      "[v_loss: 1.09795 val_acc: 0.37500 val_m_acc: 0.37500 f1: 0.34516\n",
      "[v_loss: 1.09912 val_acc: 0.31250 val_m_acc: 0.31250 f1: 0.28724\n",
      "[v_loss: 1.09764 val_acc: 0.39583 val_m_acc: 0.39583 f1: 0.39899\n",
      "[v_loss: 1.10238 val_acc: 0.31250 val_m_acc: 0.31250 f1: 0.22266\n",
      "[v_loss: 1.09908 val_acc: 0.39583 val_m_acc: 0.39583 f1: 0.32600\n",
      "[v_loss: 1.10563 val_acc: 0.31250 val_m_acc: 0.31250 f1: 0.26863\n",
      "[v_loss: 1.09698 val_acc: 0.27083 val_m_acc: 0.27083 f1: 0.25684\n",
      "[v_loss: 1.09805 val_acc: 0.31250 val_m_acc: 0.31250 f1: 0.00000\n",
      "[v_loss: 1.09646 val_acc: 0.33333 val_m_acc: 0.33333 f1: 0.00000\n",
      "[v_loss: 1.10325 val_acc: 0.29167 val_m_acc: 0.29167 f1: 0.29317\n",
      "[v_loss: 1.10423 val_acc: 0.31250 val_m_acc: 0.31250 f1: 0.28360\n",
      "[v_loss: 1.08528 val_acc: 0.35417 val_m_acc: 0.35417 f1: 0.36791\n",
      "[v_loss: 1.09717 val_acc: 0.39583 val_m_acc: 0.39583 f1: 0.38240\n",
      "[v_loss: 1.10031 val_acc: 0.33333 val_m_acc: 0.33333 f1: 0.00000\n",
      "[v_loss: 1.10510 val_acc: 0.31250 val_m_acc: 0.31250 f1: 0.26319\n",
      "[v_loss: 1.09705 val_acc: 0.37500 val_m_acc: 0.37500 f1: 0.35561\n",
      "[v_loss: 1.10301 val_acc: 0.27083 val_m_acc: 0.27083 f1: 0.00000\n",
      "[v_loss: 1.10357 val_acc: 0.31250 val_m_acc: 0.31250 f1: 0.00000\n",
      "[v_loss: 1.09863 val_acc: 0.27083 val_m_acc: 0.27083 f1: 0.24837\n",
      "[v_loss: 1.09301 val_acc: 0.37500 val_m_acc: 0.37500 f1: 0.34650\n",
      "[v_loss: 1.10737 val_acc: 0.35417 val_m_acc: 0.35417 f1: 0.27996\n",
      "[v_loss: 1.09395 val_acc: 0.39583 val_m_acc: 0.39583 f1: 0.36567\n",
      "[v_loss: 1.09759 val_acc: 0.35417 val_m_acc: 0.35417 f1: 0.35968\n",
      "[v_loss: 1.09545 val_acc: 0.27083 val_m_acc: 0.27083 f1: 0.24257\n",
      "[v_loss: 1.10404 val_acc: 0.33333 val_m_acc: 0.33333 f1: 0.29500\n",
      "[v_loss: 1.09111 val_acc: 0.39583 val_m_acc: 0.39583 f1: 0.38175\n",
      "[v_loss: 1.10069 val_acc: 0.35417 val_m_acc: 0.35417 f1: 0.31675\n",
      "[v_loss: 1.11336 val_acc: 0.20833 val_m_acc: 0.20833 f1: 0.20416\n",
      "[v_loss: 1.09454 val_acc: 0.37500 val_m_acc: 0.37500 f1: 0.36599\n",
      "[v_loss: 1.09554 val_acc: 0.37500 val_m_acc: 0.37500 f1: 0.34920\n",
      "[v_loss: 1.09257 val_acc: 0.52083 val_m_acc: 0.52083 f1: 0.49472\n",
      "[v_loss: 1.09946 val_acc: 0.35417 val_m_acc: 0.35417 f1: 0.00000\n",
      "[v_loss: 1.09978 val_acc: 0.37500 val_m_acc: 0.37500 f1: 0.35280\n",
      "[v_loss: 1.08912 val_acc: 0.50000 val_m_acc: 0.50000 f1: 0.47871\n",
      "[v_loss: 1.09857 val_acc: 0.35417 val_m_acc: 0.35417 f1: 0.00000\n",
      "[v_loss: 1.10052 val_acc: 0.33333 val_m_acc: 0.33333 f1: 0.27067\n",
      "[v_loss: 1.10161 val_acc: 0.33333 val_m_acc: 0.33333 f1: 0.31195\n",
      "[v_loss: 1.09701 val_acc: 0.39583 val_m_acc: 0.39583 f1: 0.37275\n",
      "[v_loss: 1.09421 val_acc: 0.39583 val_m_acc: 0.39583 f1: 0.38914\n",
      "[v_loss: 1.08345 val_acc: 0.47917 val_m_acc: 0.47917 f1: 0.45897\n",
      "[v_loss: 1.09685 val_acc: 0.37500 val_m_acc: 0.37500 f1: 0.32006\n",
      "[v_loss: 1.09803 val_acc: 0.39583 val_m_acc: 0.39583 f1: 0.32538\n",
      "[v_loss: 1.09384 val_acc: 0.39583 val_m_acc: 0.39583 f1: 0.37859\n",
      "[v_loss: 1.09567 val_acc: 0.35417 val_m_acc: 0.35417 f1: 0.30394\n",
      "[v_loss: 1.09073 val_acc: 0.37500 val_m_acc: 0.37500 f1: 0.32581\n",
      "[v_loss: 1.09667 val_acc: 0.43750 val_m_acc: 0.43750 f1: 0.42211\n",
      "[v_loss: 1.09751 val_acc: 0.27083 val_m_acc: 0.27083 f1: 0.26420\n",
      "[v_loss: 1.09430 val_acc: 0.43750 val_m_acc: 0.43750 f1: 0.43541\n",
      "[v_loss: 1.09515 val_acc: 0.39583 val_m_acc: 0.39583 f1: 0.33017\n",
      "[v_loss: 1.10244 val_acc: 0.27083 val_m_acc: 0.27083 f1: 0.00000\n",
      "[v_loss: 1.09198 val_acc: 0.43750 val_m_acc: 0.43750 f1: 0.43123\n",
      "[v_loss: 1.09925 val_acc: 0.22917 val_m_acc: 0.22917 f1: 0.21115\n",
      "[v_loss: 1.10028 val_acc: 0.31250 val_m_acc: 0.31250 f1: 0.28716\n",
      "[v_loss: 1.08998 val_acc: 0.43750 val_m_acc: 0.43750 f1: 0.39348\n",
      "[v_loss: 1.09350 val_acc: 0.39583 val_m_acc: 0.39583 f1: 0.39135\n",
      "[v_loss: 1.10232 val_acc: 0.20833 val_m_acc: 0.20833 f1: 0.19444\n",
      "[v_loss: 1.10254 val_acc: 0.33333 val_m_acc: 0.33333 f1: 0.30353\n",
      "[v_loss: 1.09991 val_acc: 0.29167 val_m_acc: 0.29167 f1: 0.29183\n",
      "[v_loss: 1.09745 val_acc: 0.41667 val_m_acc: 0.41667 f1: 0.39359\n",
      "[v_loss: 1.09990 val_acc: 0.29167 val_m_acc: 0.29167 f1: 0.28288\n",
      "[v_loss: 1.09484 val_acc: 0.35417 val_m_acc: 0.35417 f1: 0.34190\n",
      "[v_loss: 1.09933 val_acc: 0.31250 val_m_acc: 0.31250 f1: 0.30891\n",
      "[v_loss: 1.09764 val_acc: 0.29167 val_m_acc: 0.29167 f1: 0.28994\n",
      "[v_loss: 1.09395 val_acc: 0.41667 val_m_acc: 0.41667 f1: 0.39534\n",
      "[v_loss: 1.09815 val_acc: 0.33333 val_m_acc: 0.33333 f1: 0.32110\n",
      "[v_loss: 1.09993 val_acc: 0.39583 val_m_acc: 0.39583 f1: 0.39675\n",
      "[v_loss: 1.09793 val_acc: 0.35417 val_m_acc: 0.35417 f1: 0.30223\n",
      "[v_loss: 1.09865 val_acc: 0.29167 val_m_acc: 0.29167 f1: 0.29109\n",
      "[v_loss: 1.08550 val_acc: 0.47917 val_m_acc: 0.47917 f1: 0.44831\n",
      "[v_loss: 1.09232 val_acc: 0.41667 val_m_acc: 0.41667 f1: 0.41813\n",
      "[v_loss: 1.10150 val_acc: 0.20833 val_m_acc: 0.20833 f1: 0.19634\n",
      "[v_loss: 1.08956 val_acc: 0.43750 val_m_acc: 0.43750 f1: 0.41901\n",
      "[v_loss: 1.09465 val_acc: 0.50000 val_m_acc: 0.50000 f1: 0.49177\n",
      "[v_loss: 1.09578 val_acc: 0.33333 val_m_acc: 0.33333 f1: 0.33697\n",
      "[v_loss: 1.10195 val_acc: 0.22917 val_m_acc: 0.22917 f1: 0.00000\n",
      "[v_loss: 1.09442 val_acc: 0.39583 val_m_acc: 0.39583 f1: 0.39671\n",
      "[v_loss: 1.09418 val_acc: 0.27083 val_m_acc: 0.27083 f1: 0.24792\n",
      "[v_loss: 1.09525 val_acc: 0.41667 val_m_acc: 0.41667 f1: 0.38696\n",
      "[v_loss: 1.09477 val_acc: 0.35417 val_m_acc: 0.35417 f1: 0.34663\n",
      "[v_loss: 1.09676 val_acc: 0.37500 val_m_acc: 0.37500 f1: 0.36030\n",
      "[v_loss: 1.09543 val_acc: 0.41667 val_m_acc: 0.41667 f1: 0.39155\n",
      "[v_loss: 1.10356 val_acc: 0.31250 val_m_acc: 0.31250 f1: 0.26194\n",
      "[v_loss: 1.09571 val_acc: 0.41667 val_m_acc: 0.41667 f1: 0.41381\n",
      "[v_loss: 1.09241 val_acc: 0.33333 val_m_acc: 0.33333 f1: 0.32963\n",
      "[v_loss: 1.09756 val_acc: 0.39583 val_m_acc: 0.39583 f1: 0.38668\n",
      "[v_loss: 1.09771 val_acc: 0.37500 val_m_acc: 0.37500 f1: 0.37008\n",
      "[v_loss: 1.09374 val_acc: 0.37500 val_m_acc: 0.37500 f1: 0.37365\n",
      "[v_loss: 1.09369 val_acc: 0.37500 val_m_acc: 0.37500 f1: 0.37328\n",
      "[v_loss: 1.09517 val_acc: 0.43750 val_m_acc: 0.43750 f1: 0.41667\n",
      "[v_loss: 1.10045 val_acc: 0.39583 val_m_acc: 0.39583 f1: 0.38617\n",
      "[v_loss: 1.09778 val_acc: 0.39583 val_m_acc: 0.39583 f1: 0.38132\n",
      "[v_loss: 1.09497 val_acc: 0.37500 val_m_acc: 0.37500 f1: 0.36035\n",
      "[v_loss: 1.10102 val_acc: 0.31250 val_m_acc: 0.31250 f1: 0.30226\n",
      "[v_loss: 1.09651 val_acc: 0.33333 val_m_acc: 0.33333 f1: 0.32092\n",
      "[v_loss: 1.09273 val_acc: 0.41667 val_m_acc: 0.41667 f1: 0.39821\n",
      "[v_loss: 1.10005 val_acc: 0.39583 val_m_acc: 0.39583 f1: 0.00000\n",
      "[v_loss: 1.09511 val_acc: 0.43750 val_m_acc: 0.43750 f1: 0.41558\n",
      "[v_loss: 1.09324 val_acc: 0.50000 val_m_acc: 0.50000 f1: 0.47897\n",
      "[v_loss: 1.10134 val_acc: 0.35417 val_m_acc: 0.35417 f1: 0.33510\n",
      "[v_loss: 1.09080 val_acc: 0.39583 val_m_acc: 0.39583 f1: 0.38990\n",
      "[v_loss: 1.09734 val_acc: 0.35417 val_m_acc: 0.35417 f1: 0.34885\n",
      "[v_loss: 1.10115 val_acc: 0.29167 val_m_acc: 0.29167 f1: 0.29351\n",
      "[v_loss: 1.09786 val_acc: 0.35417 val_m_acc: 0.35417 f1: 0.00000\n",
      "[v_loss: 1.09737 val_acc: 0.31250 val_m_acc: 0.31250 f1: 0.31882\n",
      "[v_loss: 1.09880 val_acc: 0.25000 val_m_acc: 0.25000 f1: 0.25011\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[v_loss: 1.09549 val_acc: 0.33333 val_m_acc: 0.33333 f1: 0.30028\n",
      "[v_loss: 1.10276 val_acc: 0.35417 val_m_acc: 0.35417 f1: 0.32050\n",
      "[v_loss: 1.09771 val_acc: 0.37500 val_m_acc: 0.37500 f1: 0.37592\n",
      "[v_loss: 1.09484 val_acc: 0.39583 val_m_acc: 0.39583 f1: 0.38168\n",
      "[v_loss: 1.10175 val_acc: 0.35417 val_m_acc: 0.35417 f1: 0.35570\n",
      "[v_loss: 1.10424 val_acc: 0.20833 val_m_acc: 0.20833 f1: 0.19776\n",
      "[v_loss: 1.09830 val_acc: 0.37500 val_m_acc: 0.37500 f1: 0.36485\n",
      "[v_loss: 1.09517 val_acc: 0.47917 val_m_acc: 0.47917 f1: 0.00000\n",
      "[v_loss: 1.10063 val_acc: 0.37500 val_m_acc: 0.37500 f1: 0.34026\n",
      "[v_loss: 1.10150 val_acc: 0.27083 val_m_acc: 0.27083 f1: 0.00000\n",
      "[v_loss: 1.09718 val_acc: 0.41667 val_m_acc: 0.41667 f1: 0.40720\n",
      "[v_loss: 1.09709 val_acc: 0.39583 val_m_acc: 0.39583 f1: 0.38966\n",
      "[v_loss: 1.09701 val_acc: 0.29167 val_m_acc: 0.29167 f1: 0.29057\n",
      "[v_loss: 1.09719 val_acc: 0.35417 val_m_acc: 0.35417 f1: 0.34365\n",
      "[v_loss: 1.10107 val_acc: 0.37500 val_m_acc: 0.37500 f1: 0.36483\n",
      "[v_loss: 1.09792 val_acc: 0.31250 val_m_acc: 0.31250 f1: 0.27727\n",
      "[v_loss: 1.09859 val_acc: 0.37500 val_m_acc: 0.37500 f1: 0.37272\n",
      "[v_loss: 1.09862 val_acc: 0.35417 val_m_acc: 0.35417 f1: 0.34446\n",
      "[v_loss: 1.09707 val_acc: 0.41667 val_m_acc: 0.41667 f1: 0.41212\n",
      "[v_loss: 1.09856 val_acc: 0.39583 val_m_acc: 0.39583 f1: 0.38207\n",
      "[v_loss: 1.09036 val_acc: 0.45833 val_m_acc: 0.45833 f1: 0.45185\n",
      "[v_loss: 1.09692 val_acc: 0.37500 val_m_acc: 0.37500 f1: 0.38017\n",
      "[v_loss: 1.10235 val_acc: 0.31250 val_m_acc: 0.31250 f1: 0.32338\n",
      "[v_loss: 1.09795 val_acc: 0.35417 val_m_acc: 0.35417 f1: 0.33333\n",
      "[v_loss: 1.09651 val_acc: 0.37500 val_m_acc: 0.37500 f1: 0.35070\n",
      "[v_loss: 1.09208 val_acc: 0.41667 val_m_acc: 0.41667 f1: 0.40217\n",
      "[v_loss: 1.09325 val_acc: 0.45833 val_m_acc: 0.45833 f1: 0.42143\n",
      "[v_loss: 1.09965 val_acc: 0.33333 val_m_acc: 0.33333 f1: 0.32752\n",
      "[v_loss: 1.09629 val_acc: 0.41667 val_m_acc: 0.41667 f1: 0.39685\n",
      "[v_loss: 1.09492 val_acc: 0.43750 val_m_acc: 0.43750 f1: 0.43899\n",
      "[v_loss: 1.09535 val_acc: 0.39583 val_m_acc: 0.39583 f1: 0.34529\n",
      "[v_loss: 1.09538 val_acc: 0.35417 val_m_acc: 0.35417 f1: 0.34975\n",
      "[v_loss: 1.09561 val_acc: 0.37500 val_m_acc: 0.37500 f1: 0.33487\n",
      "[v_loss: 1.09462 val_acc: 0.41667 val_m_acc: 0.41667 f1: 0.41439\n",
      "[v_loss: 1.09675 val_acc: 0.37500 val_m_acc: 0.37500 f1: 0.37529\n",
      "[v_loss: 1.08792 val_acc: 0.52083 val_m_acc: 0.52083 f1: 0.51362\n",
      "[v_loss: 1.09527 val_acc: 0.31250 val_m_acc: 0.31250 f1: 0.30759\n",
      "[v_loss: 1.09768 val_acc: 0.31250 val_m_acc: 0.31250 f1: 0.31104\n",
      "[v_loss: 1.10017 val_acc: 0.31250 val_m_acc: 0.31250 f1: 0.31156\n",
      "[v_loss: 1.09672 val_acc: 0.37500 val_m_acc: 0.37500 f1: 0.34532\n",
      "[v_loss: 1.09822 val_acc: 0.27083 val_m_acc: 0.27083 f1: 0.26201\n",
      "[v_loss: 1.10397 val_acc: 0.16667 val_m_acc: 0.16667 f1: 0.00000\n",
      "[v_loss: 1.08846 val_acc: 0.43750 val_m_acc: 0.43750 f1: 0.42352\n",
      "[v_loss: 1.09350 val_acc: 0.35417 val_m_acc: 0.35417 f1: 0.35210\n",
      "[v_loss: 1.09756 val_acc: 0.29167 val_m_acc: 0.29167 f1: 0.27407\n",
      "[v_loss: 1.08957 val_acc: 0.52083 val_m_acc: 0.52083 f1: 0.52007\n",
      "[v_loss: 1.09967 val_acc: 0.25000 val_m_acc: 0.25000 f1: 0.24327\n",
      "[v_loss: 1.09811 val_acc: 0.35417 val_m_acc: 0.35417 f1: 0.32949\n",
      "[v_loss: 1.09822 val_acc: 0.35417 val_m_acc: 0.35417 f1: 0.33797\n",
      "[v_loss: 1.10096 val_acc: 0.29167 val_m_acc: 0.29167 f1: 0.00000\n",
      "[v_loss: 1.09194 val_acc: 0.43750 val_m_acc: 0.43750 f1: 0.43376\n",
      "[v_loss: 1.09298 val_acc: 0.41667 val_m_acc: 0.41667 f1: 0.40087\n",
      "[v_loss: 1.09801 val_acc: 0.35417 val_m_acc: 0.35417 f1: 0.34613\n",
      "[v_loss: 1.09248 val_acc: 0.39583 val_m_acc: 0.39583 f1: 0.37976\n",
      "[v_loss: 1.09899 val_acc: 0.35417 val_m_acc: 0.35417 f1: 0.33830\n",
      "[v_loss: 1.09489 val_acc: 0.43750 val_m_acc: 0.43750 f1: 0.43409\n",
      "[v_loss: 1.09461 val_acc: 0.35417 val_m_acc: 0.35417 f1: 0.31608\n",
      "[v_loss: 1.09062 val_acc: 0.54167 val_m_acc: 0.54167 f1: 0.53704\n",
      "[v_loss: 1.09469 val_acc: 0.35417 val_m_acc: 0.35417 f1: 0.34992\n",
      "[v_loss: 1.09351 val_acc: 0.39583 val_m_acc: 0.39583 f1: 0.39418\n",
      "[v_loss: 1.09491 val_acc: 0.37500 val_m_acc: 0.37500 f1: 0.34304\n",
      "[v_loss: 1.09571 val_acc: 0.43750 val_m_acc: 0.43750 f1: 0.40333\n",
      "[v_loss: 1.10387 val_acc: 0.33333 val_m_acc: 0.33333 f1: 0.30122\n",
      "[v_loss: 1.10022 val_acc: 0.29167 val_m_acc: 0.29167 f1: 0.00000\n",
      "[v_loss: 1.09846 val_acc: 0.39583 val_m_acc: 0.39583 f1: 0.38594\n",
      "[v_loss: 1.09520 val_acc: 0.43750 val_m_acc: 0.43750 f1: 0.40388\n",
      "[v_loss: 1.09490 val_acc: 0.37500 val_m_acc: 0.37500 f1: 0.37773\n",
      "[v_loss: 1.10190 val_acc: 0.39583 val_m_acc: 0.39583 f1: 0.39385\n",
      "[v_loss: 1.09675 val_acc: 0.37500 val_m_acc: 0.37500 f1: 0.36903\n",
      "[v_loss: 1.09670 val_acc: 0.37500 val_m_acc: 0.37500 f1: 0.37773\n",
      "[v_loss: 1.10092 val_acc: 0.33333 val_m_acc: 0.33333 f1: 0.32593\n",
      "[v_loss: 1.09729 val_acc: 0.27083 val_m_acc: 0.27083 f1: 0.26207\n",
      "[v_loss: 1.09914 val_acc: 0.29167 val_m_acc: 0.29167 f1: 0.27978\n",
      "[v_loss: 1.09403 val_acc: 0.37500 val_m_acc: 0.37500 f1: 0.36376\n",
      "[v_loss: 1.09464 val_acc: 0.41667 val_m_acc: 0.41667 f1: 0.39314\n",
      "[v_loss: 1.10018 val_acc: 0.35417 val_m_acc: 0.35417 f1: 0.33069\n",
      "[v_loss: 1.10189 val_acc: 0.27083 val_m_acc: 0.27083 f1: 0.25926\n",
      "[v_loss: 1.09677 val_acc: 0.37500 val_m_acc: 0.37500 f1: 0.32541\n",
      "[v_loss: 1.09366 val_acc: 0.39583 val_m_acc: 0.39583 f1: 0.39728\n",
      "[v_loss: 1.10051 val_acc: 0.29167 val_m_acc: 0.29167 f1: 0.26377\n",
      "[v_loss: 1.09582 val_acc: 0.31250 val_m_acc: 0.31250 f1: 0.30350\n",
      "[v_loss: 1.09883 val_acc: 0.25000 val_m_acc: 0.25000 f1: 0.24848\n",
      "[v_loss: 1.09633 val_acc: 0.35417 val_m_acc: 0.35417 f1: 0.31995\n",
      "[v_loss: 1.09979 val_acc: 0.31250 val_m_acc: 0.31250 f1: 0.27698\n",
      "[v_loss: 1.10251 val_acc: 0.31250 val_m_acc: 0.31250 f1: 0.27778\n",
      "[v_loss: 1.10211 val_acc: 0.33333 val_m_acc: 0.33333 f1: 0.29541\n",
      "[v_loss: 1.09724 val_acc: 0.37500 val_m_acc: 0.37500 f1: 0.37722\n",
      "[v_loss: 1.09200 val_acc: 0.41667 val_m_acc: 0.41667 f1: 0.40278\n",
      "[v_loss: 1.09889 val_acc: 0.39583 val_m_acc: 0.39583 f1: 0.37746\n",
      "[v_loss: 1.09446 val_acc: 0.37500 val_m_acc: 0.37500 f1: 0.35677\n",
      "[v_loss: 1.10116 val_acc: 0.39583 val_m_acc: 0.39583 f1: 0.35180\n",
      "[v_loss: 1.09793 val_acc: 0.39583 val_m_acc: 0.39583 f1: 0.36393\n",
      "[v_loss: 1.09441 val_acc: 0.43750 val_m_acc: 0.43750 f1: 0.43283\n",
      "[v_loss: 1.10102 val_acc: 0.31250 val_m_acc: 0.31250 f1: 0.29630\n",
      "[v_loss: 1.10180 val_acc: 0.22917 val_m_acc: 0.22917 f1: 0.22160\n",
      "[v_loss: 1.09386 val_acc: 0.43750 val_m_acc: 0.43750 f1: 0.43876\n",
      "[v_loss: 1.09810 val_acc: 0.43750 val_m_acc: 0.43750 f1: 0.43402\n",
      "[v_loss: 1.09529 val_acc: 0.35417 val_m_acc: 0.35417 f1: 0.34390\n",
      "[v_loss: 1.09792 val_acc: 0.39583 val_m_acc: 0.39583 f1: 0.38053\n",
      "[v_loss: 1.09412 val_acc: 0.35417 val_m_acc: 0.35417 f1: 0.32748\n",
      "[v_loss: 1.09990 val_acc: 0.39583 val_m_acc: 0.39583 f1: 0.37931\n",
      "[v_loss: 1.08668 val_acc: 0.52083 val_m_acc: 0.52083 f1: 0.51420\n",
      "[v_loss: 1.10154 val_acc: 0.33333 val_m_acc: 0.33333 f1: 0.29649\n",
      "[v_loss: 1.10069 val_acc: 0.39583 val_m_acc: 0.39583 f1: 0.37859\n",
      "[v_loss: 1.09609 val_acc: 0.39583 val_m_acc: 0.39583 f1: 0.36517\n",
      "[v_loss: 1.10012 val_acc: 0.31250 val_m_acc: 0.31250 f1: 0.30619\n",
      "[v_loss: 1.09764 val_acc: 0.37500 val_m_acc: 0.37500 f1: 0.36148\n",
      "[v_loss: 1.09528 val_acc: 0.37500 val_m_acc: 0.37500 f1: 0.37746\n",
      "[v_loss: 1.10157 val_acc: 0.29167 val_m_acc: 0.29167 f1: 0.28902\n",
      "[v_loss: 1.10411 val_acc: 0.20833 val_m_acc: 0.20833 f1: 0.20066\n",
      "[v_loss: 1.09386 val_acc: 0.50000 val_m_acc: 0.50000 f1: 0.48210\n",
      "[v_loss: 1.09561 val_acc: 0.33333 val_m_acc: 0.33333 f1: 0.32200\n",
      "[v_loss: 1.10040 val_acc: 0.25000 val_m_acc: 0.25000 f1: 0.23294\n",
      "[v_loss: 1.09454 val_acc: 0.27083 val_m_acc: 0.27083 f1: 0.27288\n",
      "[v_loss: 1.09769 val_acc: 0.37500 val_m_acc: 0.37500 f1: 0.35982\n",
      "[v_loss: 1.09912 val_acc: 0.39583 val_m_acc: 0.39583 f1: 0.39394\n",
      "[v_loss: 1.09660 val_acc: 0.29167 val_m_acc: 0.29167 f1: 0.29002\n",
      "[v_loss: 1.09387 val_acc: 0.37500 val_m_acc: 0.37500 f1: 0.36214\n",
      "[v_loss: 1.09566 val_acc: 0.45833 val_m_acc: 0.45833 f1: 0.45543\n",
      "[v_loss: 1.09644 val_acc: 0.41667 val_m_acc: 0.41667 f1: 0.38840\n",
      "[v_loss: 1.09898 val_acc: 0.33333 val_m_acc: 0.33333 f1: 0.29307\n",
      "[v_loss: 1.09516 val_acc: 0.37500 val_m_acc: 0.37500 f1: 0.37364\n",
      "[v_loss: 1.09494 val_acc: 0.41667 val_m_acc: 0.41667 f1: 0.36937\n",
      "[v_loss: 1.10360 val_acc: 0.25000 val_m_acc: 0.25000 f1: 0.00000\n",
      "[v_loss: 1.09702 val_acc: 0.31250 val_m_acc: 0.31250 f1: 0.30418\n",
      "[v_loss: 1.09499 val_acc: 0.41667 val_m_acc: 0.41667 f1: 0.40736\n",
      "[v_loss: 1.10312 val_acc: 0.31250 val_m_acc: 0.31250 f1: 0.27733\n",
      "[v_loss: 1.09702 val_acc: 0.35417 val_m_acc: 0.35417 f1: 0.33889\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[v_loss: 1.10323 val_acc: 0.27083 val_m_acc: 0.27083 f1: 0.25840\n",
      "[v_loss: 1.09727 val_acc: 0.35417 val_m_acc: 0.35417 f1: 0.34803\n",
      "[v_loss: 1.10417 val_acc: 0.27083 val_m_acc: 0.27083 f1: 0.25008\n",
      "[v_loss: 1.09889 val_acc: 0.41667 val_m_acc: 0.41667 f1: 0.39820\n",
      "[v_loss: 1.09866 val_acc: 0.33333 val_m_acc: 0.33333 f1: 0.29955\n",
      "[v_loss: 1.10300 val_acc: 0.27083 val_m_acc: 0.27083 f1: 0.00000\n",
      "[v_loss: 1.09453 val_acc: 0.35417 val_m_acc: 0.35417 f1: 0.35610\n",
      "[v_loss: 1.10008 val_acc: 0.33333 val_m_acc: 0.33333 f1: 0.30267\n",
      "[v_loss: 1.09187 val_acc: 0.43750 val_m_acc: 0.43750 f1: 0.42197\n",
      "[v_loss: 1.09575 val_acc: 0.37500 val_m_acc: 0.37500 f1: 0.34554\n",
      "[v_loss: 1.09590 val_acc: 0.35417 val_m_acc: 0.35417 f1: 0.34466\n",
      "[v_loss: 1.09562 val_acc: 0.41667 val_m_acc: 0.41667 f1: 0.40476\n",
      "[v_loss: 1.09935 val_acc: 0.43750 val_m_acc: 0.43750 f1: 0.43533\n",
      "[v_loss: 1.08885 val_acc: 0.47917 val_m_acc: 0.47917 f1: 0.47781\n",
      "[v_loss: 1.09782 val_acc: 0.29167 val_m_acc: 0.29167 f1: 0.24622\n",
      "[v_loss: 1.09745 val_acc: 0.37500 val_m_acc: 0.37500 f1: 0.36096\n",
      "[v_loss: 1.09556 val_acc: 0.52083 val_m_acc: 0.52083 f1: 0.51480\n",
      "[v_loss: 1.09430 val_acc: 0.37500 val_m_acc: 0.37500 f1: 0.36910\n",
      "[v_loss: 1.09815 val_acc: 0.35417 val_m_acc: 0.35417 f1: 0.35972\n",
      "[v_loss: 1.09731 val_acc: 0.35417 val_m_acc: 0.35417 f1: 0.35044\n",
      "[v_loss: 1.10091 val_acc: 0.33333 val_m_acc: 0.33333 f1: 0.33015\n",
      "[v_loss: 1.09720 val_acc: 0.39583 val_m_acc: 0.39583 f1: 0.38815\n",
      "[v_loss: 1.09344 val_acc: 0.37500 val_m_acc: 0.37500 f1: 0.36879\n",
      "[v_loss: 1.09699 val_acc: 0.41667 val_m_acc: 0.41667 f1: 0.41381\n",
      "[v_loss: 1.09777 val_acc: 0.35417 val_m_acc: 0.35417 f1: 0.35031\n",
      "[v_loss: 1.09894 val_acc: 0.33333 val_m_acc: 0.33333 f1: 0.33153\n",
      "[v_loss: 1.09813 val_acc: 0.31250 val_m_acc: 0.31250 f1: 0.31209\n",
      "[v_loss: 1.09591 val_acc: 0.37500 val_m_acc: 0.37500 f1: 0.37778\n",
      "[v_loss: 1.09633 val_acc: 0.33333 val_m_acc: 0.33333 f1: 0.32092\n",
      "[v_loss: 1.09318 val_acc: 0.39583 val_m_acc: 0.39583 f1: 0.36975\n",
      "[v_loss: 1.09996 val_acc: 0.22917 val_m_acc: 0.22917 f1: 0.00000\n",
      "[v_loss: 1.09813 val_acc: 0.33333 val_m_acc: 0.33333 f1: 0.33625\n",
      "[v_loss: 1.09729 val_acc: 0.35417 val_m_acc: 0.35417 f1: 0.33044\n",
      "[v_loss: 1.09840 val_acc: 0.33333 val_m_acc: 0.33333 f1: 0.31257\n",
      "[v_loss: 1.09545 val_acc: 0.39583 val_m_acc: 0.39583 f1: 0.36491\n",
      "[v_loss: 1.09763 val_acc: 0.45833 val_m_acc: 0.45833 f1: 0.39697\n",
      "[v_loss: 1.09965 val_acc: 0.35417 val_m_acc: 0.35417 f1: 0.34549\n",
      "[v_loss: 1.09804 val_acc: 0.29167 val_m_acc: 0.29167 f1: 0.28555\n",
      "[v_loss: 1.09771 val_acc: 0.35417 val_m_acc: 0.35417 f1: 0.35539\n",
      "[v_loss: 1.09049 val_acc: 0.45833 val_m_acc: 0.45833 f1: 0.44212\n",
      "[v_loss: 1.09580 val_acc: 0.35417 val_m_acc: 0.35417 f1: 0.34243\n",
      "[v_loss: 1.09865 val_acc: 0.27083 val_m_acc: 0.27083 f1: 0.26176\n",
      "[v_loss: 1.09580 val_acc: 0.39583 val_m_acc: 0.39583 f1: 0.39152\n",
      "[v_loss: 1.10181 val_acc: 0.29167 val_m_acc: 0.29167 f1: 0.28673\n",
      "[v_loss: 1.09986 val_acc: 0.29167 val_m_acc: 0.29167 f1: 0.28386\n",
      "[v_loss: 1.09404 val_acc: 0.50000 val_m_acc: 0.50000 f1: 0.47355\n",
      "[v_loss: 1.09037 val_acc: 0.45833 val_m_acc: 0.45833 f1: 0.44053\n",
      "[v_loss: 1.09660 val_acc: 0.47917 val_m_acc: 0.47917 f1: 0.46720\n",
      "[v_loss: 1.09756 val_acc: 0.39583 val_m_acc: 0.39583 f1: 0.38807\n",
      "[v_loss: 1.09677 val_acc: 0.37500 val_m_acc: 0.37500 f1: 0.34907\n",
      "[v_loss: 1.09366 val_acc: 0.45833 val_m_acc: 0.45833 f1: 0.43726\n",
      "[v_loss: 1.10089 val_acc: 0.31250 val_m_acc: 0.31250 f1: 0.28588\n",
      "[v_loss: 1.09322 val_acc: 0.41667 val_m_acc: 0.41667 f1: 0.41710\n",
      "[v_loss: 1.09690 val_acc: 0.37500 val_m_acc: 0.37500 f1: 0.37958\n",
      "[v_loss: 1.09753 val_acc: 0.35417 val_m_acc: 0.35417 f1: 0.34112\n",
      "[v_loss: 1.09927 val_acc: 0.31250 val_m_acc: 0.31250 f1: 0.30786\n",
      "[v_loss: 1.09841 val_acc: 0.39583 val_m_acc: 0.39583 f1: 0.38688\n",
      "[v_loss: 1.10000 val_acc: 0.29167 val_m_acc: 0.29167 f1: 0.27909\n",
      "[v_loss: 1.09865 val_acc: 0.45833 val_m_acc: 0.45833 f1: 0.44127\n",
      "[v_loss: 1.10215 val_acc: 0.31250 val_m_acc: 0.31250 f1: 0.00000\n",
      "[v_loss: 1.09473 val_acc: 0.37500 val_m_acc: 0.37500 f1: 0.00000\n",
      "[v_loss: 1.09891 val_acc: 0.39583 val_m_acc: 0.39583 f1: 0.37523\n",
      "[v_loss: 1.10040 val_acc: 0.22917 val_m_acc: 0.22917 f1: 0.21147\n",
      "[v_loss: 1.09430 val_acc: 0.47917 val_m_acc: 0.47917 f1: 0.45233\n",
      "[v_loss: 1.09779 val_acc: 0.43750 val_m_acc: 0.43750 f1: 0.41313\n",
      "[v_loss: 1.09565 val_acc: 0.47917 val_m_acc: 0.47917 f1: 0.47880\n",
      "[v_loss: 1.09431 val_acc: 0.41667 val_m_acc: 0.41667 f1: 0.39621\n",
      "[v_loss: 1.09912 val_acc: 0.39583 val_m_acc: 0.39583 f1: 0.37037\n",
      "[v_loss: 1.09717 val_acc: 0.37500 val_m_acc: 0.37500 f1: 0.34779\n",
      "[v_loss: 1.09653 val_acc: 0.33333 val_m_acc: 0.33333 f1: 0.30942\n",
      "[v_loss: 1.09838 val_acc: 0.31250 val_m_acc: 0.31250 f1: 0.28628\n",
      "[v_loss: 1.10090 val_acc: 0.29167 val_m_acc: 0.29167 f1: 0.25223\n",
      "[v_loss: 1.09333 val_acc: 0.37500 val_m_acc: 0.37500 f1: 0.37365\n",
      "[v_loss: 1.10424 val_acc: 0.22917 val_m_acc: 0.22917 f1: 0.22797\n",
      "[v_loss: 1.09871 val_acc: 0.37500 val_m_acc: 0.37500 f1: 0.32894\n",
      "[v_loss: 1.09881 val_acc: 0.35417 val_m_acc: 0.35417 f1: 0.35354\n",
      "[v_loss: 1.10125 val_acc: 0.31250 val_m_acc: 0.31250 f1: 0.32004\n",
      "[v_loss: 1.09624 val_acc: 0.45833 val_m_acc: 0.45833 f1: 0.45238\n",
      "[v_loss: 1.09953 val_acc: 0.35417 val_m_acc: 0.35417 f1: 0.33136\n",
      "[v_loss: 1.09746 val_acc: 0.39583 val_m_acc: 0.39583 f1: 0.39356\n",
      "[v_loss: 1.09684 val_acc: 0.37500 val_m_acc: 0.37500 f1: 0.37089\n",
      "[v_loss: 1.10582 val_acc: 0.25000 val_m_acc: 0.25000 f1: 0.21691\n",
      "[v_loss: 1.09485 val_acc: 0.39583 val_m_acc: 0.39583 f1: 0.00000\n",
      "[v_loss: 1.09902 val_acc: 0.27083 val_m_acc: 0.27083 f1: 0.26006\n",
      "[v_loss: 1.09465 val_acc: 0.37500 val_m_acc: 0.37500 f1: 0.37458\n",
      "[v_loss: 1.09735 val_acc: 0.41667 val_m_acc: 0.41667 f1: 0.41000\n",
      "[v_loss: 1.09287 val_acc: 0.43750 val_m_acc: 0.43750 f1: 0.43438\n",
      "[v_loss: 1.10144 val_acc: 0.18750 val_m_acc: 0.18750 f1: 0.00000\n",
      "[v_loss: 1.09931 val_acc: 0.33333 val_m_acc: 0.33333 f1: 0.30696\n",
      "[v_loss: 1.09419 val_acc: 0.41667 val_m_acc: 0.41667 f1: 0.40742\n",
      "[v_loss: 1.09999 val_acc: 0.35417 val_m_acc: 0.35417 f1: 0.33107\n",
      "[v_loss: 1.09730 val_acc: 0.35417 val_m_acc: 0.35417 f1: 0.34853\n",
      "[v_loss: 1.09672 val_acc: 0.41667 val_m_acc: 0.41667 f1: 0.39904\n",
      "[v_loss: 1.09702 val_acc: 0.35417 val_m_acc: 0.35417 f1: 0.35813\n",
      "[v_loss: 1.10137 val_acc: 0.27083 val_m_acc: 0.27083 f1: 0.27447\n",
      "[v_loss: 1.09578 val_acc: 0.39583 val_m_acc: 0.39583 f1: 0.39432\n",
      "[v_loss: 1.10046 val_acc: 0.29167 val_m_acc: 0.29167 f1: 0.27506\n",
      "[v_loss: 1.09376 val_acc: 0.45833 val_m_acc: 0.45833 f1: 0.46099\n",
      "[v_loss: 1.09754 val_acc: 0.31250 val_m_acc: 0.31250 f1: 0.30714\n",
      "[v_loss: 1.09557 val_acc: 0.33333 val_m_acc: 0.33333 f1: 0.31617\n",
      "[v_loss: 1.09600 val_acc: 0.39583 val_m_acc: 0.39583 f1: 0.38551\n",
      "[v_loss: 1.09795 val_acc: 0.39583 val_m_acc: 0.39583 f1: 0.38843\n",
      "[v_loss: 1.09149 val_acc: 0.41667 val_m_acc: 0.41667 f1: 0.41494\n",
      "[v_loss: 1.09815 val_acc: 0.33333 val_m_acc: 0.33333 f1: 0.33059\n",
      "[v_loss: 1.09994 val_acc: 0.33333 val_m_acc: 0.33333 f1: 0.33825\n",
      "[v_loss: 1.09560 val_acc: 0.33333 val_m_acc: 0.33333 f1: 0.32193\n",
      "[v_loss: 1.10015 val_acc: 0.29167 val_m_acc: 0.29167 f1: 0.27895\n",
      "[v_loss: 1.09568 val_acc: 0.43750 val_m_acc: 0.43750 f1: 0.43778\n",
      "[v_loss: 1.09939 val_acc: 0.33333 val_m_acc: 0.33333 f1: 0.32570\n",
      "[v_loss: 1.09867 val_acc: 0.37500 val_m_acc: 0.37500 f1: 0.36476\n",
      "[v_loss: 1.10153 val_acc: 0.27083 val_m_acc: 0.27083 f1: 0.27502\n",
      "[v_loss: 1.09835 val_acc: 0.37500 val_m_acc: 0.37500 f1: 0.37897\n",
      "[v_loss: 1.10030 val_acc: 0.29167 val_m_acc: 0.29167 f1: 0.25226\n",
      "[v_loss: 1.09636 val_acc: 0.35417 val_m_acc: 0.35417 f1: 0.34772\n",
      "[v_loss: 1.10277 val_acc: 0.27083 val_m_acc: 0.27083 f1: 0.24636\n",
      "[v_loss: 1.09898 val_acc: 0.27083 val_m_acc: 0.27083 f1: 0.24274\n",
      "[v_loss: 1.09609 val_acc: 0.33333 val_m_acc: 0.33333 f1: 0.30849\n",
      "[v_loss: 1.10039 val_acc: 0.29167 val_m_acc: 0.29167 f1: 0.26462\n",
      "[v_loss: 1.09414 val_acc: 0.43750 val_m_acc: 0.43750 f1: 0.42950\n",
      "[v_loss: 1.09825 val_acc: 0.31250 val_m_acc: 0.31250 f1: 0.29921\n",
      "[v_loss: 1.09175 val_acc: 0.47917 val_m_acc: 0.47917 f1: 0.47031\n",
      "[v_loss: 1.08683 val_acc: 0.41667 val_m_acc: 0.41667 f1: 0.41904\n",
      "[v_loss: 1.09469 val_acc: 0.43750 val_m_acc: 0.43750 f1: 0.42315\n",
      "[v_loss: 1.09422 val_acc: 0.41667 val_m_acc: 0.41667 f1: 0.40873\n",
      "[v_loss: 1.10069 val_acc: 0.33333 val_m_acc: 0.33333 f1: 0.32908\n",
      "[v_loss: 1.09616 val_acc: 0.33333 val_m_acc: 0.33333 f1: 0.31367\n",
      "[v_loss: 1.09382 val_acc: 0.50000 val_m_acc: 0.50000 f1: 0.44853\n",
      "[v_loss: 1.09596 val_acc: 0.31250 val_m_acc: 0.31250 f1: 0.31070\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[v_loss: 1.09705 val_acc: 0.37500 val_m_acc: 0.37500 f1: 0.35738\n",
      "[v_loss: 1.09587 val_acc: 0.33333 val_m_acc: 0.33333 f1: 0.30556\n",
      "[v_loss: 1.09595 val_acc: 0.43750 val_m_acc: 0.43750 f1: 0.44413\n",
      "[v_loss: 1.09820 val_acc: 0.35417 val_m_acc: 0.35417 f1: 0.35043\n",
      "[v_loss: 1.10068 val_acc: 0.31250 val_m_acc: 0.31250 f1: 0.27697\n",
      "[v_loss: 1.09414 val_acc: 0.45833 val_m_acc: 0.45833 f1: 0.43507\n",
      "[v_loss: 1.09705 val_acc: 0.35417 val_m_acc: 0.35417 f1: 0.35455\n",
      "[v_loss: 1.09486 val_acc: 0.37500 val_m_acc: 0.37500 f1: 0.00000\n",
      "[v_loss: 1.09311 val_acc: 0.41667 val_m_acc: 0.41667 f1: 0.40617\n",
      "[v_loss: 1.09519 val_acc: 0.31250 val_m_acc: 0.31250 f1: 0.31729\n",
      "[v_loss: 1.09026 val_acc: 0.45833 val_m_acc: 0.45833 f1: 0.44388\n",
      "[v_loss: 1.10017 val_acc: 0.31250 val_m_acc: 0.31250 f1: 0.31435\n",
      "[v_loss: 1.09814 val_acc: 0.35417 val_m_acc: 0.35417 f1: 0.35610\n",
      "[v_loss: 1.09787 val_acc: 0.37500 val_m_acc: 0.37500 f1: 0.37664\n",
      "[v_loss: 1.10044 val_acc: 0.25000 val_m_acc: 0.25000 f1: 0.24735\n",
      "[v_loss: 1.09877 val_acc: 0.41667 val_m_acc: 0.41667 f1: 0.40540\n",
      "[v_loss: 1.09886 val_acc: 0.31250 val_m_acc: 0.31250 f1: 0.29484\n",
      "[v_loss: 1.08478 val_acc: 0.56250 val_m_acc: 0.56250 f1: 0.54145\n",
      "[v_loss: 1.09375 val_acc: 0.45833 val_m_acc: 0.45833 f1: 0.45674\n",
      "[v_loss: 1.09975 val_acc: 0.22917 val_m_acc: 0.22917 f1: 0.22917\n",
      "[v_loss: 1.09649 val_acc: 0.37500 val_m_acc: 0.37500 f1: 0.35816\n",
      "[v_loss: 1.09585 val_acc: 0.39583 val_m_acc: 0.39583 f1: 0.38619\n",
      "[v_loss: 1.09054 val_acc: 0.37500 val_m_acc: 0.37500 f1: 0.36239\n",
      "[v_loss: 1.09800 val_acc: 0.29167 val_m_acc: 0.29167 f1: 0.28333\n",
      "[v_loss: 1.09129 val_acc: 0.41667 val_m_acc: 0.41667 f1: 0.40705\n",
      "[v_loss: 1.09980 val_acc: 0.29167 val_m_acc: 0.29167 f1: 0.00000\n",
      "[v_loss: 1.09819 val_acc: 0.39583 val_m_acc: 0.39583 f1: 0.36882\n",
      "[v_loss: 1.09380 val_acc: 0.47917 val_m_acc: 0.47917 f1: 0.47454\n",
      "[v_loss: 1.09611 val_acc: 0.39583 val_m_acc: 0.39583 f1: 0.39322\n",
      "[v_loss: 1.09578 val_acc: 0.45833 val_m_acc: 0.45833 f1: 0.44423\n",
      "[v_loss: 1.09533 val_acc: 0.39583 val_m_acc: 0.39583 f1: 0.38922\n",
      "[v_loss: 1.09923 val_acc: 0.47917 val_m_acc: 0.47917 f1: 0.47917\n",
      "[v_loss: 1.10024 val_acc: 0.33333 val_m_acc: 0.33333 f1: 0.30536\n",
      "[v_loss: 1.09750 val_acc: 0.37500 val_m_acc: 0.37500 f1: 0.37645\n",
      "[v_loss: 1.09655 val_acc: 0.43750 val_m_acc: 0.43750 f1: 0.42129\n",
      "[v_loss: 1.09680 val_acc: 0.33333 val_m_acc: 0.33333 f1: 0.33333\n",
      "[v_loss: 1.10118 val_acc: 0.27083 val_m_acc: 0.27083 f1: 0.23357\n",
      "[v_loss: 1.09850 val_acc: 0.39583 val_m_acc: 0.39583 f1: 0.39019\n",
      "[v_loss: 1.09318 val_acc: 0.41667 val_m_acc: 0.41667 f1: 0.39777\n",
      "[v_loss: 1.10041 val_acc: 0.41667 val_m_acc: 0.41667 f1: 0.41247\n",
      "[v_loss: 1.09523 val_acc: 0.35417 val_m_acc: 0.35417 f1: 0.00000\n",
      "[v_loss: 1.09616 val_acc: 0.41667 val_m_acc: 0.41667 f1: 0.39656\n",
      "[v_loss: 1.09709 val_acc: 0.27083 val_m_acc: 0.27083 f1: 0.23260\n",
      "[v_loss: 1.09995 val_acc: 0.27083 val_m_acc: 0.27083 f1: 0.24049\n",
      "[v_loss: 1.09736 val_acc: 0.33333 val_m_acc: 0.33333 f1: 0.32078\n",
      "[v_loss: 1.10165 val_acc: 0.37500 val_m_acc: 0.37500 f1: 0.33883\n",
      "[v_loss: 1.09756 val_acc: 0.29167 val_m_acc: 0.29167 f1: 0.24608\n",
      "[v_loss: 1.10308 val_acc: 0.31250 val_m_acc: 0.31250 f1: 0.31159\n",
      "[v_loss: 1.09810 val_acc: 0.41667 val_m_acc: 0.41667 f1: 0.38630\n",
      "[v_loss: 1.09588 val_acc: 0.33333 val_m_acc: 0.33333 f1: 0.31679\n",
      "[v_loss: 1.09548 val_acc: 0.39583 val_m_acc: 0.39583 f1: 0.39749\n",
      "[v_loss: 1.10043 val_acc: 0.31250 val_m_acc: 0.31250 f1: 0.31644\n",
      "[v_loss: 1.09385 val_acc: 0.37500 val_m_acc: 0.37500 f1: 0.37620\n",
      "[v_loss: 1.10213 val_acc: 0.20833 val_m_acc: 0.20833 f1: 0.00000\n",
      "[v_loss: 1.09407 val_acc: 0.37500 val_m_acc: 0.37500 f1: 0.35371\n",
      "[v_loss: 1.09629 val_acc: 0.37500 val_m_acc: 0.37500 f1: 0.33717\n",
      "[v_loss: 1.09592 val_acc: 0.43750 val_m_acc: 0.43750 f1: 0.41880\n",
      "[v_loss: 1.09778 val_acc: 0.29167 val_m_acc: 0.29167 f1: 0.27682\n",
      "[v_loss: 1.09346 val_acc: 0.45833 val_m_acc: 0.45833 f1: 0.44137\n",
      "[v_loss: 1.09391 val_acc: 0.50000 val_m_acc: 0.50000 f1: 0.45850\n",
      "[v_loss: 1.10551 val_acc: 0.27083 val_m_acc: 0.27083 f1: 0.26652\n",
      "[v_loss: 1.10194 val_acc: 0.29167 val_m_acc: 0.29167 f1: 0.28968\n",
      "[v_loss: 1.10513 val_acc: 0.20833 val_m_acc: 0.20833 f1: 0.00000\n",
      "[v_loss: 1.10231 val_acc: 0.33333 val_m_acc: 0.33333 f1: 0.30807\n",
      "[v_loss: 1.09756 val_acc: 0.33333 val_m_acc: 0.33333 f1: 0.32222\n",
      "[v_loss: 1.09449 val_acc: 0.37500 val_m_acc: 0.37500 f1: 0.38136\n",
      "[v_loss: 1.09627 val_acc: 0.33333 val_m_acc: 0.33333 f1: 0.33416\n",
      "[v_loss: 1.09912 val_acc: 0.27083 val_m_acc: 0.27083 f1: 0.25816\n",
      "[v_loss: 1.10060 val_acc: 0.27083 val_m_acc: 0.27083 f1: 0.26081\n",
      "[v_loss: 1.09491 val_acc: 0.37500 val_m_acc: 0.37500 f1: 0.32744\n",
      "[v_loss: 1.09499 val_acc: 0.37500 val_m_acc: 0.37500 f1: 0.36713\n",
      "[v_loss: 1.10006 val_acc: 0.35417 val_m_acc: 0.35417 f1: 0.34220\n",
      "[v_loss: 1.09821 val_acc: 0.33333 val_m_acc: 0.33333 f1: 0.33340\n",
      "[v_loss: 1.09836 val_acc: 0.45833 val_m_acc: 0.45833 f1: 0.45185\n",
      "[v_loss: 1.10102 val_acc: 0.33333 val_m_acc: 0.33333 f1: 0.32505\n",
      "[v_loss: 1.09639 val_acc: 0.39583 val_m_acc: 0.39583 f1: 0.36868\n",
      "[v_loss: 1.09718 val_acc: 0.25000 val_m_acc: 0.25000 f1: 0.23697\n",
      "[v_loss: 1.09637 val_acc: 0.37500 val_m_acc: 0.37500 f1: 0.37730\n",
      "[v_loss: 1.09931 val_acc: 0.35417 val_m_acc: 0.35417 f1: 0.33175\n",
      "[v_loss: 1.09841 val_acc: 0.37500 val_m_acc: 0.37500 f1: 0.32739\n",
      "[v_loss: 1.09442 val_acc: 0.33333 val_m_acc: 0.33333 f1: 0.27352\n",
      "[v_loss: 1.09617 val_acc: 0.39583 val_m_acc: 0.39583 f1: 0.39618\n",
      "[v_loss: 1.09688 val_acc: 0.35417 val_m_acc: 0.35417 f1: 0.31558\n",
      "[v_loss: 1.10072 val_acc: 0.25000 val_m_acc: 0.25000 f1: 0.22762\n",
      "[v_loss: 1.09503 val_acc: 0.41667 val_m_acc: 0.41667 f1: 0.40324\n",
      "[v_loss: 1.09409 val_acc: 0.41667 val_m_acc: 0.41667 f1: 0.38566\n",
      "[v_loss: 1.09430 val_acc: 0.39583 val_m_acc: 0.39583 f1: 0.39749\n",
      "[v_loss: 1.09805 val_acc: 0.33333 val_m_acc: 0.33333 f1: 0.32660\n",
      "[v_loss: 1.09908 val_acc: 0.39583 val_m_acc: 0.39583 f1: 0.39252\n",
      "[v_loss: 1.09360 val_acc: 0.41667 val_m_acc: 0.41667 f1: 0.41781\n",
      "[v_loss: 1.09791 val_acc: 0.37500 val_m_acc: 0.37500 f1: 0.35613\n",
      "[v_loss: 1.09597 val_acc: 0.35417 val_m_acc: 0.35417 f1: 0.33015\n",
      "[v_loss: 1.09463 val_acc: 0.47917 val_m_acc: 0.47917 f1: 0.47532\n",
      "[v_loss: 1.09747 val_acc: 0.37500 val_m_acc: 0.37500 f1: 0.37062\n",
      "[v_loss: 1.09509 val_acc: 0.54167 val_m_acc: 0.54167 f1: 0.53864\n",
      "[v_loss: 1.09568 val_acc: 0.33333 val_m_acc: 0.33333 f1: 0.31710\n",
      "[v_loss: 1.09625 val_acc: 0.45833 val_m_acc: 0.45833 f1: 0.43068\n",
      "[v_loss: 1.09699 val_acc: 0.39583 val_m_acc: 0.39583 f1: 0.38645\n",
      "[v_loss: 1.09615 val_acc: 0.43750 val_m_acc: 0.43750 f1: 0.43554\n",
      "[v_loss: 1.09718 val_acc: 0.41667 val_m_acc: 0.41667 f1: 0.38378\n",
      "[v_loss: 1.09673 val_acc: 0.33333 val_m_acc: 0.33333 f1: 0.33120\n",
      "[v_loss: 1.09230 val_acc: 0.52083 val_m_acc: 0.52083 f1: 0.50649\n",
      "[v_loss: 1.09531 val_acc: 0.45833 val_m_acc: 0.45833 f1: 0.45130\n",
      "[v_loss: 1.09695 val_acc: 0.43750 val_m_acc: 0.43750 f1: 0.42632\n",
      "[v_loss: 1.10290 val_acc: 0.18750 val_m_acc: 0.18750 f1: 0.17717\n",
      "[v_loss: 1.09570 val_acc: 0.31250 val_m_acc: 0.31250 f1: 0.31209\n",
      "[v_loss: 1.09861 val_acc: 0.35417 val_m_acc: 0.35417 f1: 0.34789\n",
      "[v_loss: 1.09869 val_acc: 0.33333 val_m_acc: 0.33333 f1: 0.33639\n",
      "[v_loss: 1.09742 val_acc: 0.41667 val_m_acc: 0.41667 f1: 0.39755\n",
      "[v_loss: 1.09599 val_acc: 0.37500 val_m_acc: 0.37500 f1: 0.36522\n",
      "[v_loss: 1.09737 val_acc: 0.29167 val_m_acc: 0.29167 f1: 0.28142\n",
      "[v_loss: 1.09589 val_acc: 0.37500 val_m_acc: 0.37500 f1: 0.36419\n",
      "[v_loss: 1.09854 val_acc: 0.31250 val_m_acc: 0.31250 f1: 0.29038\n",
      "[v_loss: 1.10138 val_acc: 0.14583 val_m_acc: 0.14583 f1: 0.14142\n",
      "[v_loss: 1.09774 val_acc: 0.27083 val_m_acc: 0.27083 f1: 0.00000\n",
      "[v_loss: 1.09586 val_acc: 0.35417 val_m_acc: 0.35417 f1: 0.34708\n",
      "[v_loss: 1.09826 val_acc: 0.35417 val_m_acc: 0.35417 f1: 0.35437\n",
      "[v_loss: 1.10173 val_acc: 0.31250 val_m_acc: 0.31250 f1: 0.29832\n",
      "[v_loss: 1.09531 val_acc: 0.47917 val_m_acc: 0.47917 f1: 0.46447\n",
      "[v_loss: 1.09592 val_acc: 0.35417 val_m_acc: 0.35417 f1: 0.36515\n",
      "[v_loss: 1.09738 val_acc: 0.41667 val_m_acc: 0.41667 f1: 0.41064\n",
      "[v_loss: 1.09792 val_acc: 0.35417 val_m_acc: 0.35417 f1: 0.33015\n",
      "[v_loss: 1.09791 val_acc: 0.39583 val_m_acc: 0.39583 f1: 0.38217\n",
      "[v_loss: 1.09534 val_acc: 0.39583 val_m_acc: 0.39583 f1: 0.38103\n",
      "[v_loss: 1.09923 val_acc: 0.27083 val_m_acc: 0.27083 f1: 0.26021\n",
      "[v_loss: 1.10088 val_acc: 0.27083 val_m_acc: 0.27083 f1: 0.27167\n",
      "[v_loss: 1.09486 val_acc: 0.43750 val_m_acc: 0.43750 f1: 0.42892\n",
      "[v_loss: 1.09494 val_acc: 0.43750 val_m_acc: 0.43750 f1: 0.40271\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[v_loss: 1.09216 val_acc: 0.50000 val_m_acc: 0.50000 f1: 0.49974\n",
      "[v_loss: 1.09583 val_acc: 0.39583 val_m_acc: 0.39583 f1: 0.39436\n",
      "[v_loss: 1.09503 val_acc: 0.45833 val_m_acc: 0.45833 f1: 0.45238\n",
      "[v_loss: 1.10039 val_acc: 0.22917 val_m_acc: 0.22917 f1: 0.23310\n",
      "[v_loss: 1.10507 val_acc: 0.18750 val_m_acc: 0.18750 f1: 0.17965\n",
      "[v_loss: 1.09301 val_acc: 0.47917 val_m_acc: 0.47917 f1: 0.47588\n",
      "[v_loss: 1.09834 val_acc: 0.41667 val_m_acc: 0.41667 f1: 0.40073\n",
      "[v_loss: 1.09465 val_acc: 0.43750 val_m_acc: 0.43750 f1: 0.41429\n",
      "[v_loss: 1.09945 val_acc: 0.33333 val_m_acc: 0.33333 f1: 0.32479\n",
      "[v_loss: 1.09510 val_acc: 0.41667 val_m_acc: 0.41667 f1: 0.40801\n",
      "[v_loss: 1.09610 val_acc: 0.37500 val_m_acc: 0.37500 f1: 0.36883\n",
      "[v_loss: 1.09077 val_acc: 0.54167 val_m_acc: 0.54167 f1: 0.54297\n",
      "[v_loss: 1.09768 val_acc: 0.31250 val_m_acc: 0.31250 f1: 0.30620\n",
      "[v_loss: 1.09824 val_acc: 0.31250 val_m_acc: 0.31250 f1: 0.31208\n",
      "[v_loss: 1.09009 val_acc: 0.56250 val_m_acc: 0.56250 f1: 0.55965\n",
      "[v_loss: 1.09692 val_acc: 0.35417 val_m_acc: 0.35417 f1: 0.36036\n",
      "[v_loss: 1.09616 val_acc: 0.45833 val_m_acc: 0.45833 f1: 0.44538\n",
      "[v_loss: 1.09481 val_acc: 0.35417 val_m_acc: 0.35417 f1: 0.34823\n",
      "[v_loss: 1.09812 val_acc: 0.33333 val_m_acc: 0.33333 f1: 0.31692\n",
      "[v_loss: 1.09797 val_acc: 0.31250 val_m_acc: 0.31250 f1: 0.31309\n",
      "[v_loss: 1.09860 val_acc: 0.27083 val_m_acc: 0.27083 f1: 0.26356\n",
      "[v_loss: 1.10286 val_acc: 0.22917 val_m_acc: 0.22917 f1: 0.21895\n",
      "[v_loss: 1.09823 val_acc: 0.37500 val_m_acc: 0.37500 f1: 0.37460\n",
      "[v_loss: 1.09583 val_acc: 0.33333 val_m_acc: 0.33333 f1: 0.33333\n",
      "[v_loss: 1.09736 val_acc: 0.39583 val_m_acc: 0.39583 f1: 0.38401\n",
      "[v_loss: 1.09817 val_acc: 0.29167 val_m_acc: 0.29167 f1: 0.29675\n",
      "[v_loss: 1.09584 val_acc: 0.37500 val_m_acc: 0.37500 f1: 0.37038\n",
      "[v_loss: 1.09655 val_acc: 0.41667 val_m_acc: 0.41667 f1: 0.40857\n",
      "[v_loss: 1.09320 val_acc: 0.45833 val_m_acc: 0.45833 f1: 0.45068\n",
      "[v_loss: 1.09662 val_acc: 0.39583 val_m_acc: 0.39583 f1: 0.39027\n",
      "[v_loss: 1.09794 val_acc: 0.39583 val_m_acc: 0.39583 f1: 0.39135\n",
      "[v_loss: 1.09872 val_acc: 0.39583 val_m_acc: 0.39583 f1: 0.39195\n",
      "[v_loss: 1.10228 val_acc: 0.22917 val_m_acc: 0.22917 f1: 0.22605\n",
      "[v_loss: 1.09715 val_acc: 0.37500 val_m_acc: 0.37500 f1: 0.35139\n",
      "[v_loss: 1.09340 val_acc: 0.45833 val_m_acc: 0.45833 f1: 0.45536\n",
      "[v_loss: 1.09600 val_acc: 0.39583 val_m_acc: 0.39583 f1: 0.38197\n",
      "[v_loss: 1.09651 val_acc: 0.43750 val_m_acc: 0.43750 f1: 0.43189\n",
      "[v_loss: 1.09655 val_acc: 0.35417 val_m_acc: 0.35417 f1: 0.33427\n",
      "[v_loss: 1.09823 val_acc: 0.37500 val_m_acc: 0.37500 f1: 0.33868\n",
      "[v_loss: 1.09504 val_acc: 0.56250 val_m_acc: 0.56250 f1: 0.56711\n",
      "[v_loss: 1.09471 val_acc: 0.45833 val_m_acc: 0.45833 f1: 0.44470\n",
      "[v_loss: 1.09913 val_acc: 0.33333 val_m_acc: 0.33333 f1: 0.33114\n",
      "[v_loss: 1.09339 val_acc: 0.56250 val_m_acc: 0.56250 f1: 0.56307\n",
      "[v_loss: 1.09809 val_acc: 0.37500 val_m_acc: 0.37500 f1: 0.37244\n",
      "[v_loss: 1.09384 val_acc: 0.37500 val_m_acc: 0.37500 f1: 0.37548\n",
      "[v_loss: 1.09681 val_acc: 0.35417 val_m_acc: 0.35417 f1: 0.35161\n",
      "[v_loss: 1.09721 val_acc: 0.39583 val_m_acc: 0.39583 f1: 0.38593\n",
      "[v_loss: 1.09473 val_acc: 0.50000 val_m_acc: 0.50000 f1: 0.49839\n",
      "[v_loss: 1.09439 val_acc: 0.43750 val_m_acc: 0.43750 f1: 0.43063\n",
      "[v_loss: 1.09600 val_acc: 0.31250 val_m_acc: 0.31250 f1: 0.27850\n",
      "[v_loss: 1.09613 val_acc: 0.41667 val_m_acc: 0.41667 f1: 0.41753\n",
      "[v_loss: 1.09191 val_acc: 0.62500 val_m_acc: 0.62500 f1: 0.61099\n",
      "[v_loss: 1.09616 val_acc: 0.39583 val_m_acc: 0.39583 f1: 0.38815\n",
      "[v_loss: 1.09604 val_acc: 0.39583 val_m_acc: 0.39583 f1: 0.39142\n",
      "[v_loss: 1.09932 val_acc: 0.25000 val_m_acc: 0.25000 f1: 0.24339\n",
      "[v_loss: 1.09816 val_acc: 0.37500 val_m_acc: 0.37500 f1: 0.36523\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import sys\n",
    "from baseline_train import *\n",
    "\n",
    "\n",
    "def main(k_shot):\n",
    "    # Set seed for reproducibility\n",
    "    torch.manual_seed(222)\n",
    "    torch.cuda.manual_seed_all(222)\n",
    "    np.random.seed(222)\n",
    "\n",
    "    n_way = 3\n",
    "    k_query = 16\n",
    "    num_episodes = 200\n",
    "    num_workers = 12\n",
    "    bs = 4\n",
    "    root = '../../../../scratch/rl80/mimic-cxr-jpg-2.0.0.physionet.org/files'\n",
    "    path_splits = '../splits/splits.csv'  # Location of preprocessed splits\n",
    "    path_results = f'../../results/{k_shot}shot'  # Folder to save the CSV results\n",
    "    path_pretrained = '../results/basic/basic_39.pth'\n",
    "\n",
    "    # Set device to GPU if it exists\n",
    "    torch.cuda.set_device(0)\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Load in data\n",
    "    dataset = MimicCxrJpgEpisodes(root, path_splits, n_way, k_shot, k_query, num_episodes, 'novel')\n",
    "    loader = DataLoader(dataset, batch_size=bs, shuffle=True, num_workers=num_workers)\n",
    "\n",
    "    # Create Dataframe to export results to CSV\n",
    "    df_results = pd.DataFrame(columns=['Validation Loss', 'Accuracy', 'Macro Accuracy',\n",
    "                                       'Macro-F1 Score'] + [str(x) + ' F1' for x in range(n_way)])\n",
    "    df_hold = pd.DataFrame(columns=['Validation Loss', 'Accuracy', 'Macro Accuracy',\n",
    "                                    'Macro-F1 Score'] + [str(x) + ' F1' for x in range(n_way)])\n",
    "\n",
    "    # Iterate through batched episodes. One episode is one experiment\n",
    "    for step, (support_imgs, support_labels, query_imgs, query_labels) in enumerate(loader):\n",
    "        # Convert Tensors to appropriate device\n",
    "        batch_support_x, batch_support_y = support_imgs.to(device), support_labels.to(device)\n",
    "        batch_query_x, batch_query_y = query_imgs.to(device), query_labels.to(device)\n",
    "\n",
    "        # [num_batch, training_sz, channels, height, width] = support_x.size()\n",
    "        # num_batch = num of episodes\n",
    "        # training_sz = size of support or query set\n",
    "        num_batch = batch_support_x.size(0)  # Number of episodes in the batch\n",
    "\n",
    "        # Break down the batch of episodes into single episodes\n",
    "        for i in range(num_batch):\n",
    "            # Load in model and reset weights every episode/experiment\n",
    "            model = CosineSimilarityNet(n_way).to(device)\n",
    "            pretrained_dict = torch.load(path_pretrained)\n",
    "            del pretrained_dict['linear.weight']  # Remove the last layer\n",
    "            del pretrained_dict['linear.bias']\n",
    "            model_dict = model.state_dict()\n",
    "            model_dict.update(pretrained_dict)\n",
    "            model.load_state_dict(model_dict)\n",
    "\n",
    "            # Break down the sets into individual episodes\n",
    "            support_x, support_y = batch_support_x[i], batch_support_y[i]\n",
    "            query_x, query_y = batch_query_x[i], batch_query_y[i]\n",
    "\n",
    "            # Find Average Features\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                # Initialise list containing features sorted by class\n",
    "                label_features = [torch.FloatTensor([]).to(device) for i in range(n_way)]\n",
    "\n",
    "                # Initialise weight for the nearest centroid, last layer weight\n",
    "                fc_weight = torch.FloatTensor([]).to(device)\n",
    "\n",
    "                # Get Features\n",
    "                _, features = model(support_x, extract_features=True)\n",
    "\n",
    "                # Sort features by labels to be averaged later on\n",
    "                for i in range(features.size(0)):\n",
    "                    label = support_y[i]\n",
    "                    label_features[label] = torch.cat((label_features[label], features[i][None]))\n",
    "\n",
    "                # Create weight for the last layer\n",
    "                for j in range(n_way):\n",
    "                    feature_avg = torch.mean(label_features[j], 0)\n",
    "                    fc_weight = torch.cat((fc_weight, feature_avg[None]), 0)\n",
    "\n",
    "                # Apply weight to the model\n",
    "                nc_dict = model.state_dict()\n",
    "                nc_dict['cos_sim.weight'] = fc_weight\n",
    "                model.load_state_dict(nc_dict)\n",
    "\n",
    "            # Testing\n",
    "            val_loss, acc, m_acc, macro_f1, class_f1 = test(query_x, query_y, model, criterion, device, n_way)\n",
    "\n",
    "            # Print the results per experiment\n",
    "            print(f'[v_loss: {val_loss:.5f} val_acc: {acc:.5f} val_m_acc: {m_acc:.5f} f1: {macro_f1:.5f}')\n",
    "\n",
    "            # Record the experiment to be saved into a CSV\n",
    "            df_hold.loc[0] = [val_loss, acc, m_acc, macro_f1] + class_f1\n",
    "            df_results = df_results.append(df_hold.loc[0], ignore_index=True)\n",
    "\n",
    "    # Create results folder if it does not exist\n",
    "    if not os.path.exists(path_results):\n",
    "        os.makedirs(path_results)\n",
    "\n",
    "    df_results.to_csv(os.path.join(path_results, f'{k_shot}shot_nc.csv'), index=False)  # Export results to a CSV file\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    #print(f'NC Training {sys.argv[1]} shot')\n",
    "    #main(int(sys.argv[1]))  # Get the k_shot variable from command line\n",
    "    main(1)\n",
    "    main(3)\n",
    "    main(5)\n",
    "    main(10)\n",
    "    main(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import sys\n",
    "from baseline_train import *\n",
    "\n",
    "\n",
    "def main(k_shot):\n",
    "    # Set seed for reproducibility\n",
    "    torch.manual_seed(222)\n",
    "    torch.cuda.manual_seed_all(222)\n",
    "    np.random.seed(222)\n",
    "\n",
    "    n_way = 3\n",
    "    k_query = 16\n",
    "    num_episodes = 200\n",
    "    num_workers = 12\n",
    "    bs = 4\n",
    "    root = '../../../../scratch/rl80/mimic-cxr-jpg-2.0.0.physionet.org/files'\n",
    "    path_splits = '../splits/splits.csv'  # Location of preprocessed splits\n",
    "    path_results = f'../../results/{k_shot}shot'  # Folder to save the CSV results\n",
    "    path_pretrained = '../results/basic_cosine/basic_cosine_49.pth'\n",
    "\n",
    "    # Set device to GPU if it exists\n",
    "    torch.cuda.set_device(0)\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Load in data\n",
    "    dataset = MimicCxrJpgEpisodes(root, path_splits, n_way, k_shot, k_query, num_episodes, 'novel')\n",
    "    loader = DataLoader(dataset, batch_size=bs, shuffle=True, num_workers=num_workers)\n",
    "\n",
    "    # Create Dataframe to export results to CSV\n",
    "    df_results = pd.DataFrame(columns=['Validation Loss', 'Accuracy', 'Macro Accuracy',\n",
    "                                       'Macro-F1 Score'] + [str(x) + ' F1' for x in range(n_way)])\n",
    "    df_hold = pd.DataFrame(columns=['Validation Loss', 'Accuracy', 'Macro Accuracy',\n",
    "                                    'Macro-F1 Score'] + [str(x) + ' F1' for x in range(n_way)])\n",
    "\n",
    "    # Iterate through batched episodes. One episode is one experiment\n",
    "    for step, (support_imgs, support_labels, query_imgs, query_labels) in enumerate(loader):\n",
    "        # Convert Tensors to appropriate device\n",
    "        batch_support_x, batch_support_y = support_imgs.to(device), support_labels.to(device)\n",
    "        batch_query_x, batch_query_y = query_imgs.to(device), query_labels.to(device)\n",
    "\n",
    "        # [num_batch, training_sz, channels, height, width] = support_x.size()\n",
    "        # num_batch = num of episodes\n",
    "        # training_sz = size of support or query set\n",
    "        num_batch = batch_support_x.size(0)  # Number of episodes in the batch\n",
    "\n",
    "        # Break down the batch of episodes into single episodes\n",
    "        for i in range(num_batch):\n",
    "            # Load in model and reset weights every episode/experiment\n",
    "            model = CosineSimilarityNet(n_way).to(device)\n",
    "            pretrained_dict = torch.load(path_pretrained)\n",
    "            del pretrained_dict['cos_sim.weight'] # Remove the last layer\n",
    "            model_dict = model.state_dict()\n",
    "            model_dict.update(pretrained_dict)\n",
    "            model.load_state_dict(model_dict)\n",
    "\n",
    "            # Break down the sets into individual episodes\n",
    "            support_x, support_y = batch_support_x[i], batch_support_y[i]\n",
    "            query_x, query_y = batch_query_x[i], batch_query_y[i]\n",
    "\n",
    "            # Find Average Features\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                # Initialise list containing features sorted by class\n",
    "                label_features = [torch.FloatTensor([]).to(device) for i in range(n_way)]\n",
    "\n",
    "                # Initialise weight for the nearest centroid, last layer weight\n",
    "                fc_weight = torch.FloatTensor([]).to(device)\n",
    "\n",
    "                # Get Features\n",
    "                _, features = model(support_x, extract_features=True)\n",
    "\n",
    "                # Sort features by labels to be averaged later on\n",
    "                for i in range(features.size(0)):\n",
    "                    label = support_y[i]\n",
    "                    label_features[label] = torch.cat((label_features[label], features[i][None]))\n",
    "\n",
    "                # Create weight for the last layer\n",
    "                for j in range(n_way):\n",
    "                    feature_avg = torch.mean(label_features[j], 0)\n",
    "                    fc_weight = torch.cat((fc_weight, feature_avg[None]), 0)\n",
    "\n",
    "                # Apply weight to the model\n",
    "                nc_dict = model.state_dict()\n",
    "                nc_dict['cos_sim.weight'] = fc_weight\n",
    "                model.load_state_dict(nc_dict)\n",
    "\n",
    "            # Testing\n",
    "            val_loss, acc, m_acc, macro_f1, class_f1 = test(query_x, query_y, model, criterion, device, n_way)\n",
    "\n",
    "            # Print the results per experiment\n",
    "            #print(f'[v_loss: {val_loss:.5f} val_acc: {acc:.5f} val_m_acc: {m_acc:.5f} f1: {macro_f1:.5f}')\n",
    "\n",
    "            # Record the experiment to be saved into a CSV\n",
    "            df_hold.loc[0] = [val_loss, acc, m_acc, macro_f1] + class_f1\n",
    "            df_results = df_results.append(df_hold.loc[0], ignore_index=True)\n",
    "\n",
    "    # Create results folder if it does not exist\n",
    "    if not os.path.exists(path_results):\n",
    "        os.makedirs(path_results)\n",
    "        \n",
    "    print('saved')\n",
    "    df_results.to_csv(os.path.join(path_results, f'{k_shot}shot_nc_cs.csv'), index=False)  # Export results to a CSV file\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    #print(f'NC_CS Training {sys.argv[1]} shot')\n",
    "    #main(int(sys.argv[1]))  # Get the k_shot variable from command line\n",
    "    main(1)\n",
    "    main(3)\n",
    "    main(5)\n",
    "    main(10)\n",
    "    main(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
