{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[v_loss: 1.09060 val_acc: 0.43750 val_m_acc: 0.43750 f1: 0.43250\n",
      "[v_loss: 1.07681 val_acc: 0.52083 val_m_acc: 0.52083 f1: 0.51811\n",
      "[v_loss: 1.07761 val_acc: 0.45833 val_m_acc: 0.45833 f1: 0.44218\n",
      "[v_loss: 1.06002 val_acc: 0.50000 val_m_acc: 0.50000 f1: 0.49971\n",
      "[v_loss: 1.06967 val_acc: 0.60417 val_m_acc: 0.60417 f1: 0.60603\n",
      "[v_loss: 1.08967 val_acc: 0.41667 val_m_acc: 0.41667 f1: 0.41691\n",
      "[v_loss: 1.08648 val_acc: 0.47917 val_m_acc: 0.47917 f1: 0.47532\n",
      "[v_loss: 1.04882 val_acc: 0.60417 val_m_acc: 0.60417 f1: 0.58220\n",
      "[v_loss: 1.08524 val_acc: 0.45833 val_m_acc: 0.45833 f1: 0.45919\n",
      "[v_loss: 1.06479 val_acc: 0.64583 val_m_acc: 0.64583 f1: 0.62974\n",
      "[v_loss: 1.08607 val_acc: 0.54167 val_m_acc: 0.54167 f1: 0.52756\n",
      "[v_loss: 1.08939 val_acc: 0.33333 val_m_acc: 0.33333 f1: 0.31992\n",
      "[v_loss: 1.04297 val_acc: 0.68750 val_m_acc: 0.68750 f1: 0.68067\n",
      "[v_loss: 1.07869 val_acc: 0.43750 val_m_acc: 0.43750 f1: 0.40556\n",
      "[v_loss: 1.08665 val_acc: 0.33333 val_m_acc: 0.33333 f1: 0.33086\n",
      "[v_loss: 1.06841 val_acc: 0.50000 val_m_acc: 0.50000 f1: 0.50737\n",
      "[v_loss: 1.08386 val_acc: 0.47917 val_m_acc: 0.47917 f1: 0.47239\n",
      "[v_loss: 1.08900 val_acc: 0.33333 val_m_acc: 0.33333 f1: 0.32466\n",
      "[v_loss: 1.06791 val_acc: 0.39583 val_m_acc: 0.39583 f1: 0.37593\n",
      "[v_loss: 1.06523 val_acc: 0.47917 val_m_acc: 0.47917 f1: 0.47464\n"
     ]
    }
   ],
   "source": [
    "from baseline_train import *\n",
    "\n",
    "\n",
    "def main():\n",
    "    n_way = 3\n",
    "    k_shot = 20\n",
    "    k_query = 16\n",
    "    num_episodes = 20\n",
    "    num_workers = 12\n",
    "    bs = 4\n",
    "    root = '../../../../scratch/rl80/mimic-cxr-jpg-2.0.0.physionet.org/files'\n",
    "    path_splits = '../splits/splits.csv'  # Location of preprocessed splits\n",
    "    path_results = '../../results'  # Folder to save the CSV results\n",
    "    path_pretrained = '../results/basic_cosine/basic_cosine_55.pth'\n",
    "\n",
    "    # Set device to GPU if it exists\n",
    "    torch.cuda.set_device(0)\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Load in data\n",
    "    dataset = NovelMimicCxrJpg(root, path_splits, n_way, k_shot, k_query, num_episodes)\n",
    "    loader = DataLoader(dataset, batch_size=bs, shuffle=True, num_workers=num_workers)\n",
    "\n",
    "    # Create Dataframe to export results to CSV\n",
    "    df_results = pd.DataFrame(columns=['Validation Loss', 'Accuracy', 'Macro Accuracy',\n",
    "                                       'Macro-F1 Score'] + [str(x) + ' F1' for x in range(n_way)])\n",
    "    df_hold = pd.DataFrame(columns=['Validation Loss', 'Accuracy', 'Macro Accuracy',\n",
    "                                    'Macro-F1 Score'] + [str(x) + ' F1' for x in range(n_way)])\n",
    "\n",
    "    # Iterate through batched episodes. One episode is one experiment\n",
    "    for step, (support_imgs, support_labels, query_imgs, query_labels) in enumerate(loader):\n",
    "        # Convert Tensors to appropriate device\n",
    "        batch_support_x, batch_support_y = support_imgs.to(device), support_labels.to(device)\n",
    "        batch_query_x, batch_query_y = query_imgs.to(device), query_labels.to(device)\n",
    "\n",
    "        # [num_batch, training_sz, channels, height, width] = support_x.size()\n",
    "        # num_batch = num of episodes\n",
    "        # training_sz = size of support or query set\n",
    "        num_batch = batch_support_x.size(0)  # Number of episodes in the batch\n",
    "\n",
    "        # Break down the batch of episodes into single episodes\n",
    "        for i in range(num_batch):\n",
    "            # Load in model and reset weights every episode/experiment\n",
    "            model = CosineSimilarityNet(n_way).to(device)\n",
    "            pretrained_dict = torch.load(path_pretrained)\n",
    "            del pretrained_dict['cos_sim.weight'] # Remove the last layer\n",
    "            model_dict = model.state_dict()\n",
    "            model_dict.update(pretrained_dict)\n",
    "            model.load_state_dict(model_dict)\n",
    "\n",
    "            # Break down the sets into individual episodes\n",
    "            support_x, support_y = batch_support_x[i], batch_support_y[i]\n",
    "            query_x, query_y = batch_query_x[i], batch_query_y[i]\n",
    "\n",
    "            # Find Average Features\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                # Initialise list containing features sorted by class\n",
    "                label_features = [torch.FloatTensor([]).to(device) for i in range(n_way)]\n",
    "\n",
    "                # Initialise weight for the nearest centroid, last layer weight\n",
    "                fc_weight = torch.FloatTensor([]).to(device)\n",
    "\n",
    "                # Get Features\n",
    "                _, features = model(support_x, extract_features=True)\n",
    "\n",
    "                # Sort features by labels to be averaged later on\n",
    "                for i in range(features.size(0)):\n",
    "                    label = support_y[i]\n",
    "                    label_features[label] = torch.cat((label_features[label], features[i][None]))\n",
    "\n",
    "                # Create weight for the last layer\n",
    "                for j in range(n_way):\n",
    "                    feature_avg = torch.mean(label_features[j], 0)\n",
    "                    fc_weight = torch.cat((fc_weight, feature_avg[None]), 0)\n",
    "\n",
    "                # Apply weight to the model\n",
    "                nc_dict = model.state_dict()\n",
    "                nc_dict['cos_sim.weight'] = fc_weight\n",
    "                model.load_state_dict(nc_dict)\n",
    "\n",
    "            # Testing\n",
    "            val_loss, acc, m_acc, macro_f1, class_f1 = test(query_x, query_y, model, criterion, device, n_way)\n",
    "\n",
    "            # Print the results per experiment\n",
    "            print(f'[v_loss: {val_loss:.5f} val_acc: {acc:.5f} val_m_acc: {m_acc:.5f} f1: {macro_f1:.5f}')\n",
    "\n",
    "            # Record the experiment to be saved into a CSV\n",
    "            df_hold.loc[0] = [val_loss, acc, m_acc, macro_f1] + class_f1\n",
    "            df_results = df_results.append(df_hold.loc[0], ignore_index=True)\n",
    "\n",
    "    # Create results folder if it does not exist\n",
    "    if not os.path.exists(path_results):\n",
    "        os.makedirs(path_results)\n",
    "\n",
    "    df_results.to_csv(os.path.join(path_results, f'{k_shot}shot_nc_cs.csv'), index=False)  # Export results to a CSV file\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from basic_train import *\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Set Training Parameters\n",
    "    num_workers = 12\n",
    "    bs = 64\n",
    "    n_way = 3\n",
    "    path_pretrained = '../results/basic_cosine/basic_cosine_55.pth'\n",
    "    save_models = True  # Whether to save the trained models (Occurs every epoch)\n",
    "    k_shot = 20  # Must have the generated split to match it\n",
    "\n",
    "    path_splits = f'../splits/{k_shot}_shot.csv'  # Location of preprocessed splits\n",
    "    path_results = f'../../results/{k_shot}shot_nc_cs.csv'  # Full path to save the CSV results\n",
    "    path_models = f'../../models/nc_cs/{k_shot}_shot'  # Folder path to save the trained models to\n",
    "\n",
    "    torch.cuda.set_device(0)\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Load in model\n",
    "    model = CosineSimilarityNet(n_way).to(device)\n",
    "    pretrained_dict = torch.load(path_pretrained)\n",
    "    # del pretrained_dict['linear.weight']  # Remove the last layer\n",
    "    # del pretrained_dict['linear.bias']\n",
    "    del pretrained_dict['cos_sim.weight']\n",
    "    model_dict = model.state_dict()\n",
    "    model_dict.update(pretrained_dict)\n",
    "    model.load_state_dict(model_dict)\n",
    "\n",
    "    # Load in data\n",
    "    train_dataset = MimicCxrJpg(root='../../../../scratch/rl80/mimic-cxr-jpg-2.0.0.physionet.org/files/',\n",
    "                                csv_path=path_splits, mode='novel_train', resize=224)\n",
    "    test_dataset = MimicCxrJpg(root='../../../../scratch/rl80/mimic-cxr-jpg-2.0.0.physionet.org/files/',\n",
    "                               csv_path=path_splits, mode='novel_validate', resize=224)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=bs, shuffle=True, num_workers=num_workers)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=bs, shuffle=True, num_workers=num_workers)\n",
    "\n",
    "    # Create Dataframe to export results to CSV\n",
    "    df_results = pd.DataFrame(columns=['Validation Loss', 'Accuracy', 'Macro Accuracy',\n",
    "                                       'Macro-F1 Score'] + [str(x) + ' F1' for x in range(n_way)])\n",
    "\n",
    "    # Find Average Features\n",
    "    label_features = [torch.FloatTensor([]).to(device) for i in range(n_way)]\n",
    "    model.eval()\n",
    "    fc_weight = torch.FloatTensor([]).to(device)  # Initialise weight for the nearest centroid, last layer weight\n",
    "    with torch.no_grad():\n",
    "        for step, (data_inputs, data_labels) in enumerate(train_loader):\n",
    "            # Get Features\n",
    "            inputs, labels = data_inputs.to(device), data_labels.to(device)\n",
    "            _, features = model(inputs, extract_features=True)\n",
    "\n",
    "            # Sort features into labels\n",
    "            for i in range(features.size(0)):\n",
    "                label = labels[i]\n",
    "                label_features[label] = torch.cat((label_features[label], features[i][None]))\n",
    "\n",
    "        # Create weight for the last layer\n",
    "        for j in range(n_way):\n",
    "            feature_avg = torch.mean(label_features[j], 0)\n",
    "            fc_weight = torch.cat((fc_weight, feature_avg[None]), 0)\n",
    "\n",
    "        # Apply weight to the model\n",
    "        nc_dict = model.state_dict()\n",
    "        nc_dict['cos_sim.weight'] = fc_weight\n",
    "        model.load_state_dict(nc_dict)\n",
    "\n",
    "    # Testing\n",
    "    val_loss, acc, m_acc, macro_f1, class_f1 = test(model, test_loader, criterion, device, n_way)\n",
    "\n",
    "    if (save_models):\n",
    "        torch.save(model.state_dict(), os.path.join(path_models, f'nc_cs.pth'))  # Save the model\n",
    "\n",
    "    # Append and report results\n",
    "    df_results.loc[0] = [val_loss, acc, m_acc, macro_f1] + class_f1\n",
    "    print(f'v_loss: {val_loss:.5f} val_acc: {acc:.5f} val_m_acc: {m_acc:.5f} f1: {macro_f1:.5f}')\n",
    "\n",
    "    df_results.to_csv(path_results, index=False)  # Export results to a CSV file\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
